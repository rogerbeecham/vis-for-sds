[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Visualization for Social Data Science",
    "section": "",
    "text": "Version: 0.0.1\n\n\nSocial scientists have at their disposal an expanding array of data measuring very many social behaviours. This undoubtedly is a positive. Previously unmeasurable aspects of human behaviour can now be explored in a large-scale empirical way, whilst already measured aspects of behaviour can be re-evaluated. These data, and the sorts of analysis approaches that they provoke, elevate visual analysis approaches in importance due to visualization’s heavy emphasis on discovery. When encountering new data for the first time, data graphics help expose complex structure and multivariate relations, and in so doing advance analysis situations where the questions to be asked and techniques to be deployed may not be immediately obvious.\nNumerous visualization toolkits such as ggplot2, vega-lite and Tabeleau have been designed to ease the process of generating data graphics for analysis. There is a comprehensive set of texts and resources on visualization design theory and practice, and several notable how-to primers on visualization. However, comparatively few existing resources demonstrate with real large-scale data and real social science scenarios how and why visual data analysis approaches can be inserted into an analyst’s workflow.\nThis book aims to fill this space. It presents principled workflows, with accompanying code, for using data graphics and statistics in tandem. In doing so it equips readers with the key critical design and technical skills needed to analyse and communicate with a wide range of datasets in the social sciences.\nThe book emphasises application. Each chapter introduces the fundamentals by analysing a real-world dataset located within various social science domains: Political Science, Crime Science, Urban and Transport Planning. Some of the structure inferred from these datasets may be spurious. It will be necessary to think critically about claims that can be made and about techniques that might be used to help guard against over-interpretation and false discovery. As well as learning how (and why) to use graphics and statistics to explore patterns in data, the book demonstrates how to communicate and tell stories with integrity, implementing recent ideas from data journalism.\n\n\n\nChapters of the book are divided into Concepts and Techniques. The Concepts sections discuss key literature, ideas and approaches that might be leveraged to analyse the dataset introduced in the chapter. In the Techniques sections, code examples are provided for implementing the discussed techniques. Each chapter starts with a list of Knowlegde outcomes and Skills outcomes that map directly to Concepts and Techniques.\nChapters are also accompanied, via the Appendices, with Tasks for readers to complete in order to test learning. Some tasks are technical and require code to be written; some are more conceptual. To support these tasks, each chapter has a corresponding computational notebook template file. The templates contain pre-prepared code chunks to be executed. As the book progresses, the amount of pre-prepared code in the templates reduces and readers must contribute more code of their own.\nAfter reading the book, you will be able to:\n\nDescribe, process and combine social science datasets from a range of sources\nDesign statistical graphics that expose structure in social science data and that are underpinned by established principles in information visualization and cartography\nUse modern data science and visualization frameworks to produce coherent, reproducible data analyses\nApply modern statistical techniques for analysing, representing and communicating data and model uncertainty\n\n\n\n\nThe book is for people working in the broadly defined social sciences, including Geography, Political Science, Economics, Business, Sociology and Psychology. It is aimed at a borad range of interests and experience levels: postgraduate students and researchers, data journalists, analysts working in public sector, non-governmental and commercial organisations.\nAll technical examples are implemented using the R programming environment. The book assumes only cursory prior knowledge of the R ecosystem. However, as the chapters progress more advanced concepts and coding procedures, core to modern data analysis, are introduced. While the book covers many of the fundamentals of R for working with social science datasets, it is designed to complement core resources that more fully cover these how-to aspects: R for Data Science (Wickham and Grolemund 2017), Geocomputation with R (Lovelace, Nowosad, and Muenchow 2019) and .\n\n\n\nBlah\n\n\n\n\n\n\nThis website is licensed under the Creative Commons Attribution-NonCommercial-NoDerivs 4.0 License."
  },
  {
    "objectID": "01-intro.html",
    "href": "01-intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "By the end of this chapter you should gain the following knowledge and practical skills."
  },
  {
    "objectID": "01-intro.html#introduction",
    "href": "01-intro.html#introduction",
    "title": "1  Introduction",
    "section": "\n1.1 Introduction",
    "text": "1.1 Introduction\nThis chapter introduces the what, why and how of the book. An argument is presented for the use of visual approaches in modern data analysis before the key technologies and analysis frameworks for the book are introduced: computational notebooks executed in Quarto. The technical component consolidates on readers’ knowledge and understanding of R and Quarto as well as demonstrating how to organise and curate data science analyses as RStudio Projects."
  },
  {
    "objectID": "01-intro.html#concepts",
    "href": "01-intro.html#concepts",
    "title": "1  Introduction",
    "section": "\n1.2 Concepts",
    "text": "1.2 Concepts\n\n1.2.1 Why vis4sds?\nIt is now taken-for-granted that over the last decade or so new data, new technology and new ways of doing science have transformed how we approach the world’s problems. Evidence for this can be seen in the response to the Covid-19 pandemic. Enter Covid19 github into a search and you’ll be confronted with hundreds of repositories demonstrating how an ever-expanding array of data related to the pandemic can be collected, processed and analysed. Data Science is a term used widely to capture this shift.\nThe term Data Science has been somewhat stretched over the years, but it has its origins in the work of John Tukey’s The Future of Data Analysis (1962). Drawing on this, and a survey of more recent work, Donoho (2017) identifies six key facets that a Data Science discipline might encompass1:\n\ndata gathering, preparation and exploration;\ndata representation and transformation;\ncomputing with data;\ndata visualization and presentation;\ndata modelling;\nand a more introspective “science about data science”\n\nGlancing at this list, data visualization could be interpreted as a single facet of data science process – something that happens after data gathering, preparation, exploration, but before modelling. Through this book you’ll learn, rather, that data visualization is intrinsic to, and should inform, each of these activities.\nLet’s develop this idea by exploring why data visualizations are used in the first place. The most commonly-cited case for the use of visualization in data anaysis is Anscombe’s quartet. This consists of four datasets, each containing eleven observations and two variables for each observation. The data are synthetic, but let’s say that they are the weight and height of independent samples taken from a population of postgraduate students studying Social Data Science. Presented with a new dataset it makes sense to compute some summaries, and doing so we observe that the data appear identical – they contain the same means, variances and strong positive correlation coefficient (Figure 1.1). This seems appropriate since the data are measuring individuals’ weight and height. Although there may be some variation, we’d expect taller students to be heavier. Given these statistical summaries we can be assured that we are drawing samples from the same population of (Data Science) students.\n\n\n\nFigure 1.1: Data from Anscombe’s quartet\n\n\nLaying out the data in a meaningful way, horizontally according to weight (x) and vertically according to the height (y) to form a scatterplot, we quickly see that whilst these data contain the same high-level statistical properties they are very different. Only dataset #1 now looks plausible if it were truly a sample of weights and heights drawn from a population of students.\n\n\nFigure 1.2: Plots of Anscombe’s quartet\n\n\nAnscombe’s is a deliberately contrived example2, but one can imagine many cases where important structure is missed or incorrect assumptions are made without data being subject to visual examination. The consequences are poorly specified models and potentially faulty claims. This is not to undermine the importance of numerical analysis. Numeric summaries that simplify patterns are extremely useful and Statistics has at its disposal an array of tools for helping to guard against making false claims from datasets – a theme that we will return to in chapters 6 and 7 when we think critically about the use of visual approaches for data analysis. There remain, though, certain classes of relation and context that cannot be easily captured through statistics alone.\nGeographic context, for example, is undoubtedly challenging to capture numerically. Many of the early examples of data visualization have been of spatial phenomena and generated by Geographers (see Friendly 2007). We can also make a special case for the use of visual approaches in Social Data Science (SDS) applications due to their exploratory nature. Often datasets are being repurposed for social sciences research for the first time; contain complex structure and relations that cannot be easily captured by statistical summaries alone; and so the types of questions that can be asked and the techniques deployed to answer them cannot be easily specified in advance. We will experience this through the book as we explore (Chapter 4 and 5), model under uncertainty (Chapter 6 and 7) and communicate (Chapter 8) with various social science datasets.\n\n\n\n\n\n\nTask\n\n\n\nWatch Jo Wood’s TEDx talk demonstrating how visual techniques can be used to analyse urban travel behaviours. In the video Jo argues that bikeshare schemes can help democratise cycling, but also for their potential contributions to research – he briefly contrasts new, passively collected data sets with more traditional actively collected data for analysing how people move around cities. A compelling case is then made for the use of visualization to support analysis of these new forms of behavioural data.\n\n\n\n1.2.2 What vis4sds?\nThe chapters of this book blend both theory and practical coding activity. We will cover the fundamentals of visual data analysis from Information Visualization and Statistics. As the chapters progress, data processing and analysis code is applied to datasets from the Political Science, Urban and Transport Planning and Health domains. So the examples in the book demonstrate how visual approaches can be used to generate and evaluate real findings and knowledge.\nTo do this, we have to cover a reasonably broad set of data processing and analysis procedures. As well as developing expertise around designing data-rich, visually compelling graphics, we will need to cover more tedious aspects of data processing and wrangling. Additionally, if we are to learn how to make and communicate claims under uncertainty with our data graphics, then we need to cover some aspects of estimation and modelling from Statistics. In short, we cover most of Donoho (2017)’s six key facets of a data science discipline:\n\ndata gathering, preparation, and exploration (Chapters 2, 3, 4);\ndata representation and transformation (Chapters 2, 3);\ncomputing with data (Chapter 2, All chapters);\ndata visualization and presentation (All chapters);\ndata modelling (Chapters 4, 6, 7);\nand the “science about data science” (All chapters)\n\nThere is already a rich and impressive set of open resources practically introducing how to do modern Data Science (Wickham and Grolemund 2017), Visualization (Healy 2018) and Geographic Analysis (Lovelace, Nowosad, and Muenchow 2019). What makes this book different from these existing resources is that we will be doing applied data science throughout – we will be identifying and diagnosing problems when gathering data, discovering patterns (some may even be spurious) as we do exploratory analysis, and attempt to make claims under uncertainty as we generate models based on observed patterns.\n\n1.2.3 How vis4sds?\n\n1.2.3.1 R for modern data analysis\nAll data collection, analysis and reporting activity will be completed using R. Released as open source software as part of a research project in 1995, for some time R was the preserve of academics. From 2010s onwards, the R community expanded rapidly and along with Python is regarded as a key programming environment technology for modern data analysis.\nThere are many benefits that come from being fully open-source, with a critical mass of users. Firstly, there is an array of online fora, tutorials and code examples from which to learn. Second, with such a large community, there are numerous expert R users who themselves contribute by developing packages that extend its use.\nOf particular importance is the tidyverse package. This is a set of packages for doing data science authored by the software development team at Posit. tidyverse packages share a principled underlying philosophy, syntax and documentation. Contained within the tidyverse is its data visualization package, ggplot2. This package predates the tidyverse and is one of the most widely-used toolkits for generating data graphics. As with other heavily used visualization toolkits (Tableau, vega-lite) it is inspired by Wilkinson (1999)’s The Grammar of Graphics, the gg in ggplot2 stands for Grammar of Graphics. We will cover some of the design principles behind the Grammar of Graphics (and tidyverse) in Chapter 3.\n\n1.2.3.2 Quarto for reproducible research\n\nReproducible research is the idea that data analyses, and more generally, scientific claims, are published with their data and software code so that others may verify the findings and build upon them.\nRoger Peng, Jeff Leek and Brian Caffo\n\nIn recent years there has been much introspection into how science works – into how statistical claims are made from reasoning over evidence. This came on the back of, amongst other things, a high profile paper published in Science (Open Science Collaboration 2015), which found that of 100 contemporary peer-reviewed empirical papers in Psychology, the findings of only 39 could be replicated. The upshot is that researchers must now make every possible effort to make their work transparent, such that “all aspects of the answer generated by any given analysis [can] be tested” (Brunsdon and Comber 2020).\nA reproducible research project should be accompanied with:\n\ncode and data that allows tables and figures presented in research outputs to be regenerated\ncode and data that does what it claims (the code works)\ncode and data that can be justified and explained through proper documentation\n\nIf these goals are met, then it may be possible for others to use the code on new and different data to study whether the findings reported in one project might be consistent with another. Or alternatively to use the same data, but update the code to extend the original analysis. This model – generate findings, test for replicability in new contexts and re-analysis – is how knowledge development has always worked. However, to achieve this the data and procedures on which findings are generated must be made open and transparent.\nIn this setting proprietary data analysis software that support point-and-click interaction, often used widely in the socual sciences, are problematic. First, these software are often underpinned by code for implementing statistical procedures that is closed. It is not possible, and therefore less common, for the researcher to fully interrogate into the underlying processes that are being implemented and the results need to be taken more or less on faith. Second, replicating and updating anayses in light of new data, is chellenging. It would be tedious to make notes describing all interactions performed when working with a dataset via a point-and-click- interface. As a declarative programming environment, it is very easy to provide such a provenance trail in R (with the tidyverse) since this necessarily exists in the analysis scripts. But also, the Integrated Development Environments (IDEs) through which R (and Python) are most often accessed provide notebook environments that allow users to curate reproducible computational documents that blend input code, explanatory prose and outputs. Through the practical exercises, we will prepare these sorts of notebooks using Quarto."
  },
  {
    "objectID": "01-intro.html#techniques",
    "href": "01-intro.html#techniques",
    "title": "1  Introduction",
    "section": "\n1.3 Techniques",
    "text": "1.3 Techniques\nReaders of this book might already have some familiarity with R and the RStudio IDE. If not, then this section is designed to quickly acclimatise readers with R and RStudio and to briefly introduce Quarto, R scripts and RStudio Projects. The accompanying template file, 01-template.qmd can be downloaded from the book’s companion website.\n\n1.3.1 R and RStudio\n\nInstall the latest version of R. Note that there are installations for Windows, macOS and Linux. Run the installation from the file you downloaded (an .exe or .pkg extension).\nInstall the latest version of RStudio Desktop. Note again that there are separate installations depending on operating system – for Windows an .exe extension, macOS a .dmg extension.\nOnce installed, open the RStudio IDE.\nOpen an R Script by clicking File > New File > R Script .\n\n\n\nFigure 1.3: The RStudio IDE\n\n\nYou should see a set of windows roughly similar to those in Figure 1.3. The top left pane is used either as a Code Editor (the tab named Untitled1) or Data Viewer. This is where you’ll write, organise and comment R code for execution or inspect datasets as a spreadsheet representation. Below this in the bottom left pane is the R Console, in which you write and execute commands directly. To the top right is a pane with the tabs Environment and History. This displays all objects – data and plot items, calculated functions – stored in-memory during an R session. In the bottom right is a pane for navigating through project directories, displaying plots, details of installed and loaded packages and documentation on their functions.\n\n1.3.2 Compute in the console\nYou will write and execute almost all code from the code editor pane. To start though let’s use R as a calculator by typing some commands into the Console. You’ll create an object (x) and assign it a value using the assignment operator (<-), then perform some simple statistical calculations using functions that are held within the base package.\n\n\n\n\n\n\nR package documentation\n\n\n\nThe base package exists as standard in R. Unlike other packages, it does not need to be installed and called explicitly. One means of checking the package to which a function you are using belongs is to call the help command (?) on that function: e.g. ?mean().\n\n\nType the commands contained in the code block below into your R Console. Notice that since you are assigning values to each of these objects they are stored in memory and appear under the Global Environment pane.\n\n# Create variable and assign a value.\nx <- 4\n# Perform some calculations using R as a calculator.\nx_2 <- x^2\n# Perform some calculations using functions that form baseR.\nx_root <- sqrt(x_2)\n\n\n1.3.3 Install some packages\nThere are two steps to getting packages down and available in your working environment:\n\n\ninstall.packages(\"<package-name>\") downloads the named package from a repository.\n\nlibrary(<package-name>) makes the package available in your current session.\n\nInstall tidyverse, the core collection of packages for doing Data Science in R, by running the code below:\n\ninstall.packages(\"tidyverse\")\n\nIf you have little or no experience in R, it is easy to get confused about downloading and then using packages in a session. For example, let’s say we want to make use of the simple features package (sf), which we will draw on heavily for performing spatial operations.\n\nlibrary(sf)\n\nUnless you’ve previously installed sf, you’ll probably get an error message that looks like this:\n\n> Error in library(sf): there is no package called ‘sf’\n\nSo let’s install it.\n\ninstall.packages(\"sf\")\n\nAnd now it’s installed, why not bring up some documentation on one of its functions (st_contains()).\n\n?st_contains()\n\nSince you’ve downloaded the package but not made it available to your session, you should get the message:\n\n> No documentation for ‘st_contains’ in specified packages and libraries\n\nSo let’s try again, by first calling library(sf).\n\nlibrary(sf)\n## Linking to GEOS 3.7.2, GDAL 2.4.1, PROJ 6.1.0\n?st_contains()\n\nNow let’s install some of the remaining core packages on which this book depends. Run the block below, which passes a vector of package names to the install.packages() function.\n\npkgs <- c(\n  \"devtools\",\"here\", \"quarto\",\"fst\",\"tidyverse\", \"lubridate\", \n  \"tidymodels\", \"gganimate\", \"ggforce\", \"distributional\", \"ggdist\"\n)\ninstall.packages(pkgs)\n\n\n\n\n\n\n\nR package visibility\n\n\n\nIf you wanted to make use of a package only very occasionally in a single session, you could access it without explicitly loading it via library(<package-name>), using this syntax: <package-name>::<function_name>, e.g. ?sf::st_contains().\n\n\n\n1.3.4 Experiment with Quarto\nQuarto documents are suffixed with the extension .qmd. They are computational notebooks that blend code with textual explanation and images, and so are a mechanism for supporting literate programming (Knuth 1984). They resemble Markdown, a lightweight language designed to minimise tedious markup tags (<header></header>) when preparing HTML documents. The idea is that you trade some flexibility in the formatting of your HTML for ease-of-writing. Working with Quarto documents feels very similar to Markdown. Sections are denoted hierarchically with hashes (#, ##, ###) and emphasis using * symbols (*emphasis* **added** reads emphasis added ).\nDifferent from standard Markdown, Quarto documents can also contain code chunks to be run when the document is rendered: they are a mechanism for producing reproducible, dynamic and interactive notebooks. Dynamic and reproducible because the outputs may change when there are changes to the underlying data; interactive because they can execute not just R code blocks, but also Jupyter Widgets, Shiny and Observable JS. Each chapter has an accompanying Quarto file. In later chapters you will use these to author computational notebooks that blend code, analysis prose and outputs.\nDownload the 01-template.qmd file for this chapter and open it in RStudio by clicking File > Open File ... > <your-downloads>/01-template.qmd. Note that there are two tabs that you can switch between when woring with .qmd files. Source retains markdown syntax (e.g. #|##|### for headings); Visual renders these tags and allows you to, for example, perform formatting and build tables through point-and-click utilities.\nA quick anatomy of .qmd files :\n\n\nYAML - positioned at the head of the document and contains metadata determining amongst other things the author details and the output format when rendering.\nTEXT - incorporated throughout to document and comment on your analysis.\nCODE chunks - containing discrete blocks that are run when the .qmd file is rendered.\n\n\n\nFigure 1.4: The anatomy of .qmd files\n\n\nThe YAML section of an .qmd file controls how your file is rendered and consists of key : value pairs enclosed by ---. Notice that you can change the output format to generate for example .pdf, .docx files for your reports.\n---\nauthor: \"Roger Beecham\"\ntitle: \"Chapter 01\"\nformat: html\n---\nQuarto files are rendered with the Render button, annotated in the Figure above. This starts pandoc and executes all the code chunks and outputs in the case above an .html file. The markdown file can then be converted to many different output formats via pandoc.\n\nRender the 01-template.qmd file for this chapter by clicking the Render button.\n\nYou will notice that code chunks in Quarto can be customised in different ways. This is achieved by populating fields immediately after the curly brackets used to declare the code chunk:\n```{r}\n#| label: chunk-name\n#| echo: true\n#| eval: false\n\n# R that appears below that is not run (evaluated) but which is printed (echoed)\n# in this position when the .qmd doc is rendered.\n```\nA quick overview of the parameters.\n\n\nlabel: <chunk-name> Chunks can be given distinct names. This is useful for navigating Quarto files. It also supports chaching – chunks with distinct names are only run once, important if certain chunks take some time to execute.\n\necho: <true|false> Determines whether the code is visible or hidden from the rendered file. If the output file is a data analysis report you may not wish to expose lengthy code chunks as these may disrupt the discursive text that appears outside of the code chunks.\n\neval: <true|false> Determines whether the code is evaluated (executed). This is useful if you wish to present some code in your document for display purposes.\n\ncache: <true|false> Determines whether the results from the code chunk are cached.\n\n1.3.5 R Scripts\nWhilst there are obvious benefits to working in .qmd documents when doing data analysis, there may be occasions where working in a script is preferable. R scripts are plain text files with the extension .R. They are typically used for writing discrete but substantial code blocks that are to be executed. For example, a set of functions that relate to a particular use case might be organised into an R script, and those functions referred to in a data analysis from a .qmd in a similar way as one might import a package. Below is an example script file with helper functions to support flow visualizations in R. The script is saved with the file name bezier_path.R. If it were stored in a sensible location, like a project’s code folder, it could be called from a .qmd file with source(./code/bezier_path). R Scripts can be edited in the same way as Quarto files in RStudio, via the Code Editor pane.\n\n# bezier_path.R\n#\n# Author: Roger Beecham\n##############################################################################\n\n#' Functions for generating input data for asymmetric bezier curve,\n#' such that the origin is straight and destination curve. The retuned tibble\n#' is passed to geom_bezier().\n#'\n#' Parametrtisation follows that published in Wood et al. 2011.\n#' doi: 10.3138/carto.46.4.239.\n#'\n#' @param data A df with origin and destination pairs of 2D locations\n#' (o_east, o_north, d_east, d_north) in cartesian space.\n#' @param degrees For converting to radians.\n#' @return A tibble of coordinate pairs representing asymmetric curve\n\nget_trajectory <- function(data) {\n  o_east=data$o_east\n  o_north=data$o_north\n  d_east=data$d_east\n  d_north=data$d_north\n  od_pair=data$od_pair\n\n  curve_angle=get_radians(-90)\n  east=(o_east-d_east)/6\n  north=(o_north-d_north)/6\n  c_east=d_east + east*cos(curve_angle) - north*sin(curve_angle)\n  c_north=d_north + north*cos(curve_angle) + east*sin(curve_angle)\n  d <- tibble(\n    x=c(o_east,c_east,d_east),\n    y=c(o_north,c_north,d_north),\n    od_pair=od_pair\n  )\n}\n\n# Convert degrees to radians.\nget_radians <- function(degrees) {\n  (degrees * pi) / (180)\n}\n\nR Scripts are more straightforward than Quarto files in that you don’t have to worry about configuring code chunks. They are really useful for quickly developing bits of code. This can be achieved by highlighting over the code that you wish to execute and clicking the Run icon at the top of the Code Editor pane or by typing ctrl + rtn on Windows, ⌘ + rtn on macOS.\n\n1.3.6 Create an RStudio Project\nThroughout the book we will use project-oriented workflows. This is where all files pertaining to a data analysis – data, code and outputs – are organised from a single root folder and where file path discipline is used such that all paths are relative to the project’s root folder (see Bryan & Hester 2020). You can imagine that this sort of self-contained project set-up is necessary for achieving reproducibility of your research. It allows anyone to take a project and run it on their own machines without having to make any adjustments.\nYou might have noticed that when you open RStudio it automatically points to a working directory, likely the home folder for your local machine, denoted with ~/ in the Console. RStudio will by default save any outputs to this folder and will also expect any data you use to be saved there. Clearly if you want to incorporate neat, self-contained project workflows then you will want to organise your work from a dedicated project folder rather than the default home folder for your machine. This can be achieved with the setwd(<path-to-your-project>) function. The problem with doing this is that you insert a path which cannot be understood outside of your local machine at the time it was created. This is a real pain. It makes simple things like moving projects around on your machine an arduous task and most importantly it hinders reproducibility if others are to reuse your work.\nRStudio Projects resolve these problems. Whenever you load an RStudio Project, R starts up and the working directory is automatically set to the project’s root folder. If you were to move the project elsewhere on your machine, or to another machine, a new root is automatically generated – so RStudio projects ensure that relative paths work.\n\n\nFigure 1.5: Creating an RStudio Project\n\n\nLet’s create a new Project for this book:\n\nSelect File > New Project > New Directory.\nBrowse to a sensible location and give the project a suitable name. Then click Create Project.\n\nYou will notice that the top of the Console window now indicates the root for this new project (~projects/vis4sds).\n\nIn the root of your project, create folders called reports, code, data, figures.\nSave this session’s 01-template.qmd file to the reports folder.\n\nYour project’s folder structure should now look like this:\nvis4sds\\\n  vis4sds.Rproj\n  code\\\n  data\\\n  figures\\\n  reports\\\n    01-template.qmd"
  },
  {
    "objectID": "01-intro.html#conclusions",
    "href": "01-intro.html#conclusions",
    "title": "1  Introduction",
    "section": "\n1.4 Conclusions",
    "text": "1.4 Conclusions\nVisual data analysis approaches are necessary for exploring complex patterns in data and to make and communicate claims under uncertainty. This is especially true of social data science applications, where: datasets are repurposed for social sciences research often for the first time; contain complex structure and geo-spatial relations that cannot be easily captured by statistical summaries alone; and, consequently, where the types of questions that can be asked and the techniques deployed to answer them cannot be easily specified in advance.\nIn this book we will demonstrate this as we explore (Chapter 4 and 5), model under uncertainty (Chapter 6) and communicate (Chapter 7 and 8) with various social science datasets. We will work with both new, large-scale behavioural datasets, as well as more traditional, administrative datasets located within various social science domains: Political Science, Crime Science, Urban and Transport Planning. We will do so using the statistical programming environment R, making use of various tools and software libraries that form part of the R ecosystem: the tidyverse for doing modern data science and Quarto for authoring reproducible research projects."
  },
  {
    "objectID": "02-data.html",
    "href": "02-data.html",
    "title": "2  Data Fundamentals",
    "section": "",
    "text": "By the end of this chapter you should gain the following knowledge and practical skills."
  },
  {
    "objectID": "02-data.html#introduction",
    "href": "02-data.html#introduction",
    "title": "2  Data Fundamentals",
    "section": "\n2.1 Introduction",
    "text": "2.1 Introduction\nThis chapter covers some of the basics of how to describe and organise data. Whilst this might sound prosaic, there are several reasons why being able to consistently describe a dataset is important. First, it is the initial step in any analysis and helps delimit the analytical procedures that can be deployed. This is especially relevant to data science-type workflows, where it is common to apply the same analysis templates for working over many different datasets. Describing your dataset using a consistent vocabulary enables you to identify which analysis templates to reuse. Second relates to the point in Chapter 1 that Social Data Science (SDS) projects usually involve repurposing datasets for the first time. It is often not obvious whether the data contain sufficient detail and structure to characterise the behaviours being researched and the target populations they are assumed to represent. This leads to additional levels of uncertainty and places greater importance on the initial steps of data processing, description and exploration.\nThrough the chapter we will learn both language for describing and thinking about data, but also how to deploy some of the most important data processing and organisation techniques in R to wrangle real datasets. We will do so using data from New York’s Citibike scheme, accessed through the bikedata package, an API to Citibike’s publicly available origin-destination trip data.\n\n\n\n\n\n\nData vocabulary\n\n\n\nThe idea of applying a consistent vocabulary to describing your data applies especially to working with modern visualization toolkits (ggplot2, Tableau, vega-lite), and will be covered in some detail in chapter 3 as we introduce Visualization Fundamentals and the Grammar of Graphics (Wilkinson 1999)."
  },
  {
    "objectID": "02-data.html#concepts",
    "href": "02-data.html#concepts",
    "title": "2  Data Fundamentals",
    "section": "\n2.2 Concepts",
    "text": "2.2 Concepts\n\n2.2.1 Data structure\nThroughout this book we will work with data frames. These are spreadsheet-like representations where rows are observations and columns are variables. In a data frame, variables are vectors that must be of equal length. Where observations have missing values for certain variables – that is, where they may violate this equal-length requirement – the missing values must be substituted with something, usually with NA or similar. This constraint can cause difficulties. For example, when working with variables that contain many values of different length for an observation. In these cases we create a special class of column, a list-column. Organising data according to this simple structure – rows as observations, columns as variables – makes working with data somewhat straightforward. A dedicted set of tools, made available via the tidyverse, can be deployed for performing most data tidying operations (Wickham 2014).\n\n2.2.2 Types of variable\nA familiar classification for describing variables is that developed by Stevens (1946) when cosidering the level of measurement of a variable. Stevens (1946) organises variables into two classes: variables that describe categories of things and variables that describe measurements of things.\nCategories include attributes like gender, titles, ranked orders (1st, 2nd, 3rd largest etc.). Measurements include quantities like distance, age, travel time. Categories can be further subdivided into those that are unordered (nominal) from those that are ordered (ordinal). Measurements can also be subdivided. Interval measurements are quantities where the computed difference between two values is meaningful. Ratio measurements have this property, but also have a meaningful 0, where 0 means the absence of something, and the ratio of two values can be computed. The most common cited example of an interval measurement is temperature in °C. Temperatures can be ordered and compared additively, but 0 in °C does not mean the absence of temperature and 20°C is not twice as hot as 10°C.\nWhy is this useful? The measurement level of a variable determines the types of data analysis operations that can be performed and therefore allows us to efficiently make decisions when working with a dataset for the first time (Table 4.1).\n\n\n\n\n\nTable 2.1:  Breakdown of variable types \n \n Measurement \n    Example \n    Operators \n    Midpoint \n    Spread \n  \n\nCategories\n\n Nominal \n    Political parties; street names \n    =  ≠ \n    mode \n    entropy \n  \n\n Ordinal \n    Terrorism threat levels \n    =  ≠  \n    median \n    percentile \n  \nMeasures\n\n Interval \n    Temperatures; years \n    =  ≠  +  - \n    mean \n    variance \n  \n\n Ratio \n    Distances; prices \n    =  ≠  +  - | × ÷ \n    mean \n    variance \n  \n\n\n\n\n\n\n2.2.3 Types of observation\nObservations either together form an entire population or a subset, or sample, that we expect is representative of a target population. In Social Data Science applications we may often be working with datasets that are so-called population-level. The Citibike dataset is a complete, population-level dataset in that every journey made through the scheme is recorded. Whether or not this is truly a population-level dataset, however, depends on the analysis purpose. When analysing the bikeshare dataset are we interested only in describing use within the Citibike scheme? Or are we taking the patterns observed through our analysis to make claims and inferences about cycling more generally?\nIf the latter, then there are problems as the level of detail we have on our sample is pretty trivial compared to traditional, actively-collected datasets, where data collection activities are designed with a specified target population in mind. It may therefore be difficult to gauge how representative Citibike users and Citibike cycling is of New York’s general cycling population. The flipside is that such passively-collected data do not suffer from the same problems such as non-response bias and social-desirability bias as traditional, actively-collected datasets.\n\n2.2.4 Tidy data\nWe will be working with data frames organised such that columns always and only refer to variables and rows always and only refer to observations. This arrangement, called tidy (Wickham 2014), has two key advantages. First, if data are arranged in a consistent way, then it is easier to apply and re-use tools for wrangling them due to data having the same underlying structure. Second, placing variables into columns, with each column containing a vector of values, means that we can take advantage of R’s vectorised functions for transforming data – we will demonstrate this in the technical element of the chapter.\nThe three rules for tidy data:\n\nEach variable forms a column.\nEach observation forms a row.\nEach type of observational unit forms a table.\n\nDrug treatment dataset\nTo elaborate further, we can use the example given in Wickham (2014), a drug treatment dataset in which two different treatments are administered to participants.\n\n\nTable 2.2: Table 1 of Wickham 2014\n\n\n\n\n\n(a) One untidy organisation \n \n person \n    treatment_a \n    treatment_b \n  \n\n\n John Smith \n    -- \n    2 \n  \n\n Jane Doe \n    16 \n    11 \n  \n\n Mary Johnson \n    3 \n    1 \n  \n\n\n\n\n\n\n(b) Alternative untidy organisation \n \n treatment \n    John Smith \n    Jane Doe \n    Mary Johnson \n  \n\n\n treatment_a \n    -- \n    16 \n    3 \n  \n\n treatment_b \n    2 \n    11 \n    1 \n  \n\n\n\n\n\n\n\n\n(c) Tidy organisation \n \n person \n    treatment \n    result \n  \n\n\n John Smith \n    a \n    -- \n  \n\n John Smith \n    b \n    2 \n  \n\n Jane Doe \n    a \n    16 \n  \n\n Jane Doe \n    b \n    11 \n  \n\n Mary Johnson \n    a \n    3 \n  \n\n Mary Johnson \n    b \n    1 \n  \n\n\n\n\n\nBoth untidy tables present the same information unambiguously – Table 2.2 (b) is simply Table 2.2 (a) transposed. However, neither is tidy as the observations are spread across both the rows and columns. This means that we need to apply different procedures to extract, perform computations on and visually represent, these data.\nIn the tidy form, each observation is a test result returned for each combination of person and treatment. To get to this formulation, it is necessary to identify the discrete variables:\n\n\nperson: a categorical nominal variable which takes three values: John Smith, Jane Doe, Mary Johnson.\n\ntreatment: a categorical nominal variable which takes values: a and b.\n\nresult: a measurement ratio variable with six recorded values (including the missing value): –, 16, 3, 2, 11,\nGapminder population dataset\nIn Wickham and Grolemund (2017), the benefits of tidy layouts are demonstrated using the gapminder dataset and with reference to key data processing functions in R. To consolidate our understanding of tidy data let’s quickly look at the gapminder data, as it is an example that we’re probably more likely to encounter in social science research.\n\n\nTable 2.3: Excerpts of gapminder dataset.\n\n\n\n\n\n(a) Tidy organisation \n \n country \n    year \n    cases \n    pop \n  \n\n\n Afghanistan \n    1999 \n    745 \n    19987071 \n  \n\n Afghanistan \n    2000 \n    2666 \n    20595360 \n  \n\n Brazil \n    1999 \n    37737 \n    172006362 \n  \n\n Brazil \n    2000 \n    80488 \n    174504898 \n  \n\n China \n    1999 \n    212258 \n    1272915272 \n  \n\n China \n    2000 \n    213766 \n    1280428583 \n  \n\n\n\n\n\n\n(b) Untidy organisation \n \n country \n    year \n    type \n    count \n  \n\n\n Afghanistan \n    1999 \n    cases \n    745 \n  \n\n Afghanistan \n    1999 \n    pop \n    19987071 \n  \n\n Afghanistan \n    2000 \n    cases \n    2666 \n  \n\n Afghanistan \n    2000 \n    pop \n    20595360 \n  \n\n Brazil \n    1999 \n    cases \n    37737 \n  \n\n Brazil \n    1999 \n    pop \n    174504898 \n  \n\n ... \n    ... \n    ... \n    ... \n  \n\n\n\n\n\n\n\n\n(c) Another untidy organisation \n \n country \n    year \n    f_cases \n    m_cases \n    f_pop \n    m_pop \n  \n\n\n Afghanistan \n    1999 \n    447 \n    298 \n    9993400 \n    9993671 \n  \n\n Afghanistan \n    2000 \n    1599 \n    1067 \n    10296280 \n    10299080 \n  \n\n Brazil \n    1999 \n    16982 \n    20755 \n    86001181 \n    86005181 \n  \n\n Brazil \n    2000 \n    39440 \n    41048 \n    87251329 \n    87253569 \n  \n\n China \n    1999 \n    104007 \n    108252 \n    636451250 \n    636464022 \n  \n\n China \n    2000 \n    104746 \n    109759 \n    640212600 \n    640215983 \n  \n\n\n\n\n\nFrom the tidy version of the data, we can identify the variables and note that each observation is a recorded count of cases and population for a country in a year.\n\n\ncountry: a categorical nominal variable.\n\nyear: a date (interval) variable.\n\ncases: a ratio (count) variable.\n\npopulation: a ratio (count) variable.\n\nAn alternative organisation is in Table 2.3 (b). This is untidy as the observations are spread across two rows, making operations that we might want to perform on the cases and population variables, for example computing exposure rates, somewhat tedious.\nImagine that the gapminder dataset reported values of cases separately by gender – Table 2.3 (c). A type of representation often seen in social science domains, probably as it is helpful for data entry, is where observations are spread across the columns. This too creates problems for performing aggregate functions, but also for specifying visualization designs in ggplot2."
  },
  {
    "objectID": "02-data.html#techniques",
    "href": "02-data.html#techniques",
    "title": "2  Data Fundamentals",
    "section": "\n2.3 Techniques",
    "text": "2.3 Techniques\nThe technical element to this chapter imports, describes, transforms and tidies data from New York’s Citibike scheme.\n\nDownload the 02-template.qmd file and save it to the reports folder of your vis4sds project that you created in chapter 1.\nOpen your vis4sds project in RStudio and load the template file by clicking File > Open File ... > reports/02-template.qmd.\n\n\n2.3.1 Import\nIn the template file there is a discussion of how to setup your R session with key packages – tidyverse , fst, lubridate, sf – and also the bikedata package for accessing bikeshare data.\nAvailable via the bikedata package are trip and occupancy data for a number of bikeshare schemes (as below). We will work with data from New York’s Citibike scheme for June 2020. A list of all cities covered by the bikedata package is below:\n\nbike_cities()\n##    city     city_name      bike_system\n## 1    bo        Boston           Hubway\n## 2    ch       Chicago            Divvy\n## 3    dc Washington DC CapitalBikeShare\n## 4    gu   Guadalajara           mibici\n## 5    la   Los Angeles            Metro\n## 6    lo        London        Santander\n## 7    mo      Montreal             Bixi\n## 8    mn   Minneapolis         NiceRide\n## 9    ny      New York         Citibike\n## 10   ph  Philadelphia           Indego\n## 11   sf      Bay Area       FordGoBike\n\nIn the template there are code chunks demonstrating how to download and process these data using bikedata’s API. This takes some time to execute and we use the fst package for serializing and reading in the these data.\n\nlibrary(bikedata) # API to TfL's trip data.\n\n# Create subdirectory in data folder for storing bike data.\nif(!dir.exists(here(\"bikedata\"))) dir.create(here(\"bikedata\"))\n\n# Download data for June 2020.\ndl_bikedata (city = \"citibike\",  data_dir = here(\"bikedata\"), dates=202006)\n\n# Read in and store in SQLite3 database.\n# Create sqlite db container.\nbikedb <- file.path (tempdir (), \"bikedb.sqlite\") \nstore_bikedat (data_dir = here(\"data\", \"bikedata\"), bikedb = bikedb)\n# Index dbase to speed up working.\nindex_bikedata_db(bikedb = bikedb)\n\n# Set up connection to SQLite db.\ncon <- DBI::dbConnect(RSQLite::SQLite(), bikedb)\n# Check tables that form the dbase.\nDBI::dbListTables(con)\n# Collect trips and stations tables for writing out to fst.\ntrips <- tbl(con, \"trips\") |>  collect()\nstations <- tbl(con, \"stations\") |>  collect()\n\n# Write trips out to .fst.\nfst::write_fst(trips, here(\"data\", \"\"bikedata\", \"ny_trips.fst\"))\n# Write stations out to .csv.\nwrite_csv(stations, here(\"data\", \"bikedata\", \"ny_stations.csv\"))\n\n# Clean workspace\nbike_rm_db(bikedb)\nrm(db, stations, trips, bikedb)\n\n\n\n\n\n\n\nNote\n\n\n\nfst implements in the background various operations such as multi-threading to reduce load on disk space. This makes it possible to work with large datasets in-memory in R rather than connecting to a database and serving up summaries/subsets to be loaded into R. We will be working with just 2 million records, but with fst it is possible to work in-memory with much larger datasets.\n\n\nSome of the above may be familiar to you. The key arguments to look at are read_csv() and read_fst(), into which we pass the path to the file. In this case we create a tmpfile() within the R session. We then write these data out and save locally to the project’s data folder. This is useful as we only want to download the data once. In the write_*<> functions we reference this location using the here package’s here() function, useful for reliably creating paths relative to your project’s root.\nTo read in these data in future R sessions, we again use read_csv() and read_fst() and point to the new file locations. Notice that we use assignment here (<-) so that these data are loaded as objects and appear in the Environment pane of your RStudio window.\n\n# Read in these local copies of the trips and stations data.\nny_trips <- read_fst(here(\"data\", \"ny_trips.fst\"))\nny_stations <- read_csv(here(\"data\", \"ny_stations.csv\"))\n\nny_stations and ny_trips are data frames, spreadsheet type representations containing observations in rows and variables in columns. Inspecting the layout of the stations data with View(ny_stations) you will notice that the top line is the header and contains column (variable) names.\n\n\nFigure 2.1: ny_trips and ny_stations as they appear when calling View()\n\n\n\n2.3.2 Describe\nThere are several functions for generating a quick overview of a data frame’s contents. glimpse<dataset-name> provides a summary of the data frame dimensions – we have c. 1.9 million trip observations in ny_trips and 11 variables1. The function also prints out the object type for each of these variables, with the variables either of type int, chr or dbl` in this case.\n\nglimpse(ny_trips)\n## Rows: 1,882,273\n## Columns: 11\n## $ id               <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21…\n## $ city             <chr> \"ny\", \"ny\", \"ny\", \"ny\", \"ny\", \"ny\", \"ny\", \"ny\", \"ny\", \"ny\", \"ny\", \"ny\", \"…\n## $ trip_duration    <dbl> 1062, 3810, 1017, 226, 1437, 355, 99, 1810, 87, 2714, 2096, 1611, 529, 69…\n## $ start_time       <chr> \"2020-06-01 00:00:03\", \"2020-06-01 00:00:03\", \"2020-06-01 00:00:09\", \"202…\n## $ stop_time        <chr> \"2020-06-01 00:17:46\", \"2020-06-01 01:03:33\", \"2020-06-01 00:17:06\", \"202…\n## $ start_station_id <chr> \"ny3419\", \"ny366\", \"ny389\", \"ny3255\", \"ny367\", \"ny248\", \"ny3232\", \"ny3263…\n## $ end_station_id   <chr> \"ny3419\", \"ny336\", \"ny3562\", \"ny505\", \"ny497\", \"ny247\", \"ny390\", \"ny496\",…\n## $ bike_id          <chr> \"39852\", \"37558\", \"37512\", \"39674\", \"21093\", \"39594\", \"43315\", \"16571\", \"…\n## $ user_type        <chr> \"Customer\", \"Subscriber\", \"Customer\", \"Customer\", \"Customer\", \"Subscriber…\n## $ birth_year       <chr> \"1997\", \"1969\", \"1988\", \"1969\", \"1997\", \"1990\", \"1938\", \"1995\", \"1971\", \"…\n## $ gender           <dbl> 2, 0, 2, 0, 2, 1, 2, 2, 2, 1, 1, 0, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n\n\nglimpse(ny_stations)\n## Rows: 1,010\n## Columns: 6\n## $ id        <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 2…\n## $ city      <chr> \"ny\", \"ny\", \"ny\", \"ny\", \"ny\", \"ny\", \"ny\", \"ny\", \"ny\", \"ny\", \"ny\", \"ny\", \"ny\", \"n…\n## $ stn_id    <chr> \"ny116\", \"ny119\", \"ny120\", \"ny127\", \"ny128\", \"ny143\", \"ny144\", \"ny146\", \"ny150\",…\n## $ name      <chr> \"W 17 St & 8 Ave\", \"Park Ave & St Edwards St\", \"Lexington Ave & Classon Ave\", \"B…\n## $ longitude <chr> \"-74.00149746\", \"-73.97803415\", \"-73.95928168\", \"-74.00674436\", \"-74.00297088\", …\n## $ latitude  <chr> \"40.74177603\", \"40.69608941\", \"40.68676793\", \"40.73172428\", \"40.72710258\", \"40.6…\n\n\n\n\nThe object type of a variable relates to its measurement level, though it is common to override the notional assignment of measurement levels depending on ise case. For example, we may which to convert the start_time and stop_time variables to a date-time format so that various time-related functions can be used. For efficient storage, we may wish to convert the station identifier variables as int types by removing the redundant “ny” text which prefaces end_station_id, end_station_id, stn_id. The geographic coordinates are currently stored as type chr. These could be regarded as quantitative variables, floating points with decimals. So converting to type dbl or as a POINT geometry type may be sensible.\nIn the 02-template.qmd file there are code chunks for doing these conversions. There are some slightly more involved data transform procedures in this code. Don’t fixate too much on these, but the upshot can be seen when running glimpse() on the converted data frames:\n\nglimpse(ny_trips)\n## Rows: 1,882,273\n## Columns: 10\n## $ id               <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 2…\n## $ trip_duration    <dbl> 1062, 3810, 1017, 226, 1437, 355, 99, 1810, 87, 2714, 2096, 1611, 529, 695, 206,…\n## $ start_time       <dttm> 2020-06-01 00:00:03, 2020-06-01 00:00:03, 2020-06-01 00:00:09, 2020-06-01 00:00…\n## $ stop_time        <dttm> 2020-06-01 00:00:03, 2020-06-01 00:00:03, 2020-06-01 00:00:09, 2020-06-01 00:00…\n## $ start_station_id <int> 3419, 366, 389, 3255, 367, 248, 3232, 3263, 390, 319, 237, 3630, 3610, 3708, 465…\n## $ end_station_id   <int> 3419, 336, 3562, 505, 497, 247, 390, 496, 3232, 455, 3263, 3630, 3523, 3740, 379…\n## $ bike_id          <int> 39852, 37558, 37512, 39674, 21093, 39594, 43315, 16571, 28205, 41760, 30745, 380…\n## $ user_type        <chr> \"Customer\", \"Subscriber\", \"Customer\", \"Customer\", \"Customer\", \"Subscriber\", \"Sub…\n## $ birth_year       <int> 1997, 1969, 1988, 1969, 1997, 1990, 1938, 1995, 1971, 1989, 1990, 1969, 1984, 19…\n## $ gender           <chr> \"female\", \"unknown\", \"female\", \"unknown\", \"female\", \"male\", \"female\", \"female\", …\n\n\nglimpse(ny_stations)\n## Rows: 1,010\n## Columns: 5\n## $ id        <dbl> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, …\n## $ stn_id    <int> 116, 119, 120, 127, 128, 143, 144, 146, 150, 151, 157, 161, 164, 167, 168, 173, 174, 19…\n## $ name      <chr> \"W 17 St & 8 Ave\", \"Park Ave & St Edwards St\", \"Lexington Ave & Classon Ave\", \"Barrow S…\n## $ longitude <dbl> -74.00150, -73.97803, -73.95928, -74.00674, -74.00297, -73.99338, -73.98069, -74.00911,…\n## $ latitude  <dbl> 40.74178, 40.69609, 40.68677, 40.73172, 40.72710, 40.69240, 40.69840, 40.71625, 40.7208…\n\n\n2.3.3 Transform\nTransform with dplyr\n\n\n\n\n\n\nTable 2.4:  dplyr funcitions (verbs) for manipulating data frames. \n \n function() \n    Description \n  \n\n\n filter() \n    Picks rows (observations) if their values match a specified criteria \n  \n\n arrange() \n    Reorders rows (observations) based on their values \n  \n\n select() \n    Picks a subset of columns (variables) by name (or name characteristics) \n  \n\n rename() \n    Changes the name of columns in the data frame \n  \n\n mutate() \n    Adds new columns (or variables) \n  \n\n group_by() \n    Chunks the dataset into groups for grouped operations \n  \n\n summarise() \n    Calculate single-row (non-grouped) or multiple-row (if grouped) summary values \n  \n\n ..and more \n     \n  \n\n\n\n\n\ndplyr is a most important package. It provides a grammar of data manipulation, with access to functions that can be variously combined to support most data processing and transformation activity. Once you become familiar with dplyr functions (or verbs) you will find yourself generating analysis templates to re-use whenever you work on a new dataset.\nAll dplyr functions work in the same way:\n\nStart with a data frame.\nPass some arguments to the function which control what you do to the data frame.\nReturn the updated data frame.\n\nSo every dplyr function expects a data frame and will always return a data frame.\nUse pipes |> with dplyr\n\ndplyr is most effective when its functions are chained together – you will see this shortly as we explore the New York bikeshare data. This chaining of functions can be achieved using the pipe operator (|>). Pipes are used for passing information in a program. They take the output of a set of code (a dplyr specification) and make it the input of the next set (another dplyr specification).\nPipes can be easily applied to dplyr functions, and the functions of all packages that form the tidyverse. We mentioned in Chapter 1 that ggplot2 provides a framework for specifying a layered grammar of graphics (more on this in Chapter 3). Together with the pipe operator (|>), dplyr supports a layered grammar of data manipulation.\n\ncount() rows\nThis might sound a little abstract so let’s use and combine some dplyr functions to generate statistical summaries on the New York bikeshare data.\nFirst we’ll count the number of trips made in Jun 2020 by gender. dplyr has a convenience function for counting, so we could run the code below, also in the 02-template.qmd for this chapter. The code block is commented to convey what each line achieves.\n\n # Take the ny_trips data frame.\nny_trips |> \n  # Run the count function over the data frame \n  # and set the sort parameter to TRUE.\n  count(user_type, sort=TRUE) \n##   user_type       n\n## 1 subscriber 1326513\n## 2   customer  592681\n\nThere are a few things happening in the count() function. It takes the gender variable from ny_trips, organises or groups the rows in the data frame according to its values (female | male | unknown), counts the rows and then orders the summarised output descending on the counts.\n\nsummarise() over rows\n\n\n\n\n\nTable 2.5:  A breakdown of aggregate functions commonly used with summarise(). \n \n Function \n    Description \n  \n\n\n n() \n    Counts the number of observations \n  \n\n n_distinct(var) \n    Counts the number of unique observations \n  \n\n sum(var) \n    Sums the values of observations \n  \n\n max(var)|min(var) \n    Finds the min|max values of observations \n  \n\n mean(var)|median(var)| ... \n    Calculates central tendency of observations \n  \n\n ... \n    Many more \n  \n\n\n\n\n\nOften you will want to do more than simply counting and you may also want to be more explicit in the way the data frame is grouped for computation. We’ll demonstrate this here with a more involved analysis of the usage data and with some key aggregate functions (Table 2.5).\nA common workflow is to combine group_by() and summarise(), and in this case arrange() to replicate the count() example.\n\n# Take the ny_trips data frame.\nny_trips |> \n  # Group by user_type.\n  group_by(user_type) |> \n    # Count the number of observations per group.\n    summarise(count=n()) |>\n    # Arrange the grouped and summarised (collapsed) rows \n    # according to count. \n    arrange(desc(count)) \n## # A tibble: 2 × 2\n##   user_type    count\n##   <chr>        <int>\n## 1 subscriber 1326513\n## 2 customer    592681\n\nIn ny_trips there is a variable measuring trip duration in seconds (trip_duration) and distinguishing casual users from those formally registered to use the scheme (user_type - customer vs. subscriber). It may be instructive to calculate some summary statistics to see how trip duration varies between these groups.\nThe code below uses group_by(), summarise() and arrange() in exactly the same way, but with the addition of other aggregate functions profiles the trip_duration variable according to central tendency (mean and standard deviation) and by user_type.\n\n# Take the ny_trips data frame.\nny_trips |> \n  mutate(trip_duration=trip_duration/60) |>\n  # Group by user type.\n  group_by(user_type) |> \n  # Summarise over the grouped rows, \n  # generate a new variable for each type of summary.\n  summarise( \n    count=n(),\n    avg_duration=mean(trip_duration),\n    median_duration=median(trip_duration),\n    sd_duration=sd(trip_duration),\n    min_duration=min(trip_duration),\n    max_duration=max(trip_duration)\n    ) |>\n  # Arrange on the count variable.\n  arrange(desc(count)) \n\n## # A tibble: 2 × 7\n##   user_type    count avg_duration median_duration sd_duration min_duration max_duration\n##   <chr>        <int>        <dbl>           <dbl>       <dbl>        <dbl>        <dbl>\n## 1 subscriber 1326513         20.1            14.4        110.         1.02       33090.\n## 2 customer    592681         44.1            23.3        393.         1.02       46982.\n\nAs each line is commented you hopefully get a sense of what is happening in the code above. You will notice that dplyr functions read like verbs, and this is a very deliberate design decision. With the code laid out as above – each dplyr verb occupying a single line, separated by a pipe (|>) – you can generally understand the code with a cursory glance. There are obvious benefits to this. Once you are familiar with dplyr it becomes very easy to read, write and share code.\n\n\n\n\n\n\nNote\n\n\n\nRemembering that pipes (|>) take the output of a set of code and make it the input of the next set, you will notice separate lines for each call to the pipe operator. This is good practice for supporting readability of your code.\n\n\nManipulate dates with lubridate\n\nLet’s continue this investigation of trips by user-type by profiling how usage varies over time. To do this we will need to work with date-time variables. The lubridate package provides various convenience functions for this.\nIn the code block below we extract the day of week and hour of day from the start_time variable using lubridate’s day accessor functions. Documentation on these can be accessed in the usual way (?<function-name>), but reading down the code it should be clear to you how this works. Next we count the number of trips made by hour of day, day of week and user-type. The summarised data frame will be re-used several times in our analysis, so we store it as an object with a suitable name (ny_temporal) using the assignment operator.\n\n# Create a hod dow summary by gender and assign it the name \"ny_temporal\".\n# Take the ny_trips data frame.\nny_temporal <- ny_trips |>  \n  mutate(\n    # Create a new column identify dow.\n    day=wday(start_time, label=TRUE), \n    # Create a new column identify hod.\n    hour=hour(start_time)) |> \n  # Group by day, hour, user_type.\n  group_by(user_type, day, hour) |> \n  # Count the grouped rows.\n  summarise(count=n()) |> \n  ungroup()\n\n\n\n\n\n\n\nNote\n\n\n\nWhether or not to store derived data tables, like the newly assigned ny_temporal, in a session is not an easy decision. You want to try to avoid cluttering your Environment pane with many data objects. Often when generating charts it is necessary to create these sorts of derived tables as input data (to ggplot2), which risks an unhelpfully large number of such tables. A general rule: if the derived table is to be used more than 3 times in a data analysis or is computationally intensive, assign it (<-) to an object.\n\n\nIn Figure 2.2 below the derived data are plotted. The template contains ggplot2 code for creating the graphic. Don’t obsess too much on it – this is covered extensively in the following chapter. The plot demonstrates a familiar weekday-weekend pattern of usage. Trip frequencies peak in the morning and evening rush hours during weekdays and mid/late-morning and afternoon during weekends. This is consistent with typical travel behaviour. Notice though that the weekday afternoon peak is much larger than the morning peak. There are several speculative explanations for this. Note that there are also obvious differences in the type of trips made by subscribers versus customers – the temporal signature for subscribers appears to match more closely what one would expect of commuting behaviour.\n\n\nFigure 2.2: Line charts generated with ggplot2. Plot data computed using dplyr and lubridate\n\n\n\n\n\n\n\n\nNote\n\n\n\nOur analysis is based on data from June 2020, a time when New York residents were emerging from lockdown. It would be instructive to compare with data from a non-Covid year. The fact that bikeshare is collected continuously makes this sort of behavioural change analysis possible. Check out Jo Wood’s work analysing Covid-related change in movement behaviours across a range of cities.\n\n\nRelate tables with join()\n\nTrip distance is not recorded directly in the ny_trips table, but may be important for profiling usage behaviour. Calculating trip distance is eminently achievable as the ny_trips table contains the origin and destination station of every trip and the ny_stations table contains coordinates corresponding to those stations. To relate the two tables, we need to specify a join between them.\nA sensible approach is to:\n\nSelect all uniquely cycled trip pairs (origin-destination pairs) that appear in the ny_trips table.\nBring in the corresponding coordinate pairs representing the origin and destination stations by joining on the ny_stations table.\nCalculate the distance between the coordinate pairs representing the origin and destination.\n\nThe code below is one way of achieving this.\n\n# Take the ny_trips data frame.\nod_pairs <- ny_trips |> \n  # Select trip origin and destination (OD) station columns \n  # and extract unique OD pairs.\n  select(start_station_id, end_station_id) |>  unique() |> \n  # Select lat, lon columns from ny_stations and join on origin column.\n  left_join(\n    ny_stations |> select(stn_id, longitude, latitude), \n    by=c(\"start_station_id\"=\"stn_id\")\n    ) |> \n  # Rename new lat, lon columns -- associate with origin station.\n  rename(o_lon=longitude, o_lat=latitude) |>  \n  # Select lat, lon columns from ny_stations and join on destination column.\n  left_join(\n    ny_stations |> select(stn_id, longitude, latitude), \n    by=c(\"end_station_id\"=\"stn_id\")\n    ) |> \n  # Rename new lat, lon columns and associate with destination station.\n  rename(d_lon=longitude, d_lat=latitude) |>  \n  # Compute distance calculation one row-at-a-time.\n  rowwise() |> \n  # Calculate distance and express in kms.\n  mutate(dist=geosphere::distHaversine(c(o_lat, o_lon), c(d_lat, d_lon))/1000) |> \n  ungroup()\n\nThe code block above introduces some new functions: select() to pick or drop variables, rename() to rename variables and a convenience function for calculating straight line distance from polar coordinates (distHaversine()). The key function to emphasise is the left_join(). If you’ve worked with relational databases, dplyr’s join functions will be familiar to you. In a left_join, all the values from the main table are retained, the one on the left – ny_trips, and variables from the table on the right (ny_stations) – are added. We specify explicitly the variable on which the tables should be joined with the by= parameter, station_id in this case. If there is a station_id in ny_trips that doesn’t exist in ny_stations then NA is returned.\nOther join functions provided by dplyr are in the table below. Rather than discussing each, I recommend consulting Chapter 13 of Wickham and Grolemund (2017).\n\n\n\n\n\nTable 2.6:  A breakdown of dplyr join functions. \n\n*_join(x, y) ...\n\n\n\n\n left_join() \n    all rows from x \n  \n\n right_join() \n    all rows from y \n  \n\n full_join() \n    all rows from both x and y \n  \n\n `semi_join()` \n    all rows from x where there are matching values in y, keeping just columns from x \n  \n\n inner_join() \n    all rows from x where there are matching values in y, return all combination of multiple matches in the case of multiple matches \n  \n\n anti_join \n    return all rows from x where there are not matching values in y, never duplicate rows of x \n  \n\n\n\n\n\n\n\nFigure 2.3: Histograms generated with ggplot2. Plot data computed using dplyr and lubridate\n\n\nFrom the newly created distance variable, we can calculate the average (mean) trip distance for the 1.9 milliom trips – 1.6km. This might seem very short, but remember that these are straight-line distances between pairs of docking stations. Ideally we would calculate network distances derived from cycle trajectories. A separate reason, discovered when generating a histogram on the dist variable, is that there are a large number of trips (c.124,000) that start and end at the same docking station. Initially these might seem to be unsuccessful hires – people failing to undock a bike for example. We could investigate this further by paying attention to the docking stations at which same origin-destination trips occur, as in the code block below.\n\nny_trips |> \n  filter(start_station_id==end_station_id) |> \n  group_by(start_station_id) |>  summarise(count=n()) |> \n  left_join(\n    ny_stations |>   select(stn_id, name), \n    by=c(\"start_station_id\"=\"stn_id\")\n    ) |> \n  arrange(desc(count))\n\n## # A tibble: 958 x 3\n##    start_station_id count name\n##    <chr>            <int> <chr>\n##  1 ny3423            2017 West Drive & Prospect Park West\n##  2 ny3881            1263 12 Ave & W 125 St\n##  3 ny514             1024 12 Ave & W 40 St\n##  4 ny3349             978 Grand Army Plaza & Plaza St West\n##  5 ny3992             964 W 169 St & Fort Washington Ave\n##  6 ny3374             860 Central Park North & Adam Clayton Powell Blvd\n##  7 ny3782             837 Brooklyn Bridge Park - Pier 2\n##  8 ny3599             829 Franklin Ave & Empire Blvd\n##  9 ny3521             793 Lenox Ave & W 111 St\n## 10 ny2006             782 Central Park S & 6 Ave\n## # … with 948 more rows\n\nAll of the top 10 docking stations are either in parks, near parks or located along river promenades. This coupled with the fact that these trips occur in much greater relative number for casual than regular users (Customer vs Subscriber) is further evidence that these are valid trips.\n\n2.3.3.1 Write functions of your own\nThrough most of the course we will be making use of functions written by others – mainly those developed for packages that form the tidyverse and therefore that follow a consistent syntax. However, there may be times where you need to abstract over some of your code to make functions of your own. Chapter 19 of Wickham and Grolemund (2017) presents some helpful guidelines around the circumstances under a the data scientist might write functions. Most often this is when you find yourself copy and pasting the same chunks of code with minimal adaptation.\nFunctions have three key characteristics:\n\nThey are (usually) named – the name should be expressive and communicate what the function does (we talk about dplyr verbs).\nThey have brackets <function()> usually containing arguments – inputs which determine what the function does and returns.\nImmediately followed by <function()> are {} used to contain the body – in this is code that performs a distinct task, described by the function’s name.\n\nEffective functions are short, perform single discrete operations and are intuitive.\nYou will recall that in the ny_trips table there is a variable called birth_year. From this we can derive cyclists’ approximate age. Below I have written a function get_age() for doing this. The function expects two arguments: yob – a year of birth as type chr; yref – a reference year. In the body, lubridate’s as.period function is used to calculate the time in years that elapsed, the value that the function returns. Once defined, and loaded into the session by being executed, it can be used (as below).\n\n# Depends on lubridate\nlibrary(lubridate)\n# Function for calculating time elapsed between two dates in years (age).\nget_age <- function(yob, yref) {\n    period <- as.period(interval(yob, yref),unit = \"year\")\n    return(period$year)\n}\n\nny_trips <- ny_trips |> \n  # Calculate age from birth_date.\n  mutate(\n    age=get_age(\n      as.POSIXct(birth_year, format=\"%Y\"), \n      as.POSIXct(\"2020\", format=\"%Y\")\n    ) \n  )\n\nWe can use the two new derived variables – distance travelled and age – in our analysis. In Figure 2.4, we explore how approximate travel speeds vary by age, trip distance and customer type. The code used to generate the summary data and plot is in the template file. Again the average speed calculation should be treated cautiously as it is based on straight line distances and it is likely that this will very much vary depending on whether the trip is made for ‘utilitarian’ or ‘leisure’ purposes. I have tried to do the latter by selecting trips that occur only on weekdays; those made by Subscriber cyclists might be most heavily associated with commuter trips. Additionally, due to the heavy subsetting, data become a little volatile for certain age groups and so the age variable is aggregated into 5-year bands.\nThere are some interesting and expected patterns. Subscribers make faster trips than do Customers, although this gap narrows as trip distance increases. Trips with a straight-line distance of 4.5km are non-trivial and so may be classed as ‘utilitarian’ even for non-regular Customers. There is a very slight effect of decreasing trip speed by age cycled for the longer trips. The volatility in the older age groups for trips >4.5km suggests we probably need more data and a more involved analysis to establish this. For example, it may be that the comparatively rare occurrence of trips in the 65-70 age group is made by only a small subset of cyclists. A larger dataset would result in a regression to the mean effect and negate any noise caused by outlier individuals.\n\n\nFigure 2.4: #| fig-cap: “Line charts generated with ggplot2. Plot data computed using dplyr and lubridate.\n\n\n\n2.3.4 Tidy\nThe ny_trips and ny_stations data already comply with the rules for tidy data (Wickham 2014). Each row in ny_trips is a distinct trip and each row in ny_stations a distinct station. However, it is common to encounter datasets that need to be reshaped. There are two key functions to learn here: pivot_longer() and pivot_wider(). pivot_longer() is used to tidy data in which observations are spread across columns, as in Table 2.3 (b) (the gapminder dataset). pivot_wider() is used to tidy data in which variables are spread across rows, as in Table 2.3 (c). You will find yourself using these functions not only for fixing messy data, but for flexibly reshaping data for use in ggplot2 specifications (more on this in Chapter 3 and 4) or joining tables.\nA quick breakdown of pivot_longer:\n\npivot_longer(\n  data,\n  # Columns to pivot longer (across rows).\n  cols, \n  # Name of the column to create from values held in spread *column names*.\n  names_to=\"name\",\n   # Name of column to create form values stored in spread *cells*. \n  values_to=\"name\"\n  )\n\nA quick breakdown of pivot_wider:\n\npivot_wider(\n  data,\n  # Column in the long format which contains what will be column names \n  # in the wide format.\n  names_from, \n  # Column in the long format which contains what will be values\n  # in the new wide format.\n  values_from \n  )\n\nIn the task you will be tidying some messy derived tables based on the bikeshare data using both of these functions, but we can demonstrate their purpose in tidying the messy gapminder data in Table 2.3. Remember that these data were messy as the observations by gender were spread across the columns:\n\nuntidy_wide\n##   country     year  f_cases m_cases f_population m_population\n##   <chr>       <chr> <chr>   <chr>   <chr>        <chr>\n## 1 Afghanistan 1999  447     298     9993400      9993671\n## 2 Afghanistan 2000  1599    1067    10296280     10299080\n## 3 Brazil      1999  16982   20755   86001181     86005181\n## 4 Brazil      2000  39440   41048   87251329     87253569\n## 5 China       1999  104007  108252  636451250    636464022\n## 6 China       2000  104746  109759  640212600    640215983\n\nFirst we need to gather the problematic columns with pivot_longer().\n\nuntidy_wide |> \n  pivot_longer(\n    cols=c(f_cases: m_population), \n    names_to=c(\"gender_count_type\"), \n    values_to=c(\"counts\")\n  )\n\n##   country     year  gender_count_type       counts\n##   <chr>       <chr> <chr>                   <chr>\n##  1 Afghanistan 1999  f_cases                447\n##  2 Afghanistan 1999  m_cases                298\n##  3 Afghanistan 1999  f_population           9993400\n##  4 Afghanistan 1999  m_population           9993671\n##  5 Afghanistan 2000  f_cases                1599\n##  6 Afghanistan 2000  m_cases                1067\n##  7 Afghanistan 2000  f_population           10296280\n##  8 Afghanistan 2000  m_population           10299080\n##  9 Brazil      1999  f_cases                16982\n## 10 Brazil      1999  m_cases                20755\n## # … with 14 more rows\n\nSo this has usefully collapsed the dataset by gender, we now have a problem similar to that in Table 2.3 (b) where observations are spread across the rows – in this instance cases and population are better treated as separate variables. This can be fixed by separating the gender_count_type variables and then spreading the values of the new count_type (cases, population) across the columns. Hopefully you can see how this gets us to the tidy gapminder data structure in Table 2.3 (a).\n\nuntidy_wide |> \n  pivot_longer(\n    cols=c(f_cases: m_population), \n    names_to=c(\"gender_count_type\"), \n    values_to=c(\"counts\")\n  ) |> \n  separate(\n    col=gender_count_type, \n    into=c(\"gender\", \"count_type\"), \n    sep=\"_\"\n  )\n\n##    country     year  gender count_type counts\n##    <chr>       <chr> <chr>  <chr>      <chr>\n##  1 Afghanistan 1999  f      cases      447\n##  2 Afghanistan 1999  m      cases      298\n##  3 Afghanistan 1999  f      population 9993400\n##  4 Afghanistan 1999  m      population 9993671\n##  5 Afghanistan 2000  f      cases      1599\n##  6 Afghanistan 2000  m      cases      1067\n##  7 Afghanistan 2000  f      population 10296280\n##  8 Afghanistan 2000  m      population 10299080\n##  9 Brazil      1999  f      cases      16982\n## 10 Brazil      1999  m      cases      20755\n## # … with 14 more rows\n\nuntidy_wide |> \n  pivot_longer(\n    cols=c(f_cases: m_population), \n    names_to=c(\"gender_count_type\"), \n    values_to=c(\"counts\")\n  ) |> \n  separate(\n    col=gender_count_type, \n    into=c(\"gender\", \"count_type\"), \n    sep=\"_\"\n  ) |> \n  pivot_wider(names_from=count_type, values_from=counts)\n\n##    country     year  gender cases  population\n##    <chr>       <chr> <chr>  <chr>  <chr>\n##  1 Afghanistan 1999  f      447    9993400\n##  2 Afghanistan 1999  m      298    9993671\n##  3 Afghanistan 2000  f      1599   10296280\n##  4 Afghanistan 2000  m      1067   10299080\n##  5 Brazil      1999  f      16982  86001181\n##  6 Brazil      1999  m      20755  86005181\n##  7 Brazil      2000  f      39440  87251329\n##  8 Brazil      2000  m      41048  87253569\n##  9 China       1999  f      104007 636451250\n## 10 China       1999  m      108252 636464022\n## 11 China       2000  f      104746 640212600\n## 12 China       2000  m      109759 640215983"
  },
  {
    "objectID": "02-data.html#conclusions",
    "href": "02-data.html#conclusions",
    "title": "2  Data Fundamentals",
    "section": "\n2.4 Conclusions",
    "text": "2.4 Conclusions\nDeveloping the vocabulary and technical skills to systematically describe and organise data is crucial to modern data analysis. This chapter has covered the fundamentals here: that data consist of observations and variables of different types (Stevens 1946) and that in order to work effectively with datasets, these data must be organised according to the rules of tidy data (Wickham 2014). Most of the content was dedicated to the techniques that enable these concepts to be operationalised. We covered how to download, transform and reshape a reasonably large set of data from New York’s Citibike scheme. In doing so, we generated insights that might inform further data collection and analysis activity. In the next chapter we will apply and extend this conceptual and technical knowledge as we introduce the fundamentals of visual data analysis and ggplot2’s grammar of graphics."
  },
  {
    "objectID": "03-visual.html",
    "href": "03-visual.html",
    "title": "3  Visualization Fundamentals",
    "section": "",
    "text": "By the end of this chapter you should gain the following knowledge and practical skills."
  },
  {
    "objectID": "03-visual.html#introduction",
    "href": "03-visual.html#introduction",
    "title": "3  Visualization Fundamentals",
    "section": "\n3.1 Introduction",
    "text": "3.1 Introduction\nThis chapter outlines the fundamentals of visualization design. It offers a position on what effective data graphics should do, before discussing in detail the processes that take place when creating data graphics. It will outline a framework – a vocabulary and grammar – for supporting this process which, combined with established knowledge around visual perception, can be used to describe, evaluate and create effective data graphics. Talking about a vocabulary and grammar of data and graphics may sound somewhat abstract, the preserve of Computer Scientists. However, through an analysis of 2019 General Election results data the chapter will demonstrate how these concepts are fundamental to visual data analysis."
  },
  {
    "objectID": "03-visual.html#concepts",
    "href": "03-visual.html#concepts",
    "title": "3  Visualization Fundamentals",
    "section": "\n3.2 Concepts",
    "text": "3.2 Concepts\nData graphics take numerous forms and are used in many different ways by scientists, journalists, designers and many more. Whilst the intentions of those producing data graphics varies, those that are effective generally have the following characteristics:\n\nRepresent complex datasets graphically, exposing structure, connections and comparisons that could not be achieved easily via other means.\nAre data rich, presenting many numbers in a small space.\nReveal patterns at several levels of detail, from broad overview to fine structure.\nHave elegance, emphasising dimensions of a dataset without extraneous details.\nGenerate an aesthetic response, encouraging people to engage with the data or question.\n\nGiven these characteristics, consider the data graphic below, which presents an analysis of the 2016 US Presidential Election, or the Peaks and Valleys of Trump and Clinton’s Support. The map is reproduced from an article in the The Washington Post, in the right margin is a standard choropleth map displaying the same county-level results data.\n\n\nFigure 3.1: Map of 2016 US presidential election results. Note that for copyright reasons this is a re-implementation of the original Washington Post piece in ggplot2.\n\n\nThe Washinngton Post graphic encodes many more data items than does the Choropleth (see Table 3.1 below). It is not simply the data density that makes the graphic successful. The authors usefully incorporate annotations and transformations in order to support comparison and emphasise structure. By varying the height of triangles according to the number of votes cast, the thickness according to whether or not the result for Trump/Clinton was a landslide and rotating the map 90 degrees, the very obvious differences between metropolitan, densely populated coastal counties that voted emphatically for Clinton and the vast number of suburban, provincial town and rural counties (everywhere else) that voted Trump, are exposed.\n\n\n\n\nData items encoded in the Washington Post Peaks and Valleys graphic as compared to standard choropleth map.\n \n Data item \n    Level \n    Choropleth \n    Peaks and Valleys \n  \n\n\n county location \n    Interval \n    ✔ \n    ✔ \n  \n\n county result \n    Nominal \n    ✔ \n    ✔ \n  \n\n state result \n    Nominal \n     \n    ✔ \n  \n\n county votes cast (~pop size) \n    Ratio \n     \n    ✔ \n  \n\n county result margin \n    Ratio \n     \n    ✔ \n  \n\n county result landslide \n    Nominal \n     \n    ✔ \n  \n\n\n\n\n\n\n\n3.2.1 Grammar of Graphics\n\nData graphics visually display measured quantities by means of the combined use of points, lines, a coordinate system, numbers, symbols, words, shading, and color.\nTufte (1983)\n\nIn evidence in the Washington Post graphic is a judicious mapping of data to visuals, underpinned by a secure understanding of analysis context. This act of carefully considering how best to leverage visual systems given the available data and analysis priorities is key to designing effective data graphics.\nIn the late 1990s Leland Wilkinson, a Computer Scientist and Statistician, introduced the Grammar of Graphics (1999) as an approach that captures this process of turning data into visuals. Wilkinson (1999)’s thesis is that if graphics can be described in a consistent way according to their structure and composition, then the process of generating graphics of different types can be systematised. This has obvious benefits for building visualization toolkits. Different chart types and combinations can be specified systematicaly, thereby formalising the process of data visualization design. Tableau, vega-lite and ggplot2 are all underpinned by Grammar of Graphics thinking.\nWilkinson (1999)’s grammar separates the construction of a data graphic into a series of components. Below are the components of the Layered Grammar of Graphics on which ggplot2 is based (Wickham 2010), adapted from Wilkinson (1999)’s original work.\n\n\nFigure 3.2: Components of Wickham (2010)’s Layered Grammar of Graphics.\n\n\nThe seven components in Figure 3.2 are together used to create ggplot2 specifications. The aspects to highlight at this stage are those in emphasis, which are required in any ggplot2 specification. The data containing the variables of interest, the marks to be used to represent data (geom) and the visual channels through which variables are to be encoded (mapping=aes(...)).\nTo demonstrate this, let’s generate some scatterplots based on the 2019 General Election data. Two variables worth exploring for association here are: con_1719, the change in Conservative vote share by constituency between 2017-2019, and leave_hanretty, the size of the Leave vote in the 2016 EU referendum, estimated at Parliamentary Constituency level (see Hanretty 2017).\n\n\nFigure 3.3: Plots, grammars and underlying ggplot2 specifications for the scatterplot.\n\n\nIn Figure 3.3 are three plots and underlying ggplot2 specifications. Reading-off the graphics and the associated code, you should get a feel for how ggplot2 specifications are constructed:\n\nWe start with a data frame, in this case each observation is an electoral result for a Parliamentary Constituency. This is passed to the ggplot2 spec using the pipe operator (|>). We also identify the variables to encode and their measurement type. Remembering the previous chapter, both con_1719 and leave_hanretty are ratio scale variables.\nNext is the encoding (mapping=aes()), which determines how the data are to be mapped to visual channels. A scatterplot is a 2D representation in which horizontal and vertical position varies in a meaningful way, in response to the values of a data set. Here the values of leave_hanretty are mapped along the x-axis and the values of con_1719 are mapped along the y-axis.\nFinally, we represent individual data items with marks using the geom_point geometry.\n\nIn the middle plot, the grammar is updated such that the points are coloured according to winning_party, a variable of type categorical nominal. In the bottom plot constituencies that flipped from Labour-to-Conservative between 2017-19 are emphasised by varying the transparency (alpha) of points. flipped is labelled an ordinal variable, but strictly it is a nominal (binary) variable. Due to the way it is encoded in the plot, constituencies that flipped are given greater visual emphasis and so it is perhaps appropriate to call flipped an ordinal variable.\n\n\n\n\n\n\nOn ggplot2 ‘specifications’\n\n\n\nIt is understandable if at this stage the specifications in Figure 3.3 still seem alien to you. We will be updating, expanding and refining ggplot2 specifications throughout the book to support all aspects of modern data analysis: from data cleaning and exploratory analysis through to model evaluation and communication.\n\n\n\n3.2.2 Marks and visual channels\n\nEffective data visualization design is concerned with representing data through marks and visual channels in a way that best conveys the properties of the data that are to be depicted.\nvia Jo Wood\n\nYou might have noticed that in our descriptions of ggplot2 specifications we introduced marks as another term for geometry and visual encoding channels as another term for aesthetics. We also paid special attention to the data types that are being encoded. Marks are graphical elements such as bars, lines, points, ellipses that can be used to represent data items. In ggplot2, these are accessed through the functions prefaced with geom_*. Visual channels are attributes such as colour, size, position that, when mapped to data, control the appearance of marks in response to the values of a dataset.\nMarks and channels are terms that appear in the interface of Tableau and in vega-lite specifications. They are also used widely in Information Visualization, an academic discipline devoted to the study of data graphics, and most notably by Tamara Munzner (2014) in her textbook Visualization Analysis and Design. Munzner (2014)’s work synthesises over foundational research in Information Visualization and Cognitive Science testing how effective different visual channels are at supporting different tasks.\n\n\n\n\n\nFigure 3.4: Visual channels to which data items can be encoded, as they appear in Munzner (2014).\n\n\nFigure 3.4 is taken from Munzner (2014) and lists the main visual channels with which data might be encoded. The grouping and order of the figure is meaningful. Channels are grouped according to the tasks to which they are best suited and then ordered according to their effectiveness at supporting those tasks. To the left are magnitude:order channels – those that are best suited to tasks aimed at quantifying data items. To the right are identity:category channels – those that are most suited to supporting tasks that involve isolating, grouping and associating data items.\nWe can use this organisation of visual channels to make decisions on appropriate encodings given a variable’s measurement level. If we wished to convey the magnitude of something, for example a quantitative (ratio) variable like the size of the Conservative vote share in a constituency, we might select a channel that has good quantitative effectiveness – position on a common scale or length. If we wished to also effectively identify and associate constituencies according to the political party that was elected, a categorical nominal variable, we might select a channel that has good associative properties such as colour hue.\n\n3.2.3 Evaluating designs\nThe effectiveness rankings of visual channels in Figure Figure 3.4 are not simply based on Munzner’s preference. They are informed by detailed experimental work (Cleveland and McGill (1984), later replicated by Heer and Bostock (2010)), which involved conducting controlled experiments testing people’s ability to make judgements from graphical elements. We can use Figure 3.4 to help make decisions around which data item to encode with which visual channel. This is particularly useful when designing data-rich graphics, where several data items are to be encoded simultaneously (e.g. Beecham et al. 2021). The figure also offers a low cost way of evaluating different designs against their encoding effectiveness.\n\n\n\n\nEncoding effectiveness for Washington Post graphic that emphasises vote margin and size of counties using triangle marks.\n \n Data item \n    Type \n    Channel \n    Rank \n  \n\nMagnitude:Order\n\n Location \n    interval \n    position in x,y \n    1. quant \n  \n\n Votes cast (~pop size) \n    ratio \n    length \n    3. quant \n  \n\n Margin \n    ratio \n    length \n    3. quant \n  \n\n Landslide \n    ordinal \n    area \n    5. quant \n  \nIdentity:Category\n\n Winner \n    nominal \n    colour hue \n    2. cat \n  \n\n\n\n\nTo illustrate this, we can use Munzner’s ranking of channels to evaluate the Washington Post graphic discussed in Figure 3.1. Table 3.2 provides a summary of the encodings used in the graphic. US counties are represented using a peak-shaped mark. The key purpose of the graphic is to depict the geography of voting outcomes, and the most effective quantitative channel – position on an aligned scale – is used to order the county marks with a 2D geographic arrangement. With the positional channels taken, the two quantitive measures, votes cast and result margin, are encoded with the next highest ranked channel, 1D length: height varies according to number of votes cast and width according to result margin. The marks are additionally encoded with two categorical variables: whether the county-level result was a landslide and also the ultimate winner. Since the intention is to give greater visual saliency to counties that resulted in a landslide, this as an ordinal variable, encoded with a quantitative channel: 2D area. The winning party, a categorical nominal variable, is encoded using colour hue.\nEach of the encoding choices used in the graphic follow conventional wisdom in that data items are encoded using visual channels that are appropriate to their measurement level. Glancing down the “rank” column, the graphic has high effectiveness. Whilst technically spatial region is the most effective channel for encoding nominal data, it is already in use as the marks are arranged by geographic position. Additionally, it makes sense to distinguish Republican and Democrat wins using the colours with which they are always represented. Given the fact that the positional channels represent geographic location, length to represent votes cast and vote margin, the only superior visual channel to 2D area that could be used to encode the landslide variable is orientation. There are very good reasons for not varying the orientation of the arrow marks. Most obvious is that this would undermine perception of length encodings used to represent the vote margin (width) and absolute vote size (height).\n\n\n\n\n\n\nVisualization design trade-offs\n\n\n\nData visualization design almost always involves trade-offs. When deciding on a design configuration, it is necessary to prioritise data and analysis tasks, then match representations and encodings that are most effective to the tasks that have the greatest priority. This constrains the encoding options for less important data items and tasks. Good visualization design is sensitive to this interplay between tasks, data and encoding.\n\n\n\n3.2.4 Symbolisation\n\nSymbolization is the process of encoding something with meaning in order to represent something else. Effective symbol design requires that the relationship between a symbol and the information that symbol represents (the referent) be clear and easily interpreted.\nWhite (2017)\n\nImplicit in the discussion above, and when making design decisions, is the importance of symbolisation. From the original Washington Post article, the overall pattern that can be discerned is of population-dense coastal and metropolitan counties voting Democrat – densely-packed, tall, wide and blue  marks – contrasted against population-sparse rural and small town areas voting Republican – short, wide and red  marks. The graphic evokes a distinctive landscape of voting behaviour, emphasised by its caption: “The peaks and valleys of Trump and Clinton’s support”.\nSymbolisation is used equally well in a variant of the graphic emphasising two-party Swing between the 2012 and 2016 elections (Figure 3.5). Each county is represented as a | mark. The Swing variable is then encoded by continuously varying mark angles: counties swinging Republican are angled to the right /; counties swinging Democrat are angled to the left \\. Although angle is a less effective channel at encoding quantities than is length, there are obvious links to the political phenomena in the symbolisation – angled right for counties that moved to the right politically. Additionally, the variable itself might be regarded as cyclic – or at least it has a ceiling with an important mid-point that requires emphasis. It is worth taking a second look at the full graphic here. Since there is spatial autocorrelation in county voting, we quickly assemble from the graphic dominant patterns of Swing to the Republicans (Great Lakes, rural East Coast), predictable Republican stasis (the mid west) and to detect more isolated, locally exceptional Swings to the Democrats (rapidly urbanising counties in the deep south).\n\n\nFigure 3.5: Map of swing in 2016 US presidential election results. Note that for copyright reasons this is a re-implementation of the original Washington Post piece in ggplot2.\n\n\n\n3.2.5 Checking perceptual rankings\nWe mentioned that Munzner’s effectiveness ordering of visual channels is informed by empirical evidence – controlled experiments that examine perceptual abilities at making judgements from graphical primitives. It is worth elaborating a little on this experimental work, and on how established knowledge in Cognitive Science can be used to inform design choices.\nCleveland (1993) emphasises three perceptual activities that take place when we make sense of data graphics:\n\n\nDetection : the element of the graphic must be easily discernible.\n\nAssembly : the process of identifying patterns and structure within the graphical elements of the visualization.\n\nEstimation : the process of making comparisons of the magnitudes of data items from the visual elements used.\n\nDetection is especially important for selective and associative tasks that involve isolating and grouping data items, whilst estimation is necessary for tasks that are orderable and quantitative, involving the ranking and reading-off of quantities.\n\n3.2.6 Preattentive processing\nA useful distinction when considering graphical cognition is between processes that are attentive and pre-attentive (Ware 2008). Attentive processing describes the conscious processing that happens when we attempt to make sense of a visual field. Preattentive processing happens unconsciously and is the type of cognitive processing that allows something to be understood ‘at a glance’. Visual items that immediately pop-out to us induce preattentive processing.\nUsing preattentive processing to make some things on a data graphic more easily detectible than others is useful for supporting selective and associative tasks; in the Washington Post graphic, the use of colour hue to differentiate and group together counties that voted Republican or Democrat. Preattentive processing can also apply to assembly. We naturally construct and assemble patterns that are smooth and continuous when perceiving a graphic and so deviations from this continuity are often attended to unconsciously. An example here would be those urbanising counties in the deep South, which were locally exceptional in swinging to Democrat (to the left).\n\n3.2.7 Testing visual encoding channels\nWe can test this preattentive processing by using visual encoding channels to assist a task that requires us to select and associate visual items. Below are a set of data graphics containing 200 numbers. For each graphic try to scan across the number, isolate or select the number 3, then group or associate the 3s together and count the number of instances that they occur. Speed is important here, so work as quickly as you can.\nFirst, a set of numbers without applying any special encoding to the number 3.\n\n\n\n\n\n\nTask: click to expose graphic, then count number of occurrences of #3\n\n\n\n\n\n\n\nFigure 3.6: Encoding: none.\n\n\n\n\n\nIf you were racing to complete the task, I imagine you found it moderately stressful. Let’s explore using visual encoding to off-load some of this cognitive effort. We’ll start with a visual channel that does not have particularly strong preattentive properties: area.\n\n\n\n\n\n\nTask: click to expose graphic, then count number of occurrences of #3\n\n\n\n\n\n\n\nFigure 3.7: Encoding: area.\n\n\n\n\n\nUsing visualization to support the task makes it easier, but let’s explore some visual channels that have even more powerful properties. I mentioned that tilt/angle has preattentive properties where the data items to be emphasised deviate from some regular pattern. In the graphic below, the number 3 is encoded with tilt or angle.\n\n\n\n\n\n\nTask: click to expose graphic, then count number of occurrences of #3\n\n\n\n\n\n\n\nFigure 3.8: Encoding: tilt/angle.\n\n\n\n\n\nThis is in fact more challenging than the size encoding. This may be because the geometric patterns of the marks used (numbers) is being varied and so this limits the extent to which we unconsciously perceive smoothness and continuity (e.g. limits assembly).\nNext we’ll use a visual channel with known effectiveness at assisting selective and associative tasks. Colour hue appears as the second-ranked most effective in Munzner (2014)’s ordering.\n\n\n\n\n\n\nTask: click to expose graphic, then count number of occurrences of #3\n\n\n\n\n\n\n\nFigure 3.9: Encoding: colour hue.\n\n\n\n\n\nFinally, though a slightly contrived example, we can use the top-ranked channel according to Munzner (2014): spatial region.\n\n\n\n\n\n\nTask: click to expose graphic, then count number of occurrences of #3\n\n\n\n\n\n\n\nFigure 3.10: Encoding: spatial region.\n\n\n\n\n\n\n3.2.8 Estimation\nThe informal tests above hopefully persuade you of Munzner (2014)’s ordering of identity:category channels in the right side of Figure 3.4. The ranking of magnitude:order channels is also informed by established theory and evidence.\nWhen using data graphics to communicate quantities, certain visual channels are known to induce biases. Psychophysics is a branch of psychology that develops methods aimed at capturing the often non-linear relationship between the properties of a stimuli such as symbol length, area or colour value, and their perceived response. Stevens’ power law is an empirically-derived relationship that models this effect. The power function takes the form:\nR=kS_n\nWhere S is the magnitude of the stimulus, for example, the absolute length of a line or area of a circle, R is the response, the perceived length and area, and _n is the power law exponent that varies with the type of stimulus. If there is a perfect linear mapping between the stimulus and response, n is 1.\nStevens and Guirao (1963)’ experimental work involved varying the length of lines and areas of squares and deriving power functions for their perception. For length, an exponent of ~1.0 was estimated; for area an exponent of 0.7. So whilst variation in length is accurately perceived, we underestimate the size of areas as they increase. Flannery (1971)’s work, which was concerned with the perception of quantities in graduated point maps, estimated an exponent of 0.87 for the perception of circle size.\nExperimental findings vary and so these models of human perception are also subject to variation. Nevertheless, corrections can be applied. In cartography a Flannery compensation is used when representing quantities with area.\n\n\nFigure 3.11: Differences in power law exponents for the perception of variation in length and area.\n\n\n\n\n\n\n\n\nGraphical perception of inference\n\n\n\nExperimental work that tries to understand how encoded quantities are perceived is clearly important. But we use data graphics to do much more than estimate single quantities. If data graphics are to serve as tools for analysis, we also need some confidence that the inferences made when studying data using graphics are accurate and reliable. In the Information Visualization domain, experimental work has recently been published exploring the perception of statistical quantities – location and dispersion (Correll and Gleicher 2014), correlation (Rensink and Baldridge 2010; Harrison et al. 2014; Kay and Heer 2016) and spatial autocorrelation (Klippel, Hardisty, and Li 2011; Beecham et al. 2017) – in commonly used chart types. More on this later.\n\n\n\n3.2.9 Colour\nAs demonstrated in the section on preattentive processing, colour is a very powerful visual channel. When considering how to encode data with colour, it is helpful to consider three properties:\n\n\nHue : what we generally refer to as “colour” in everyday life – red, blue green.\n\nSaturation : how much of a colour there is.\n\nLuminance/Brightness : how dark or light a colour is.\n\nThe ultimate rule when using colour in data graphics is to use properties of colour that match the properties of the data. Categorical nominal data – data that cannot be easily ordered – should be encoded using discrete colours with no obvious order; so colour hue. Categorical ordinal data – data whose categories can be ordered – should be encoded with colours that contain an intrinsic order; saturation or brightness, usually allocated into gradients. Quantitative data – data that can be ordered and contain values on a continuous scale – should also be encoded with colours that contain an intrinsic order; saturation or brightness, expressed on a continuous scale.\nAs we will discover shortly, these principles are applied by default in ggplot2, along with access to perceptually uniform schemes.\n\n\n\n\n\n\nOn colour\n\n\n\nThere are many considerations when using colour to support visual data analysis and communication. Lisa Charotte-Rost’s Guide to Colours in Data Visualization is an excellent outline of the decision-space."
  },
  {
    "objectID": "03-visual.html#sec-techniques",
    "href": "03-visual.html#sec-techniques",
    "title": "3  Visualization Fundamentals",
    "section": "\n3.3 Techniques",
    "text": "3.3 Techniques\nThe technical component of this chapter analyses data from the 2019 UK General Election, reported at Parliamentary Constituency level. After importing and describing the dataset, we will generate data graphics that expose patterns in voting behaviour by writing ggplot2 specifications.\n\nDownload the 03-template.qmd file for this chapter and save it to the reports folder of your vis4sds project.\nOpen your vis4sds project in RStudio and load the template file by clicking File > Open File ... > reports/03-template.qmd.\n\n\n3.3.1 Import\nThe template file lists the required packages – tidyverse, sf and also parlitools. Installing parlitools brings down the 2019 UK General Election dataset, along with other constituency-level datasets. Loading it with library(parlitools) makes these data available to your R session.\nThe data frame containing 2019 UK General Election data is called bes_2019. This stores results data released by House of Commons Library. We can get a quick overview with a call to glimpse(<dataset-name>). The dataset’s variables are also described on the parlitools web pages. You will notice that bes_2019 contains 650 rows, one for each Parliamentary Constituency, and 118 columns. Contained in the columns are variables reporting vote numbers and shares for the main political parties for 2019 and 2017 General Elections, as well as names and codes (IDs) for each Parliamentary Constituency and the county, region and country in which they are contained. You might want to count the number of counties and regions in the UK, and the number of constituencies contained by counties and regions, using some of the dplyr functions introduced in the previous chapter – for example with calls to group_by() and count().\nThe aim of this analysis is to increase our familairity with ggplot2 specifications by replicating some of the visual data analysis of the 2019 UK General Election in Beecham (2020), inspired by the Washington Post graphic. For this we need to calculate an additional variable, Butler Swing (Butler and Van Beek 1990). This represents the average change in share of the vote won by two parties contesting successive elections. Code for calculating this variable (named swing_con_lab) is in the 03-template.qmd.\nAlthough initially intuitive, the measure takes a little interpretation. A Swing to the Conservatives, which we observe most often in this dataset, could happen in three ways:\n\nAn increase in Conservative vote share and a decrease in Labour vote share.\nAn increase in both Conservative and Labour vote share, but with the Conservative increase outstripping that of Labour’s.\nA decrease in both Conservative and Labour vote share, but with the Conservative decline being less severe than that of Labour’s.\n\nDifferent from the US where “third parties” play a negligible role, scenarios 2 and 3 do occur in the UK. You will notice that swing_con_lab is a signed value: positive indicates a Swing to Conservative, negative a Swing to Labour.\nThe only other dataset to load is a .geojson file containing the geometries of constituencies, collected originally from ONS Open Geography Portal and simplified using mapshaper. This is a special class of data frame containing a Simple Features geometry column.\n\n3.3.2 Summarise\nYou may be familiar with the result of the 2019 General Election: a landslide Conservative victory that confounded expectations. To start, we can quickly compute some summary statistics around the vote. In the code block below, we count the number of seats won and overall vote share by party. For the latter, the code is a little more elaborate than we might wish at this stage. We needed to reshape the data frame using pivot_wider() such that each row represents a vote for a party in a constituency. From here the vote share for each party can be easily computed.\nWhilst the Conservative party hold 56% of constituencies, they won only 44% of the vote share. The equivalent figures for Labour are 31% and 32% respectively. Incidentally, whilst the Conservatives increased their share of constituencies from 2017 (where they had just 317, 49% of constituencies) their vote share increase was reasonably small – in 2017 they gained 43% of the vote.\n\n# Number of constituencies won by party.\nbes_2019 |>\n  group_by(winner_19) |>\n  summarise(count=n()) |>\n  arrange(desc(count))\n## # A tibble: 11 x 2\n##    winner_19                        count\n##    <chr>                            <int>\n##  1 Conservative                       365\n##  2 Labour                             202\n##  3 Scottish National Party             48\n##  4 Liberal Democrat                    11\n##  5 Democratic Unionist Party            8\n##  6 Sinn Fein                            7\n##  7 Plaid Cymru                          4\n##  8 Social Democratic & Labour Party     2\n##  9 Alliance                             1\n## 10 Green                                1\n## 11 Speaker                              1\n\n# Share of vote by party.\nbes_2019 |>\n  # Select cols containing vote counts by party.\n  select(constituency_name, total_vote_19, con_vote_19:alliance_vote_19, region) |> \n  # Pivot to make each row a vote for a party in a constituency.\n  pivot_longer(cols=con_vote_19:alliance_vote_19, names_to=\"party\", values_to=\"votes\") |> \n  # Use some regex to pull out party name.\n  mutate(party=str_extract(party, \"[^_]+\")) |> \n  group_by(party) |>\n  summarise(vote_share=sum(votes, na.rm=TRUE)/sum(total_vote_19)) |>\n  arrange(desc(vote_share))\n\n## # A tibble: 12 x 2\n##    party    vote_share\n##    <chr>         <dbl>\n##  1 con         0.436\n##  2 lab         0.321\n##  3 ld          0.115\n##  4 snp         0.0388\n##  5 green       0.0270\n##  6 brexit      0.0201\n##  7 dup         0.00763\n##  8 sf          0.00568\n##  9 pc          0.00479\n## 10 alliance    0.00419\n## 11 sdlp        0.00371\n## 12 uup         0.00291\n\nBelow are some summary statistics computed over the newly created swing_con_lab variable. As the Conservative and Labour votes are negligible in Northern Ireland, it makes sense to focus on Great Britain for our analysis of Conservative-Labour Swing and so the first step in the code is to create a new data frame filtering out Northern Ireland.\n\ndata_gb <- bes_2019 |>\n  filter(region != \"Northern Ireland\") |>\n  mutate(\n    swing_con_lab=0.5*((con_19-con_17)-(lab_19-lab_17)),\n    # Recode to 0 Chorley (incoming speaker), \n    # Buckingham (outgoing speaker)  uncontested seat.\n    swing_con_lab=if_else(constituency_name %in% c(\"Chorley\", \"Buckingham\"),0,swing_con_lab)\n  )\n\ndata_gb |>\n  summarise(\n    min_swing=min(swing_con_lab),\n    max_swing=max(swing_con_lab),\n    median_swing=median(swing_con_lab),\n    num_swing=sum(swing_con_lab>0),\n    num_landslide_con=sum(con_19>50, na.rm=TRUE),\n    num_landslide_lab=sum(lab_19>50, na.rm=TRUE)\n    )\n\n## # A tibble: 1 x 6\n##   min_swing max_swing median_swing num_swing num_landslide_con num_landslide_lab\n##       <dbl>     <dbl>        <dbl>     <int>             <int>             <int>\n## 1     -6.47      18.4         4.44       599               280               120\n\n\n3.3.3 Plot distributions\n\n\n\n\nHistograms of Swing variable.\n\n\n\n\nLet’s get going with some ggplot2 specifications by plotting some of these variables. Below is the code for plotting a histogram of the Swing variable.\n\ndata_gb |>\n  ggplot(mapping=aes(swing_con_lab)) +\n  geom_histogram()\n\nA reminder of the general form of the ggplot2 specification:\n\nStart with some data: data_gb.\nDefine the encoding: mapping=aes(). In this case, we want to summarise over the swing_con_lab variable.\nSpecify the marks to be used: geom_histogram() in this case.\n\nDifferent from the scatterplot example, there is more happening in the internals of ggplot2 when creating a histogram. Technically geom_histogram() is what Munzner (2014) would describe as a chart idiom rather than a mark (geometric primitive). The Swing variable is partitioned into bins and observations in each bin are counted. The x-axis (bins) and y-axis (counts by bin) is therefore derived from the supplied variable (swing_con_lab).\nBy default the histogram’s bars are given a grey colour. To set them to a different colour, add a fill= argument to geom_histogram(). In the code block below, colour is set using hex codes – \"#003c8f\", based on the theme for this book. The term set and not map or encode is used for principled reasons. Any part of a ggplot2 specification that involves encoding data – mapping data to a visual channel – should be specified through the mapping=aes() argument. Anything else, for example changing the default colour, thickness and transparency of marks, needs to be set outside of this argument.\n\ndata_gb |>\n  ggplot(mapping=aes(swing_con_lab)) +\n  geom_histogram(fill=\"#003c8f\") +\n  labs(\n    title=\"Butler two-party Labour-Conservative Swing\",\n    subtitle=\"-- GB Constituencies 2019 versus 2017 election\",\n    caption=\"Data published by House of Commons Library, accessed via `parlitools`\",\n    x=\"Swing\", y=\"count\"\n  )\n\nYou might have noticed that different elements of a ggplot2 specification are added (+) as layers. In the example above, the additional layer of labels (labs()) is not intrinsic to the graphic. However, often you will add layers that do affect the graphic itself. For example, the scaling of encoded values (e.g. scale_*_continuous()) or whether the graphic is to be conditioned on another variable to generate small multiples for comparison (e.g. facet_*()).\n\n\n\n\n\n\nOn histograms\n\n\n\nRead this design exposition by Lunzner and McNamara, 2020 for an excellent discussion of the analysis and design considerations when working with histograms. There are of course other geoms for summarising over 1D distributions: geom_boxplot(), geom_dotplot(), geom_violin().\n\n\nFaceting by region\n\n\nFigure 3.12: Histograms of Swing variable, grouped by region.\n\n\nAdding a call to facet_*(), we can quickly compare how Swing varies by region (as in Figure B.2). The plot is annotated with the median value for Swing (4.4) by adding a vertical line layer (geom_vline()) and setting its x-intercept at this value. From this, there is some evidence of a regional geography to the 2019 vote: London and Scotland are particularly distinctive in containing relatively few constituencies swinging greater than the expected midpoint; North East, Yorkshire & The Humber, and to a lesser extent West and East Midlands, appear to show the largest relative number of constituencies swinging greater than the midpoint. It was this graphic, especially the fact that London and Scotland look different from the rest of the country, that prompted the scatterplots in Figure 3.3 comparing gain in Conservative vote shares against the Brexit vote.\n\n3.3.4 Plot ranks/magnitudes\n\n\nFigure 3.13: Plots of vote shares by party.\n\n\nPreviously we calculated overall vote share by Political Party. We could continue the exploration of votes by region, re-using this code to generate plots displaying quantities but also comparing by region, using marks and encoding channels that are suitable for magnitudes.\nTo generate a bar chart similar to the left of Figure 4.3 the ggplot2 specification would be:\n\ndata_gb |>\n  # The code block summarising vote by party.\n  <some dplyr code> |>\n  ...  |>\n  # Data frame of vote share by party, piped to ggplot2.\n  <summarised data frame>  |>\n  # Ordinal x-axis (party, reordered), Ratio y-axis (vote_share).\n  ggplot(aes(x=reorder(party, -vote_share), y=vote_share)) +\n  # Set colour by website theme.\n  geom_col(fill=\"#003c8f\")\n\nA quick breakdown of the specification:\n\n\nData: This is the summarised data frame in which each row is a political party and the column describes the vote share recorded for that party.\n\nEncoding: We have dropped the call to mapping=. ggplot2 always looks for aes() and so can save some code clutter. In this case we are mapping party to the x-axis, a categorical variable made ordinal by the fact that we reorder the axis left-to-right descending according to vote_share. vote_share is mapped to the y-axis – so encoded using bar length, on an aligned scale, an effective channel for conveying magnitudes.\n\nMarks: geom_col() for generating the bars.\n\nSetting: Again, we’ve set bar colour according to the website theme and included titles and captions. Optionally we add a coord_flip() layer in order to display the bars horizontally. This makes the category axis labels easier to read and also seems more appropriate for the visual “ranking” of bars.\n\n\n\n\n\n\n\nggplot2 themes\n\n\n\nggplot2 themes control the appearance of all non-data items – font sizes and types, gridlines, axis labels. Checkout the complete list of ggplot2’s default themes. If you like the look of the BBC’s in-house data graphics, explore their Data Journalism cookbook. We’d recommend working through the cookbook as it is a great resource for distilling many of the non-data-related decisions that are made when communicating graphically.\n\n\nFaceting by region\n\n\nFigure 3.14: Plots of vote shares by party and region.\n\n\nIn Figure 3.14 the graphic is faceted by region. This requires an updated derived dataset grouping by vote_share and region and of course adding a faceting layer (geom_facet(~region)) to the ggplot2 specification. The graphic is more data-rich, but additional cognitive effort is required in relating the bars representing political parties between different graphical subsets. We can assist this identify and associate task by encoding the bars with an appropriate visual channel: colour hue. The ggplot2 specification for this is as you would expect – we add a mapping to geom_col() and pass the variable name party to the fill argument (aes(fill=party)).\n\n<derived_data> |>\n  ggplot(aes(x=reorder(party, vote_share), y=vote_share)) +\n  geom_col(aes(fill=party)) +\n  coord_flip() +\n  facet_wrap(~region)\n\nTrying this for yourself, you will observe that the ggplot2 internals do some thinking for us. Since party is a categorical variable, a categorical (hue-based) colour scheme is automatically applied. Try passing a quantitative variable (fill=vote_share) and see what happens.\nClever as this is, when encoding political parties with colour symbolisation is important. More control over the encoding is necessary in order to specify the colours with which parties are most commonly represented. We can override ggplot2’s default colour by adding a scale_fill_manual() layer into which a vector of hex codes describing the colour of each political party is passed (party_colours). We also need to tell ggplot2 which element of party_colours to apply to which value of party. In the code below, a derived table is generated summarising vote_share by political party and region. In the final line the party variable is recoded as a factor. You might recall from the last chapter that factors are categorical variables of fixed and orderable values, called levels. The call to mutate() recodes party as a factor variable and orders the levels according to overall vote share.\n\n# Generate derived data.\ntemp_party_shares_region <- data_gb |>\n  select(\n    constituency_name, region, total_vote_19, con_vote_19:alliance_vote_19\n    ) |>\n  pivot_longer(\n    cols=con_vote_19:alliance_vote_19, \n    names_to=\"party\", values_to=\"votes\"\n    ) |>\n  mutate(party=str_extract(party, \"[^_]+\")) |>\n  group_by(party, region) |>\n  summarise(vote_share=sum(votes, na.rm=TRUE)/sum(total_vote_19)) |>\n  filter(party %in% c(\"con\", \"lab\", \"ld\", \"snp\", \"green\", \"brexit\", \"pc\")) |>\n  mutate(\n    party=factor(party, levels=c(\"con\", \"lab\", \"ld\", \"snp\", \"green\", \"brexit\" \"pc\"))\n    )\n\nNext, a vector of objects is created containing the hex codes for the colours of political parties (party_colours). This is a named vector, with names assigned from the levels of the party variable that was just created.\n\n# Define colours.\ncon <- \"#0575c9\"\nlab <- \"#ed1e0e\"\nld <- \"#fe8300\"\nsnp <- \"#ebc31c\"\ngreen <- \"#78c31e\"\npc <- \"#4e9f2f\"\nbrexit <- \"#25b6ce\"\nother <- \"#bdbdbd\"\n\nparty_colours <- c(con, lab, ld, snp, green, brexit, pc)\nnames(party_colours) <- levels(temp_party_shares_region |> pull(party))\n\nThe ggplot2 specification is then updated with the scale_fill_manual() layer:\n\ntemp_party_shares_region |>\n  ggplot(aes(x=reorder(party, vote_share), y=vote_share)) +\n  geom_col(aes(fill=party)) +\n  scale_fill_manual(values=party_colours) +\n  coord_flip() +\n  facet_wrap(~region)\n\n\n\n\n\n\n\nGrammar of Graphics-backed visualization toolkits\n\n\n\nThe idea behind visualization toolkits such as vega-lite, Tableau and ggplot2 is to insert visual data analysis approaches into the Data Scientist’s workflow. Rather than being overly concerned with low-level aspects of drawing, mapping to screen coordinates and scaling factors, the analyst instead focuses on aspects crucial to analysis – exposing patterns in the data by carefully specifying an encoding of data to visuals. Hadley Wickham talks about the type of workflow you will see used throughout this book – bits of dplyr to prepare data for charting before being piped (|>) to a ggplot2 specification – as equivalent to a grammar of interactive graphics.\nThe process of searching for, defining and inserting manual colour schemes for creating Figure 3.14 might seem inimical to this. Indeed we were reluctant to include this code so early – there is some reasonably advanced dplyr and a little regular expression in the data preparation code that you should not be overly concerned with. However, having control of these slightly more low-level properties is sometimes necessary even for exploratory analysis, in this case for enabling a symbolisation that is clear and easily interpretable. Try relating the bars without the manual setting of colours by political party – it certainly requires some mental acuity.\n\n\n\n3.3.5 Plot relationships\n\n\nFigure 3.15: Plots of 2019 versus 2017 vote shares.\n\n\nIn the Section 3.2.1 we demonstrated how scatterplots are specified in ggplot2. Scatterplots are useful examples for introducing ggplot2 specifications as they involve working with genuine mark primitives (geom_point()) and can be built up using a wide range of encoding channels.\nTo continue the investigation of change in vote shares for the major parties between 2017 and 2019, Figure 3.15 contains a scatterplot of vote Conservative share in 2019 (y-axis) against vote share in 2017 (x-axis). The graphic is annotated with a diagonal line. If constituencies voted in 2019 in exactly the same way as 2017, the points would all converge on the diagonal, points above the diagonal indicate a larger vote share than 2017, those below the diagonal represent a smaller vote share than 2017. Points are coloured according to the winning party in 2019 and constituencies that flipped from Labour to Conservative are emphasised using transparency and shape.\nThe code for generating most of the scatterplot comparing Conservative vote shares is below.\n\ndata_gb |>\n  mutate(winner_19=case_when(\n           winner_19 == \"Conservative\" ~ \"Conservative\",\n           winner_19 == \"Labour\" ~ \"Labour\",\n           TRUE ~ \"Other\"\n         )) |>\n  ggplot(aes(x=con_17, y=con_19)) +\n  geom_point(aes(colour=winner_19), alpha=.8) +\n  geom_abline(intercept = 0, slope = 1) +\n  scale_colour_manual(values=c(con,lab,other)) +\n  ...\n\nHopefully there is little surprising here:\n\n\nData: The data_gb data frame. Values of winner_19 that are not Conservative or Labour are recoded to Other using a conditional statement. This is because points are eventually coloured according to winning party, but the occlusion of points adds visual complexity and so the two main parties are retained and remaining parties recoded to other.\n\nEncoding: Conservative vote share in 2017 and 2019 are mapped to the x- and y- axes respectively and winner_19 to colour. scale_colour_manual() is used for customising the colours.\n\nMarks: geom_point() for generating the points of the scatterplot; geom_abline() for drawing the reference diagonal.\n\n\n3.3.6 Plot geography\n\n\nFigure 3.16: Choropleth of elected parties in 2019 General Election.\n\n\nIn the graphics that facet by region, our analysis suggests at a geography to voting and certainly to observed changes in voting comparing the 2017 and 2019 elections (e.g. Figure B.2). We end the chapter by generating thematic maps of the results data.\nTo do this we need to define a join on the boundary data (cons_outline):\n\n# Join constituency boundaries.\ndata_gb <- cons_outline |>\n  inner_join(data_gb, by=c(\"pcon19cd\"=\"ons_const_id\"))\n# Check class.\n## [1] \"sf\"         \"data.frame\"\n\nThe code for generating the Choropleth maps of winning party by constituency in Figure 3.16:\n\n# Recode winner_19 as a factor variable for assigning colours.\ndata_gb <- data_gb |>\n  mutate(\n    winner_19=if_else(winner_19==\"Speaker\", \"Other\", winner_19),\n    winner_19=as_factor(winner_19))\n\n# Create a named vector of colours\nparty_colours <- c(con, lab, ld, green, other, snp, pc)\nnames(party_colours) <- levels(data_gb |> pull(winner_19))\n\n# Plot map.\ndata_gb |>\n  ggplot(aes(fill=winner_19)) +\n  geom_sf(colour=\"#eeeeee\", size=0.01)+\n  # Optionally add a layer for regional boundaries.\n  # geom_sf(\n  #   data=. |> group_by(region) |> summarise(), \n  #    colour=\"#eeeeee\", fill=\"transparent\", size=0.08\n  # )+\n  coord_sf(crs=27700, datum=NA) +\n  scale_fill_manual(values=party_colours)\n\nA breakdown of the ggplot2 spec:\n\n\nData: The dplyr code updates data_gb by recoding winner_19 as a factor and defining a named vector of colours to supply to scale_fill_manual().\n\nEncoding: No surprises here – fill according to winner_19.\n\nMarks: geom_sf() is a special class of geometry. It draws objects depending on the contents of the geometry column. In this case MULTIPOLYGON, so read this as a polygon geometric primitive.\n\nCoordinates: coord_sf – we set the coordinate system (CRS) explicitly. In this case OS British National Grid.\n\nSetting: Constituency boundaries are subtly introduced by setting the geom_sf() mark to light grey (colour=\"#eeeeee\") with a thin (size=0.01) outline. On the map to the right outlines for regions are added as another geom_sf layer.\n\n\n\nFigure 3.17: Map of Butler Con-Lab Swing in 2019 General Election.\n\n\nThe details of how ggplot2 can be used in more involved visualization design is reserved for the latter chapters of the book. However, since the graphic has been discussed at length, it would be strange not to demonstrate how the encoding in the Washington Post piece can be applied here to analyse our Butler two-party swing variable (e.g. Beecham 2020).\nFirst, some helper functions – converting degrees to radians and centring geom_spoke() geometries. No need to fixate on these details, just run the code.\n\n# Convert degrees to radians.\nget_radians <- function(degrees) {\n  (degrees * pi) / (180)\n}\n# Rescaling function.\nmap_scale <- function(value, min1, max1, min2, max2) {\n  return  (min2+(max2-min2)*((value-min1)/(max1-min1)))\n}\n# Position subclass for centred geom_spoke.\n# As per https://bit.ly/3yfXdKJ.\nposition_center_spoke <- function() PositionCenterSpoke\nPositionCenterSpoke <-\n    ggplot2::ggproto('PositionCenterSpoke', ggplot2::Position,\n        compute_panel = function(self, data, params, scales)\n            {\n                data$x <- 2*data$x - data$xend\n                data$y <- 2*data$y - data$yend\n                data$radius <- 2*data$radius\n                data\n            }\n    )\n\nNext re-define party_colours, the object we use for manually setting colours, to contain just three values: hex codes for Conservative, Labour and Other.\n\nparty_colours <- c(con, lab, other)\nnames(party_colours) <- c(\"Conservative\", \"Labour\", \"Other\")\n\nAnd the ggplot2 specification:\n\nmax_shift <- max(abs(data_gb |> pull(swing_con_lab)))\nmin_shift <- -max_shift\n\ngb <- data_gb |>\n  mutate(\n    is_flipped=seat_change_1719 %in% c(\"Conservative gain from Labour\",\"Labour gain from Conservative\"),\n    elected=if_else(!winner_19 %in% c(\"Conservative\", \"Labour\"), \"Other\", as.character(winner_19))\n    ) |>\n  ggplot()+\n  geom_sf(aes(fill=elected), colour=\"#636363\", alpha=.2, size=.01)+\n  geom_spoke(\n             aes(x=bng_e, y=bng_n, angle=get_radians(map_scale(swing_con_lab,min_shift,max_shift,135,45)), colour=elected, size=is_flipped),\n             radius=7000, position=\"center_spoke\"\n             )+\n  coord_sf(crs=27700, datum=NA)+\n  scale_size_ordinal(range=c(.3,.9))+\n  scale_colour_manual(values=party_colours)+\n  scale_fill_manual(values=party_colours)\n\nA breakdown:\n\n\nData: data_gb is updated with a boolean identifying whether or not the Constituency flipped Con-Lab/Lab-Con between successive elections (is_flipped), and a variable simplifying the party elected to either Conservative, Labour or Other.\n\nEncoding: geom_sf is again filled by elected party. This encoding is made more subtle by adding transparency (alpha=.2). geom_spoke() is a line primitive that can be encoded with a location and direction. It is mapped to the geographic centroid of each Constituency (bng_e - easting, bng_n - northing), coloured according to elected party, sized according to whether the Constituency flipped its vote and tilted according to the Swing variable. A function (map_scale()) pegs the maximum Swing values in either direction to 45 degrees (max Swing to the right, Conservative) and 135 degrees (max Swing to the left, Labour).\n\nMarks: geom_sf() for the Constituency boundaries, geom_spoke() for the angled line primitives.\n\nScale: geom_spoke() primitives are sized to emphasise whether constituencies have flipped. The size encoding is censored to two values with scale_size_ordinal(). Passed to scale_colour_manual() and scale_fill_manual() is the vector of party_colours\n\n\nCoordinates: coord_sf – the CRS is OS British National Grid.\n\nSetting: The radius, the of geom_spoke() primitives is a sensible default arrived at through trial and error, its position set using our center_spoke class."
  },
  {
    "objectID": "03-visual.html#conclusions",
    "href": "03-visual.html#conclusions",
    "title": "3  Visualization Fundamentals",
    "section": "\n3.4 Conclusions",
    "text": "3.4 Conclusions\nVisualization design is ultimately a process of decision-making. Data must be filtered and prioritised before being encoded with marks, visual channels and symbolisation. The most successful data graphics are those that expose structure, connections and comparisons that could not be achieved easily via other, non-visual means. This chapter has introduced concepts – a vocabulary, framework and empirically-informed guidelines – that help support this decision-making process and that underpin modern visualization toolkits (ggplot2 included). Through an analysis of UK 2019 General Election data, we have demonstrated how these concepts can be applied in a real data analysis."
  },
  {
    "objectID": "04-explore.html",
    "href": "04-explore.html",
    "title": "4  Exploratory Data Analysis",
    "section": "",
    "text": "By the end of this chapter you should gain the following knowledge and practical skills."
  },
  {
    "objectID": "04-explore.html#introduction",
    "href": "04-explore.html#introduction",
    "title": "4  Exploratory Data Analysis",
    "section": "\n4.1 Introduction",
    "text": "4.1 Introduction\nExploratory Data Analysis (EDA) is an approach to analysis which aims to expose the properties and structure of a dataset, and from here suggest directions for analytic inquiry. In an EDA, relationships are quickly inferred, anomalies labelled, assumptions tested and new hypotheses and ideas formulated. EDA relies heavily on visual approaches to analysis; it is common to generate many dozens of (often throwaway) data graphics.\nThis chapter demonstrates how the concepts and principles introduced previously, of data types and their visual encoding, can be applied to support EDA. It does so by analysing STATS19, a dataset containing detailed information on every reported road traffic crash in Great Britain that resulted in personal injury. STATS19 is highly detailed, with many categorical variables. This chapter starts by revisiting commonly used chart idioms (Munzner 2014) for summarising within-variable variation and between-variable co-variation in a dataset. It then focuses more directly on the STATS19 case, and how detailed comparison across many categorical variables can be effected using colour, layout and statistical computation."
  },
  {
    "objectID": "04-explore.html#concepts",
    "href": "04-explore.html#concepts",
    "title": "4  Exploratory Data Analysis",
    "section": "\n4.2 Concepts",
    "text": "4.2 Concepts\n\n4.2.1 Exploratory data analysis and statistical graphics\n\nThe simple graph has brought more information to the data analyst’s mind than any other device.\nJohn Tukey\n\nIn an Exploratory Data Analysis (EDA), graphical and statistical summaries are variously used to build knowledge and understanding of a dataset. The goal of EDA is to infer relationships, identify anomalies and test new ideas and hypotheses; it is a knowlegde-building activity. Rather than a formal set of techniques, EDA should be considered an approach to analysis. It aims to reveal the underlying properties of variables in a dataset (central tendency and dispersion) and their structure (how variables relate to one another) and from there formulate hypotheses to be investigated.\nWickham and Grolemund (2017) identify two main questions that an EDA should address:\n\nWhat type of variation occurs within variables of a dataset?\nWhat type of covariation occurs between variables of a dataset?\n\nWhen summarising within-variable variation, we are interested in a variable’s spread or dispersion and its location within this distribution (central tendency). Different statistics can be applied, but a familiar distinction is between measures of central tendency that are or are not robust to outliers (e.g. mode and median versus mean). Correlation statistics are most obviously applied when studying between-variable covariation in variables measured on inerval and ratio scales, but other statistics, such as odds ratios and chi-square tests are commonly used when studying covariation in categorical data.\nDecisions around which statistic to use depend on a variable’s measurement-level (e.g. Table 4.1). As demonstrated by Anscombe’s quartet (Anscombe 1973), statistical summaries can hide important structure, or assume structure that doesn’t exist. None of the measures of central tendency in Table 4.1 would expose whether a variable for instance is multi-modal and only when studying all measures of central tendency and dispersion together might it be possible to guess at the presence of outliers that could undermine statistical assumptions. It is for this reason that data visualization is seen as intrinsic to EDA (Tukey 1977).\n\n\n\n\n\nTable 4.1:  Breakdown of variable types. \n \n Measurement \n    Statistics \n    Chart idiom \n  \n\nWithin-variable variation\n\n Nominal \n    mode | entropy \n    bar charts, dot plots ... \n  \n\n Ordinal \n    median | percentile \n    bar charts, dot plots ... \n  \n\n Continuous \n    mean | variance \n    histograms, box plots, density plots ... \n  \nBetween-variable variation\n\n Nominal \n    contingency tables \n    mosaic/spine plots ... \n  \n\n Ordinal \n    rank correlation \n    slope/bump charts ... \n  \n\n Continuous \n    correlation \n    scatterplots, parallel coordinate plots ... \n  \n\n\n\n\n\n\n4.2.2 Plots for continuous variables\nWithin-variable variation: histograms, density plots, boxplots\nFigure 4.1 presents statistical graphics that are commonly used to show continuous variables measured on a ratio and interval scale – in this instance the age of casualty for a random sample of Stast19 road crashes (casualty_age).\nIn the top row is a strip-plot. Every observation is displayed as a dot and mapped to x-position, with transparency and a random vertical perturbation applied to resolve occlusion due to overlapping observations. Although strip-plots scale poorly, the advantage is that all observations are displayed without the need to impose an aggregation. It is possible to visually identify the location of the distribution – denser dots towards 20-25 age range – but also that there is a degree of spread across the age values.\nHistograms were used in the previous chapter when analysing the 2019 UK General Election dataset. Histograms partition continuous variables into equal-range bins and observations in each bin are counted. These counts are encoded on an aligned scale using bar height. Increasing the size of the bins increases the resolution of the graphical summary. If reasonable decisions are made around choice of bin, histograms give distributions a shape that is expressive. It is easy to identify the location of a distribution and, in using length on aligned scale to encode frequency, estimate relative densities between different parts of the distribution. Different from the strip-plot, the histogram allows us to intuit that despite the heavy spread, the distribution of casualty_age is right-skewed, and we’d expect this given the location of the mean (36 years) relative to the median (33 years).\nA problem with histograms is the potential for discontinuities and artificial edge-effects around the bins. Density plots overcome this and can be thought of as smoothed histograms. They show the probability density function of a variable – the relative amount of probability attached to each value of casualty_age. From glancing at the density plots an overall shape to the distribution can be immediately derived. It is also possible to infer statistical properties: the mode of the distribution (the highest density), the mean (by visual averaging) and median (finding the midpoint of the area under the curve). Density plots are better suited to datasets that contain a reasonably large number of observations and due to the smoothing function it is possible to generate a density plot that suggests nonsensical values (e.g. negative ages in this case if the plot range hadn’t been censored).\nFinally, boxplots (McGill and Larsen 1978) encode the statistical properties inferred from strip-plots, histograms and density plots directly. The box is the interquartile range (IQR) of the casualty_age variable, the vertical line splitting the box is the median, and the whiskers are placed at observations \\leq 1.5*IQR. Whilst we lose important information around the shape of a distribution, box-plots are space-efficient and useful for comparing many distributions at once.\n\n\n\n\nFigure 4.1: Univariate plots of dispersion.\n\n\n\n\n\n\nSince the average age of road casualties is quite low, it may be instructive to quickly explore the distribution of casualty_age conditioning on another variable and differentiating between variable values using colour. Figure 4.2 displays boxplots and denisty plots of the location and spread in casualty_age by vehicle and casualty_class for all crashes involving pedestrians. A noteworthy pattern is that riders of bicycles and motorcycles tend to be younger than the pedestrians they are contacting with, whereas for buses, taxis, HGVs and cars the reverse is true. Pedestrians involved in crashes with cars are especially skewed towards the younger ages. Although less space efficient, the density plots display richer information around the nature of the driver / pedestrian age distributions.\n\n\n\n\nFigure 4.2: Boxplots of casualty age by vehicle type and class.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIt is common in an EDA to quickly compare associations between many quantitive variables in a dataset using scatterplot matrices or alternatively parallel coordinate plots. We will use both in chapters 6 and 7 when building models that attempt to formally structure and explain between-variable covariation.\n\n\n\n4.2.3 Plots for categorical variables\nWithin-variable variation: bars, dotplots, heatmaps\nWith categorical variables we are interested in exploring how relative frequencies distribute across the variable’s categories. Bar charts are most commonly used. As established in the previous session, length is an efficient visual channel for encoding quantities, counts in this case. Often it is useful to flip bar charts on their side so that category labels are arranged horizontally for ease of reading and, unless there is a natural ordering to categories, arrange the bars in descending order based on their frequency, as in Figure 4.3.\n\n\n\n\nFigure 4.3: Bars displaying crash frequencies by vehicle type.\n\n\n\n\n\nBar charts are effective at conveying frequencies where the number of categories is reasonably small. For summarising frequencies across many categories, alternative chart types that minimise non-data-ink, such as Cleveland dot plots may be appropriate. The left plot in Figure 4.4 displays pedestrian crash counts for boroughs in London, ordered by crash frequency and grouped by whether boroughs are in inner- or outer- London. To the right, dots for crashes recorded weekend | weekday are added. The addition of lines to connect dots is a useful stratgey for emphasising the relative differences between time periods by borough. Although a consistent pattern is of greater crash counts during weekdays, the gap is less obvious four outer London boroughs; there may be relatively more pedestrian crashes occuring in these boroughs at weekends. Finally, a heatmap with the same ordering and grouping of boroughs, but with columns coloured according to crash frequencies by vehicle type. Remembering Munzner (2014)’s ordering of visual channels, we trade-off some precision in estimation when encoding frequencies in heatmaps; Stevens’s power estimates for the perception of changes in lightness were n~0.5. The difficulty when making comparisons between vehicle type is that by far the most common vehicle type involved in pedestrian crashes is cars, any variation between vehicle types outside of this is imperceptible.\n\n\n\n\nFigure 4.4: Cleveland dot plots and heatmaps summarising crash frequencies by London borough and period of day.\n\n\n\n\n\nBetween-variable covariation: standardised bars and mosaic plots\n\n\n\n\n\nIn Figure 4.4 we began to make between category comparison; we asked whether there are relatively greater or fewer crashes by time period or vehicle type in certain boroughs than others. There are chart idioms that explicitly support these sorts of analysis tasks. Figure 4.5 compares pedestrian crash frequencies by vehicle type involved and injury severity (of the pedestrian). First, stacked bars are ordered by frequency, distinguishing injury severity using colour value. Injury severity is an ordinal variable and the choice of colour in Figure 4.5 reflects this order – dark red for KSI, light red for slight. Cars are by far the dominant travel mode, causing the largest number of slight and serious injuries to pedestrians. Whether or not cars result in more severe injury rates than other travel modes is not clear from the left-most chart. Length encodes absolute crash counts effectively but relative comparison of injury severity between vehicle types is challenging. In standardised bars the absolute length of bars is fixed and bars are split according to proportional injury severity (middle). Now we see that relative injury severity of pedestrians – KSI as a proportion of all crashes – varies slightly between modes (c.22% Taxis - c.26% Buses) except for HGVs where around 40% of crashes result in a serious injury or fatality to the pedestrian. However, we lose a sense of the absolute numbers involved.\n\n\n\n\nFigure 4.5: Bars (and Mosaic plot) displaying association between vehicle type and injury severity.\n\n\n\n\nFailing to encode information on absolute number (amount of information) is a problem in EDA, especially when comparisons over many category combinations are made. For combinations that are rare, for example bicycle-pedestrian casualties, it may only take a small number observations in a particular direction to change the KSI rate. Since proportional summaries are agnostic to sample size, they can induce false discoveries, overemphasising patterns that may be unlikely to replicate. It is sometimes desirable, then, to update standardised bar charts so that they are weighted by frequency: to make more visually salient those categories that occur more often and visually downweight those that occur less often. This is possible using mosaic plots (Friendly 1992). Bar widths and heights are allowed to vary; so bar area is proportional to absolute number of observations and bars are further subdivided for relative comparison across category values. Mosaic plots are useful tools for exploratory analysis. That they are space-efficient and regularly sized (squarified) means they can be flexibly laid out for comparison.\n\n\n\n\n\n\nNote\n\n\n\nThe mosaic plot in Figure 4.5 was generated using the ggmosaic package, an extension to ggplot2.\n\n\nComputing relative and absolute differences\nWhen studying the plots in Figure 4.5, there are some implied questions: Does casualty severity vary depending on the type of vehicle involved in the crash? For which vehicle types is injury severity the highest or lowest? An imbalance is clearly to be expected, but we may start by assuming that injury severity rates, the proportion of pedestrian injuries that are KSI, varies independently of vehicle type and look to quantify and locate any imbalance in these proportions. This happens automatically when comparing the relative widths of the dark red bars in Figure 4.5. Annotating with an expectation, for example, the injury severity rate for all pedestrian casualties (middle plot), helps to further locate differences from expectation for specific vehicles.\nEffect size estimates could also be computed directly using risk ratios (RR) – comparing the observed severity rates by vehicle type against an expectation of independence of severity rates by vehicle type. From this, we could find that the RR for HGVs is 1.64: a crash involving an HGV is 1.64 times more likely to result in a fatality or serious injury to the pedestrian than one not involving an HGV. Alternatively, we could calculate signed chi-score residuals (Visalingam 1981), a measure of effect size that is sensitive both to absolute and relative differences from expectation. Expected values are counts calculated for each observation (category combination of vehicle type and injury severity). Observations (O_i ... O_n) are then compared to expected values (E_i ... E_n) as below:\n\\chi=\\frac{O_i - E_i}{\\sqrt{E_i}}\nThe way in which the differences (residuals) between observed and expected values are standardised in the denominator is important. If the denominator was simply the raw expected value, the residuals would express the proportional difference between each observation and its expected value. The denominator is instead transformed using the square root (\\sqrt{E_i}), which has the effect of inflating smaller expected values and squashing larger expected values, thereby giving greater saliency to differences that are large in both relative and absolute number. The mosaic plot in effect does this visually.\nExpected values are calculated in the same way as the standard chi-square statistic that tests for independence – in this case, that counts of crashes by vehicle type distribute independently of injury severity (Slight or KSI) – and can be derived from a frequency table of category combinations:\nE_i = \\frac{C_i \\times R_i}{GT}\nSo for an observed value (O_i), C_i is its column total; R_i is its row total; and GT is the grand total. Below is this table updated with expected values and signed-chi-scores. A score >0 means that casualty counts for that injury type and vehicle is greater than expected; a score <0 means casualty counts for that injury type and vehicle is less than expected. The signed chi-square residuals have mathematical properties and can be interpreted as any standard score. They are assumed to have a mean \\sim1 and standard deviation of \\sim0. Residuals are therefore analogous to standard deviation units from the expected mean.\n\n\n\n\n\nTable 4.2:  Pedestrian casualties by vehicle involved and injury severity: contingency table with signed chi-scores. \n \n\n\nObserved\nExpected\nSigned chi-scores\n\n\n Vehicle type \n    KSI \n    Slight \n    Row Total \n    KSI Exp \n    Slight Exp \n    KSI Resid \n    Slight Resid \n  \n\n\n\n Car \n    42305 \n    137924 \n    180229 \n    43195 \n    137034 \n    -4.28 \n    2.40 \n  \n\n Van \n    3786 \n    11422 \n    15208 \n    3645 \n    11563 \n    2.34 \n    -1.31 \n  \n\n Taxi \n    2580 \n    9188 \n    11768 \n    2820 \n    8948 \n    -4.53 \n    2.54 \n  \n\n Bus \n    2951 \n    8425 \n    11376 \n    2726 \n    8650 \n    4.30 \n    -2.41 \n  \n\n Motorcycle \n    2137 \n    7102 \n    9239 \n    2214 \n    7025 \n    -1.64 \n    0.92 \n  \n\n HGV \n    2030 \n    3195 \n    5225 \n    1252 \n    3973 \n    21.98 \n    -12.34 \n  \n\n Other \n    1096 \n    3302 \n    4398 \n    1054 \n    3344 \n    1.29 \n    -0.73 \n  \n\n Bicycle \n    1033 \n    3184 \n    4217 \n    1011 \n    3206 \n    0.70 \n    -0.39 \n  \n\n Column Total \n    57918 \n    183742 \n    NA \n    NA \n    NA \n    NA \n    NA \n  \n\n\n\n\n\nThe benefit for EDA is that the signed-scores are very quick and easy to compute and can be derived from frequency tables without applying heavy prior knowledge. They help to identify and locate anomalies in individual data values in a way that is sensitive to both absolute and relative numbers. They are often used in cartographic design to emphasise spatial units that deviate from expectation [refs].\n\nUpdating graphics to show deviation from expectation\nWhen comparing crashes on vehicle type (Figure 4.4) the dominating effect of cars made additional patterns between boroughs visually unintelligible. Figure 4.6 updates the heatmaps with signed residuals encoded using a diverging colour scheme – purple for cells with fewer crash counts than expected, green for cells with more crash counts than expected. The assumption, the modelled expectation, is that crash counts by borough distribute independently of vehicle type. Laying out the heatmap such that inner and outer London boroughs are grouped for comparison is instructive: fewer than expected crashes in inner London are recorded for cars; greater than expected for all other vehicle types but especially taxis and bicycles. This pattern is strongest (largest residuals) for very central, job-rich boroughs – Westminster, City of London and, to a lesser extent, Camden.\n\n\n\n\nFigure 4.6: Heatmaps of crashes by preiod of week and vehicle type for London Boroughs.\n\n\n\n\n\n\n\n4.2.4 Supporting comparison with layout and colour\nA key role for visual methods in EDA is in supporting comparison. Three strategies typically deployed are juxtaposition, superposition and direct encoding (see Gleicher and Roberts 2011). Table 4.3 defines each and suggests how they can be implemented in ggplot2. For using juxtaposition, control over how chart elements are arranged and laid out is necessary. View composition with ggplot2 can be achieved using faceting (geom_facet), where plots are conditioned on categorical variables, or using the patchwork package for more flexible plot arrangements\n\n\n\n\n\n\nTable 4.3:  Implementing Gleicher et al.’s (2011) three comparison strategies in ggplot2. \n \n Strategy \n    Function \n    Use \n  \n\n\n Juxtaposition \n    faceting \n    Create separate plots in rows and/or columns by conditioning on a categorical variable. Each plot has same encoding and coordinate space. \n  \n\n Juxtaposition \n    patchwork pkg \n    Flexibly arrange plots of different data types, encodings and coordinate space. \n  \n\n Superposition \n    geoms \n    Layering marks on top of each other. Marks may be of different data types but must share the same coordinate space. \n  \n\n Direct encoding \n    NA \n    No strategy specialised to direct encoding. Often variables cross 0, so diverging schemes, or graphics with clearly annotated and symbolised thresholds (Swing to the right / left in the Washington Post graphic in chapter 3) are used. \n  \n\n\n\n\n\nAs with most visualization design, these strategies require careful decision-making. In ?fig-mosaic-boroughs mosaic plots are used to compare pedestrian crashes occuring in London boroughs by vehicle type. Two very different London boroughs are compared, Westminster and Harrow. As well as scaling bar height by vehicle type, widths vary according to whether crashes took place during weekdays or weekends. Whilst the modal vehicle type involved in pedestrian injuries is the same for both boroughs (cars), cars are more dominant for crashes recorded in the outer London borough of Harrow. Pedestrian crashes involving cars in Westminster occur relatively more frequently on weekends whereas crashes involving bicycles, vans and motorcycles occur relatively more frequently on weekdays.\nTo support the weekday/weeked comparison, we could superimpose a line corresponding to the expected proportion of weekend crashes in a borough given the London average (left plot in ?fig-mosaic-boroughs). Alternatively we could directly encode difference from expectation by centering the dark bars and varying the widths according to how much the relative number of weekend crashes is above (oriented to right) or below (oriented to left) this expectation (middle plot). The latter approach helps highlight boroughs that are different from the London average: the more dark blue the more atypical that borough is. The largest differences from expectation are in Westminster, with crashes involving vans, mototcycles and bicycles overrepresented amongst weekdays. Finally, if we are to compare across all 33 London boroughs we may chose to reduce some of the detail in the graphic and focus on a subset of vehicle types. As vehicle type is a categorical nominal variable it may also be sensible to encode that variable using colour hue to help asociate vehicle types between boroughs.\n::: {.cell} ::: {.cell-output-display}  ::: :::\n?fig-mosaic-boroughs-alpha shows this updated design with borooughs grouped by whether they are in inner or outer London. This grouping usefully reinforces the distinctive pattern of vehicle categories involved in pedestrian crashes. In inner London, crashes between pedestrians and vehicles other than cars are in relative terms much more common, with motorcycles and bicycles overrepresented amongst crashes taking place during weekdays and cars and taxis very much overrepresented amongst weekend crashes.\n::: {.cell} ::: {.cell-output-display}  ::: :::\nSemi-geographic layouts\nThe alphabetical layout of boroughs helps with look-up-type tasks, but since we have already identified that patterns of pedestrian injuries by vehicle type varies by central-inner-outer London, we could further support this comparison by laying out the mosaic plots with a spatial arrangement, as in Figure 4.7. This ‘map’ clearly looks different from standard maps of London that use a precise geographic arrangement. When studying social phenomena, high levels of precision are not always needed. Relaxing geography frees up space to introduce richer, more complex designs. In the layout in Figure 4.7, taken from AfterTheFlood, each borough is represented as a square of regular size and arranged in its approximate geographic position, allowing the central-inner-outer London distinctions to be made more effectively.\n\n\n\n\nFigure 4.7: Mosaic plots of vehicle type and injury severity for London Boroughs with spatial arrangement."
  },
  {
    "objectID": "04-explore.html#techniques",
    "href": "04-explore.html#techniques",
    "title": "4  Exploratory Data Analysis",
    "section": "\n4.3 Techniques",
    "text": "4.3 Techniques\nThe technical element to this session continues in our analysis of STATS19 road crash data. After importing and describing the dataset, you will generate statistical summaries and data graphics for analysing pedestrian casualties. You will focus on visual design choices – colour and layout – that support comparison of pedestrian casualties, conditioning on numerous categorical variables held in the STATS19 dataset.\n\nDownload the 04-template.qmd file for this session and save it to the reports folder of your vis4sds project.\nOpen your vis4sds project in RStudio and load the template file by clicking File > Open File ... > reports/04-template.qmd.\n\n\n4.3.1 Import\nThe template file lists the required packages – tidyverse, sf and also the stats19 package for downloading and formatting the road crash data. STATS19 is a form used by the police to record road crashes that result in injury. Raw data are released by the Department for Transport as a series of .csv files spread across numerous .zip folders. This makes working with the dataset somewhat tedious and behind the stats19 package is some laborious work combining, recoding and relabelling the raw data files.\nSTATS19 data are organised into three tables:\n\n\nAccidents (or Crashes): Each observation is a recorded road crash with a unique identifier (accident_index), date (date), time (time) and location (longitude, latitude). Many other characteristics associated with the crashes are also stored in this table.\n\nCasualties: Each observation is a recorded casualty that resulted from a road crash. The Crashes and Casualties data can be linked via the accident_index variable. As well as casualty_severity (Slight, Serious, Fatal), information on casualty demographics and other characteristics is stored in this table.\n\nVehicles: Each observation is a vehicle involved in a crash. Again Vehicles can be linked with Crashes and Casualties via the accident_index variable. As well as the vehicle type and manoeuvre being made, information on driver characteristics is recorded in this table.\n\nIn 04-template.qmd is code for downloading each of these tables using the stats19 package’s API. You will collect data on crashes, casualties and vehicles for 2010-2019, so these datasets take a little time to download. For this reason, I suggest that once downloaded you write the data out and read in as .fst. In fact I have made a version of this available for download on a separate repository in case you are having problems with the stats19 download.\nThe data analysis that follows is concerned with pedestrian-vehicle crashes. Pedestrian casualties resulting from those crashes are filtered with the purpose of exploring how casualties vary by the socio-economic characteristics of the area in which the crashes took place.\nThe first data processing task is to generate a subset of data describing these casualties and the crashes and vehicles to which they are linked. The crashes (crashes_all) and casualties (casualties_all) tables are joined and pedestrian casualties filtered filter(casualty_type==\"Pedestrian\"). This new table is then joined on vehicles (vehicles_all).\nThere are some simplifications and assumptions made here. Some pedestrian crashes involve many vehicles, but ideally for the analysis we need a single vehicle (and vehicle type) to be attributed to each crash. For each casualty, the largest vehicle involved in the crash is assigned. This is achieved by filtering all vehicle records in vehicles_all involved in a pedestrian casualty, a semi_join on the pedestrian casualty table. We then recode the vehicle_type variable as an ordered factor, group the vehicles table by the crash identifier (accident_index), and for each crash identify the largest vehicle involved, and then filter these largest vehicles. It is common to have several vehicles of the same type involved in a crash, so a final filter is on a vehicle_reference variable. This is an integer variable starting from 1 to n assigned to each vehicle involved in the crash. I do not know whether there is an inherent order to this variable, but make the assumption that vehicle_reference=1 is the main vehicle involved in the crash. So single vehicles are linked to single casualties based first on the largest vehicle involved and then on vehicle_reference."
  },
  {
    "objectID": "05-network.html",
    "href": "05-network.html",
    "title": "5  Networks",
    "section": "",
    "text": "By the end of this chapter you should gain the following knowledge and practical skills."
  },
  {
    "objectID": "05-network.html#concepts",
    "href": "05-network.html#concepts",
    "title": "5  Networks",
    "section": "5.1 Concepts",
    "text": "5.1 Concepts"
  },
  {
    "objectID": "05-network.html#techniques",
    "href": "05-network.html#techniques",
    "title": "5  Networks",
    "section": "5.2 Techniques",
    "text": "5.2 Techniques"
  },
  {
    "objectID": "06-model.html",
    "href": "06-model.html",
    "title": "6  Models",
    "section": "",
    "text": "By the end of this chapter you should gain the following knowledge and practical skills."
  },
  {
    "objectID": "06-model.html#concepts",
    "href": "06-model.html#concepts",
    "title": "6  Models",
    "section": "6.1 Concepts",
    "text": "6.1 Concepts"
  },
  {
    "objectID": "06-model.html#techniques",
    "href": "06-model.html#techniques",
    "title": "6  Models",
    "section": "6.2 Techniques",
    "text": "6.2 Techniques"
  },
  {
    "objectID": "07-uncertainty.html",
    "href": "07-uncertainty.html",
    "title": "7  Uncertainty",
    "section": "",
    "text": "By the end of this chapter you should gain the following knowledge and practical skills."
  },
  {
    "objectID": "07-uncertainty.html#concepts",
    "href": "07-uncertainty.html#concepts",
    "title": "7  Uncertainty",
    "section": "7.1 Concepts",
    "text": "7.1 Concepts"
  },
  {
    "objectID": "07-uncertainty.html#techniques",
    "href": "07-uncertainty.html#techniques",
    "title": "7  Uncertainty",
    "section": "7.2 Techniques",
    "text": "7.2 Techniques"
  },
  {
    "objectID": "08-storytelling.html",
    "href": "08-storytelling.html",
    "title": "8  Data Storytelling",
    "section": "",
    "text": "By the end of this chapter you should gain the following knowledge and practical skills."
  },
  {
    "objectID": "08-storytelling.html#concepts",
    "href": "08-storytelling.html#concepts",
    "title": "8  Data Storytelling",
    "section": "8.1 Concepts",
    "text": "8.1 Concepts"
  },
  {
    "objectID": "08-storytelling.html#techniques",
    "href": "08-storytelling.html#techniques",
    "title": "8  Data Storytelling",
    "section": "8.2 Techniques",
    "text": "8.2 Techniques"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Anscombe, F. 1973. “Graphs in Statistical\nAnalysis.” American Statistician 27 (1):\n17–21. https://doi.org/10.1080/00031305.1973.10478966.\n\n\nArribas-Bel, D., and J. Reades. 2018. “Geography and Computers:\nPast, Present, and Future.” Geography\nCompass 12 (10): e12403. https://doi.org/10.1111/gec3.12403.\n\n\nBeecham, R. 2020. “Using Position, Angle and Thickness to Expose\nthe Shifting Geographies of the 2019 UK\nGeneral Election.” Environment and\nPlanning A: Economy and Space 52 (5): 833–36.\n\n\nBeecham, R., J. Dykes, L. Hama, and N. Lomax. 2021. “On the Use of\n‘Glyphmaps’ for Analysing the Scale and Temporal Spread of\nCOVID-19 Reported Cases.” ISPRS International Journal of\nGeo-Information 10 (4). https://doi.org/10.3390/ijgi10040213.\n\n\nBeecham, R., J. Dykes, W. Meulemans, A. Slingsby, C. Turkay, and J.\nWood. 2017. “Map Line-Ups: Effects of Spatial Structure on\nGraphical Inference.” IEEE Transactions on Visualization\n& Computer Graphics 23 (1): 391–400.\n\n\nBrunsdon, C., and A. Comber. 2020. “Opening Practice: Supporting\nReproducibility and Critical Spatial Data Science.” Journal\nof Geographical Systems.\n\n\nButler, D., and S. Van Beek. 1990. “Why Not Swing?\nMeasuring Electoral Change.” Political Science\n& Politics 23 (2): 178–84.\n\n\nCleveland, W. 1993. The Elements of\nGraphing Data. Hobart Press.\n\n\nCleveland, W., and R. McGill. 1984. “Graphical\nPerception: Theory,\nExperimentation, and Application to the\nDevelopment of Graphical\nMethods.” Journal of the American Statistical\nAssociation 79 (387): 531–54.\n\n\nCorrell, M., and M. Gleicher. 2014. “Error Bars\nConsidered Harmful: Exploring\nAlternate Encodings for Mean and\nError.” IEEE Transactions on Visualization and\nComputer Graphics 20 (12): 2142–51.\n\n\nDonoho, D. 2017. “50 Years of Data\nScience.” Journal of Computational and Graphical\nStatistics 26 (6): 745–66. https://doi.org/10.1080/10618600.2017.1384734.\n\n\nFlannery, J. J. 1971. “The Relative Effectiveness of Some Common\nGraduated Point Symbols in the Presentation of Quantitative\nData.” Cartographica 8 (2): 96–109.\n\n\nFriendly, M. 1992. “Mosaic Displays for Loglinear Models.”\nIn ASA, Proceedings of the\nStatistical Graphics\nSection, 61–68.\n\n\n———. 2007. “A Brief History of\nData Visualization.” In Handbook of\nComputational Statistics: Data\nVisualization, edited by C. Chen, W. Härdle, and A.\nUnwin, III:1–34. Heidelberg: Springer-Verlag. http://datavis.ca/papers/hbook.pdf.\n\n\nGleicher, Albers, M., and J. Roberts. 2011. “Visual Comparison for\nInformation Visualization. Information\nVisualization.” Information Visualization\n10 (4): 289–309.\n\n\nHanretty, C. 2017. “Areal Interpolation and the UK’s\nReferendum on EU Membership.” Journal of\nElections, Public Opinion and Parties 37 (4): 466–83.\n\n\nHarrison, L., F. Yang, S. Franconeri, and R. Chang. 2014. “Ranking\nVisualizations of Correlation Using Weber’s\nLaw.” IEEE Conference on Information\nVisualization (InfoVis) 20 (12): 1943–52.\n\n\nHealy, K. 2018. Data Visualization: A\nPractical Introduction. Princeton:\nPrinceton University Press.\n\n\nHeer, J., and M. Bostock. 2010. “Crowdsourcing\nGraphical Perception: Using\nMechanical Turk to Assess\nVisualization Design.” In\nACM Human Factors in\nComputing Systems, 203–12. https://doi.org/10.1145/1753326.1753357.\n\n\nKay, M., and J. Heer. 2016. “Beyond Weber’s\nLaw: A Second Look\nat Ranking Visualizations of\nCorrelation.” IEEE Trans. Visualization &\nComp. Graphics (InfoVis) 22 (1): 469–78.\n\n\nKlippel, A., F. Hardisty, and Rui. Li. 2011. “Interpreting\nSpatial Patterns: An\nInquiry Into Formal and\nCognitive Aspects of Tobler’s\nFirst Law of Geography.”\nAnnals of the Association of American Geographers 101 (5):\n1011–31.\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.\n\n\nLovelace, R., J. Nowosad, and J. Muenchow. 2019. Geocomputation with\nR. London, UK: CRC Press.\n\n\nMatejka, J., and G. Fitzmaurice. 2017. “Same Stats,\nDifferent Graphs: Generating\nDatasets with Varied Appearance\nand Identical Statistics Through\nSimulated Annealing.” In, 1290–94.\nCHI ’17. New York, NY, USA: Association for Computing\nMachinery. https://doi.org/10.1145/3025453.3025912.\n\n\nMcGill, Tukey, R., and W. A. Larsen. 1978. “Variations of Box\nPlots.” The American Statistician 32: 12–16.\n\n\nMunzner, T. 2014. Visualization Analysis and\nDesign. AK Peters\nVisualization Series. Boca Raton, FL: CRC\nPress.\n\n\nOpen Science Collaboration. 2015. “Estimating the Reproducibility\nof Psychological Science.” Science 349 (6251): aac4716.\nhttps://doi.org/10.1126/science.aac4716.\n\n\nRensink, R., and G. Baldridge. 2010. “The Perception of\nCorrelation in Scatterplots.” Computer Graphics Forum 29\n(3): 1203–10.\n\n\nStevens, S. 1946. “On the Theory of Scales of Measurement.”\nScience 103 (2684): 677–80.\n\n\nStevens, S, and M. Guirao. 1963. “Subjective Scaling of Length and\nArea and the Matching of Length to Loudness and Brightness.”\nJournal of Experimental Psychology 66 (2): 177–86.\n\n\nTufte, E. 1983. The Visual Display of\nQuantitative Information. Cheshire, CT:\nGraphics Press.\n\n\nTukey, J. W. 1977. Exploratory Data\nAnalysis. Reading, MA, USA: Addison-Wesley.\n\n\nTukey, John W. 1962. “The Future of Data\nAnalysis.” The Annals of Mathematical Statistics\n33 (1): 1–67. https://doi.org/10.1214/aoms/1177704711.\n\n\nVisalingam, M. 1981. “The Signed Chi-Score Measure for the\nClassification and Mapping of Plychotomous Data.” The\nCartographic Journal 18 (1): 32–43.\n\n\nWare, C. 2008. Visual Thinking for\nDesign. Waltham, MA: Morgan Kaufman.\n\n\nWhite, T. 2017. “Symbolization and the Visual\nVariables.” In He Geographic\nInformation Science &\nTechnology Body of\nKnowledge, edited by John P. Wilson.\n\n\nWickham, H. 2010. “A Layered Grammar of Graphics.”\nJournal of Computational and Graphical Statistics 19 (1): 3–28.\n\n\n———. 2014. “Tidy Data.” Journal of\nStatistical Software 59 (10): 1–23.\n\n\nWickham, H., and G. Grolemund. 2017. R for Data\nScience: Import, Tidy,\nTransform, Visualize, and Model\nData. Sebastopol, California: O’Reilly Media.\n\n\nWilkinson, L. 1999. The Grammar of\nGraphics. New York: Springer."
  },
  {
    "objectID": "a1-data.html",
    "href": "a1-data.html",
    "title": "Appendix A — Data Fundamentals",
    "section": "",
    "text": "By the end of this Task you should be able to:"
  },
  {
    "objectID": "a1-data.html#introduction",
    "href": "a1-data.html#introduction",
    "title": "Appendix A — Data Fundamentals",
    "section": "\nA.1 Introduction",
    "text": "A.1 Introduction\nThis task requires you to apply the concepts and skills developed in the chapter on data fundamentals. Do complete each of the task components and be sure to save your work."
  },
  {
    "objectID": "a1-data.html#component-1-describe-data",
    "href": "a1-data.html#component-1-describe-data",
    "title": "Appendix A — Data Fundamentals",
    "section": "\nA.2 Component 1: Describe data",
    "text": "A.2 Component 1: Describe data\nComplete the data description table below identifying the measurement level of each variable in the (fictional) New York bikeshare stations dataset below.\n\n\nVariable name\nVariable value\nMeasurement level\n\n\n\nname\n“Central Park”\n\n\n\ncapacity\n80\n\n\n\nrank_capacity\n45\n\n\n\ndate_opened\n“2014-05-23”\n\n\n\nlongitude\n-74.00149746\n\n\n\nlatitude\n40.74177603"
  },
  {
    "objectID": "a1-data.html#component-2-diagnose-data",
    "href": "a1-data.html#component-2-diagnose-data",
    "title": "Appendix A — Data Fundamentals",
    "section": "\nA.3 Component 2: Diagnose data",
    "text": "A.3 Component 2: Diagnose data\nBelow are two different tables with results from UK General Elections. We will be working with these data in the next session. Identify whether or not each is in tidy format (Wickham 2014). If they are not, provide a layout for a tidy version. No need to use code here, just edit the markdown table. If you’re struggling to work out how to organise markdown tables, you may wish to use this tables generator.\n\nA.3.1 UK General Election Results 2019\n\n\nparty\npercent_vote\nnum_mps\n\n\n\nConservative\n43.6\n365\n\n\nLabour\n32.2\n202\n\n\nScottish National Party\n3.9\n48\n\n\nLiberal Democrats\n11.6\n11\n\n\nDemocratic Union Party\n0.8\n9\n\n\n\nA.3.2 UK General Election Results 2017 and 2019\n\n\n\n\n\n\n\n\n\nparty\npercent_vote_2017\nnum_mps_2017\npercent_vote_2019\nnum_mps_2019\n\n\n\nConservative\n42.4\n317\n43.6\n365\n\n\nLabour\n40.0\n262\n32.2\n202\n\n\nScottish National Party\n3.0\n35\n3.9\n48\n\n\nLiberal Democrats\n7.4\n12\n11.6\n11\n\n\nDemocratic Union Party\n0.9\n10\n0.8\n8"
  },
  {
    "objectID": "a1-data.html#component-3-fix-data",
    "href": "a1-data.html#component-3-fix-data",
    "title": "Appendix A — Data Fundamentals",
    "section": "\nA.4 Component 3: Fix data",
    "text": "A.4 Component 3: Fix data\nIn the 02-template.Rmd file for this session are links to two derived tables (ny_spread_columns and ny_spread_rows) from the New York bikeshare trip data that are not in a tidy format.\nUsing functions from dplyr and tidyr reorganise these data so that they conform to the rules of tidy data (Wickham 2014).\nA candidate tidy organisation of the data is below. Each row is an origin-destination pair for a weekday or weekend, and each variable describes:\n\n\no_station: station id of the origin station\n\nd_station: station id of the destination station\n\nwkday: identifies whether the OD pair describes weekday or weekend ny_trips\n\ncount: count of trips recorded for that observation (OD pair and weekday/weekend)\n\ndist: total distance (cumulative) in kms of all trips recorded for that observation (OD pair and weekday/weekend)\n\nduration: total duration in minutes (cumulative) of all trips recorded for that observation (OD pair and weekday/weekend)\n\n\n\n\n\n\n\nNote\n\n\n\nYou may wish to start with reorganising ny_spread_rows as I deliberately made ny_spread_columns quite challenging. The intention here was to replicate the sorts of arduous data formatting operations that need to be performed when working with real datasets. As always there are different approaches to this, but it can be achieved with use of pivot_longer, pivot_wider, plus a call to separate(). This may be one to post to the course Slack.\n\n\n\n## # A tibble: 386,762 x 6\n##    o_station d_station wkday   count  dist duration\n##        <int>     <int> <chr>   <int> <dbl>    <dbl>\n##  1        72       116 weekend     1  1.15     18.2\n##  2        72       127 weekend    10 18.0     339.\n##  3        72       128 weekend     2  3.18     69.6\n##  4        72       146 weekend    12 27.6     402.\n##  5        72       151 weekend     2  2.87     54.9\n##  6        72       161 weekend     2  2.52     64.8\n##  7        72       164 weekend     5 13.3      73.3\n##  8        72       167 weekend     1  2.07     17.2\n##  9        72       168 weekend     2  1.70     42.7\n## 10        72       173 weekend     9  9.59    194.\n## # … with 386,752 more rows"
  },
  {
    "objectID": "a1-data.html#component-4-compute-from-data",
    "href": "a1-data.html#component-4-compute-from-data",
    "title": "Appendix A — Data Fundamentals",
    "section": "\nA.5 Component 4: Compute from data",
    "text": "A.5 Component 4: Compute from data\nUsing dplyr functions, calculate the average distance, duration and speed of trips occurring for each observation. Print out to the Console the top 10 most heavily cycled OD pairs (and their associated summary statistics) separately for weekdays and weekends. You may wish to join on your ny_stations table in order to fetch the station names corresponding to the origin and destination stations."
  },
  {
    "objectID": "a2-visual.html",
    "href": "a2-visual.html",
    "title": "Appendix B — Visualization Fundamentals",
    "section": "",
    "text": "By the end of this Task you should be able to:"
  },
  {
    "objectID": "a2-visual.html#introduction",
    "href": "a2-visual.html#introduction",
    "title": "Appendix B — Visualization Fundamentals",
    "section": "\nB.1 Introduction",
    "text": "B.1 Introduction\nThis task requires you to apply the concepts and skills developed in the chapter on visualization fundamentals."
  },
  {
    "objectID": "a2-visual.html#component-1-describe-and-evaluate",
    "href": "a2-visual.html#component-1-describe-and-evaluate",
    "title": "Appendix B — Visualization Fundamentals",
    "section": "\nB.2 Component 1: Describe and evaluate",
    "text": "B.2 Component 1: Describe and evaluate\n\n\nFigure B.1: Map of Butler Con-Lab Swing in 2019 General Election.\n\n\nComplete the description table below identifying each data item that is encoded along with its measurement level, visual mark and visual channel and the effectiveness rank, according to Munzner (2014), of this encoding.\n\n\n\n\n\n\n\n\n\nData item\nMeasurement level\nVisual mark\nVisual channel\nRank\n\n\n\nlocation\n\n\n\n\n\n\n...\n...\n...\n...\n...\n\n\n...\n...\n...\n...\n...\n\n\n...\n...\n...\n...\n...\n\n\n...\n...\n...\n...\n...\n\n\n...\n...\n...\n...\n..."
  },
  {
    "objectID": "a2-visual.html#component-2-reproduce",
    "href": "a2-visual.html#component-2-reproduce",
    "title": "Appendix B — Visualization Fundamentals",
    "section": "\nB.3 Component 2: Reproduce",
    "text": "B.3 Component 2: Reproduce\nWrite some code for reproducing something similar to the graphic below – a set of histograms of the Swing variable, faceted by region. Place your code into the 03-template.qmd file for this chapter.\n\n#######################\n# Enter your code in the chunk provided.\n######################\n\n\nSave your plot as a .png file and insert the file as a Figure into your .qmd document (guidance here).\n\n\n\nFigure B.2: Histograms of Swing variable, grouped by region.\n\n\n\nB.3.1 Scatterplots with multiple encodings\nWrite some code for reproducing something similar to the graphic below – a scatterplot comparing 2017 and 2019 vote shares for Labour. Be sure to include every encoding as it appears in the graphic (e.g. shape and alpha according to whether constituencies flipped allegiance). Hint: you may need to use a conditional statement to generate a variable for emphasising constituencies that flipped parties between 2017 and 2019. Place your code into the 03-template.qmd file for this chapter.\n\n#######################\n# Enter your code in the chunk provided.\n######################\n\n\nSave your plot as a .png file and insert the file as a Figure into your .qmd document.\n\n\n\nFigure B.3: Plots of 2019 versus 2017 vote shares."
  },
  {
    "objectID": "a2-visual.html#component-3-create-a-map",
    "href": "a2-visual.html#component-3-create-a-map",
    "title": "Appendix B — Visualization Fundamentals",
    "section": "\nB.4 Component 3: Create a map",
    "text": "B.4 Component 3: Create a map\nWrite some code for reproducing something similar to the graphic below – a map of the estimated Leave:Remain vote margin by Parliamentary Constituency. Note the diverging colour scheme here to distinguish whether the Constituency was majority Leave and Remain – brown or green – and also the size of that majority – the darker the colour, the larger the majority.\nPlace your code into the 03-template.qmd file for this chapter.\n\n#######################\n# Enter your code in the chunk provided.\n######################\n\n\nSave your plot as a .png file and insert the file as a Figure into your .qmd document.\n\n\n\nFigure B.4: Map of 2016 EU Referendum vote, estimated by Parliamentary Constituency in GB."
  }
]