[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Visualization for Social Data Science",
    "section": "",
    "text": "Version: 0.0.1\n\n\nSocial scientists have at their disposal an expanding array of data measuring very many social behaviours. This is undoubtedly a positive. Previously unmeasurable aspects of human behaviour can now be explored in a large-scale empirical way, whilst already measured aspects of behaviour can be re-evaluated. Such data are, however, rarely generated for the sole purpose of social research, and this fact elevates visual analysis approaches in importance due to visualization’s emphasis on discovery. When encountering new data for the first time, data graphics help expose complex structure and multivariate relations, and in so doing advance analysis situations where the questions to be asked and techniques to be deployed may not be immediately obvious.\nVisualization toolkits such as ggplot2, vega-lite and Tableau have been designed to ease the process of generating data graphics for analysis. There is a comprehensive set of texts and resources on visualization design theory and practice, and several notable how-to primers on visualization. However, comparatively few existing resources demonstrate with real large-scale data and real social science scenarios how and why visual data analysis approaches can be inserted into an analyst’s workflow.\nThis book aims to fill this space. It presents principled workflows, with accompanying code, for using data graphics and statistics in tandem. In doing so it equips readers with critical design and technical skills needed to analyse and communicate with a range of datasets in the social sciences.\nThe book emphasises application. Each chapter introduces concepts and techniques by analysing a real-world dataset located within a social science domain. The ambition is to do so in a way that advances our analysis and understanding of the phenomena. A consequence of this approach is selectivity in our treatment of concepts and techniques. This is not an exhaustive guide to visual data analysis. That the book provides genuine expositions of visual data analysis process hopefully compensates for the selectivity. For example, as is often the case in social data science, we will encounter various unknowns and misdirections. Some of the structure inferred from our visual data analysis may be spurious, or at least subject to uncertainty. We will have to think critically about claims that can be made and about techniques that might be used to help guard against over-interpretation and false discovery. As well as learning how (and why) to use graphics and statistics to explore patterns in data, the book demonstrates how to communicate and tell stories with integrity, implementing recent ideas from data journalism.\n\n\n\nChapters of the book are divided into Concepts and Techniques. The Concepts sections cover key literature, ideas and approaches that can be leveraged to analyse the dataset introduced in the chapter. In the Techniques sections, code examples are provided for implementing the techniques. Each chapter starts with a list of Knowlegde outcomes and Skills outcomes that map to the Concepts and Techniques. To support the technical elements, chapters have a corresponding computational notebook template file. The templates contain pre-prepared code chunks to be executed. In the early chapters of the book, we aim at brevity in the Concepts sections, offset by slightly more lengthy Techniques sections. As the book progresses the balance shifts somewhat, with more involved conceptual discussion and more specialised and abbreviated technical demonstrations.\n\nReaders of the book will be able to:\n\nDescribe, process and combine social science datasets from a range of sources\nDesign statistical graphics that expose structure in social science data and that are underpinned by established principles in data visualization and cartography\nUse modern data science and visualization frameworks to produce data analysis code that is coherent and easily shareable\nApply modern statistical techniques for analysing, representing and communicating data and model uncertainty\n\n\n\n\nThe book is for people working in the broadly defined social sciences, including Geography, Political Science, Economics, Business, Sociology and Psychology. It is aimed at a range of interests and experience levels: postgraduate students and researchers, data journalists, analysts working in public sector, non-governmental and commercial organisations.\nAll technical examples are implemented using the R programming environment. A little prior knowledge of the R ecosystem is assumed; as the chapters progress more advanced concepts and coding procedures are introduced. Whilst the book covers many of the fundamentals of R for working with social science datasets, our ultimate aim is to demonstrate through example how data graphics can and should be used in real data analyses. In this way it complements core resources that more fully cover these how-to aspects: R for Data Science (Wickham and Grolemund 2017) and Geocomputation with R (Lovelace, Nowosad, and Muenchow 2019).\n\n\n\n\n\n\n\n\n\nThis website is licensed under the Creative Commons Attribution-NonCommercial-NoDerivs 4.0 License."
  },
  {
    "objectID": "01-intro.html",
    "href": "01-intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "By the end of this chapter you should gain the following knowledge and practical skills."
  },
  {
    "objectID": "01-intro.html#introduction",
    "href": "01-intro.html#introduction",
    "title": "1  Introduction",
    "section": "\n1.1 Introduction",
    "text": "1.1 Introduction\nThis chapter introduces the what, why and how of the book. An argument is presented for the use of visual approaches in modern data analysis, especially social data science analysis, and the key technologies and analysis frameworks for the book are introduced: computational notebooks executed in Quarto. The technical component consolidates on any prior knowledge of R and Quarto as well as demonstrates how to organise data science analyses as RStudio Projects."
  },
  {
    "objectID": "01-intro.html#concepts",
    "href": "01-intro.html#concepts",
    "title": "1  Introduction",
    "section": "\n1.2 Concepts",
    "text": "1.2 Concepts\n\n1.2.1 Why vis4sds?\nIt is now taken-for-granted that new data, new technology and new ways of doing science have transformed how we approach the world’s problems. Evidence for this can be seen in the response to the Covid-19 pandemic. Enter Covid19 github into a search and you’ll be confronted with hundreds of repositories demonstrating how various data related to the pandemic can be collected, processed and analysed. Data Science (hereafter data science) is a catch-all term used to capture this shift.\nThe definition has been somewhat stretched over the years, but data science has its origins in the work of John Tukey’s The Future of Data Analysis (1962). Drawing on this, and a survey of more recent work, Donoho (2017) identifies six key facets that a data science discipline might encompass[^01-intro-1]:\n\n\ndata gathering, preparation and exploration;\ndata representation and transformation;\ncomputing with data;\ndata visualization and presentation;\ndata modelling;\nand a more introspective “science about data science”\n\nEach is covered to varying degrees within the book, although data visualization and presentation gets a special status. Rather than a single and self-contained facet of data science process – something that happens after data gathering, preparation, exploration, but before modelling – the book demonstrates how data visualization is intrinsic to, or at least should inform, every facet of data science work.\nThis special status is justified when considering the circumstances under which Social Data Science (hereafter social data science) projects operate. Often new datasets are repurposed for social sciences research for the first time; contain complex structure and relations that cannot be easily captured and modelled using conventional statistics; and, as a consequence, the types of questions asked and techniques deployed to answer them cannot be easily specified in advance. Through the examples in the book, we will demonstrate how visual approaches can capture complex, multivariate structure (chapters 3, 4, 5), provoke critical thinking around data transformation and modelling (chapters 4, 5 and 6) and communicate observed patterns with integrity (chapters 7 and 8).\n\n\n\n\n\n\n1.2.2 What vis4sds?\nThe chapters of this book blend both theory and practical coding activity to cover the fundamentals of visual data analysis. As the chapters progress, data processing and analysis code is applied to datasets from the Political Science, Urban and Transport Planning and Health domains. So the examples in the book demonstrate how visual approaches can be used to generate and evaluate real findings and knowledge.\nTo do this, a reasonably broad set of data processing and analysis procedures is covered. As well as developing expertise around designing data-rich, visually compelling graphics, some tedious aspects of data processing and wrangling are required. Additionally, to learn how to make and communicate claims under uncertainty with data graphics, techniques for estimation and modelling from Statistics are needed. In short, Donoho (2017)’s six key facets of a data science discipline:\n\ndata gathering, preparation, and exploration (Chapters 2, 3, 4);\ndata representation and transformation (Chapters 2, 3);\ncomputing with data (Chapter 2, All chapters);\ndata visualization and presentation (All chapters);\ndata modelling (Chapters 4, 6, 7);\nand the “science about data science” (All chapters)\n\nThere is already an impressive set of open resources practically introducing how to do modern Data Science (Wickham and Grolemund 2017), Visualization (Healy 2018) and Geographic Analysis (Lovelace, Nowosad, and Muenchow 2019). What makes this book different is the emphasis on doing applied data science throughout – we will be identifying and diagnosing problems when gathering data, discovering patterns (some may even be spurious) as we do exploratory analysis, and attempt to make claims under uncertainty as we generate models from observed patterns.\n\n1.2.3 How vis4sds?\n\n1.2.3.1 R for modern data analysis\nAll data collection, analysis and reporting activity will be completed using the open source statistical programming environment R. There are many benefits that come from being fully open-source, with a critical mass of users. Firstly, there is an array of online fora, tutorials and code examples from which to learn. Second, with such a large community, there are numerous expert R users who themselves contribute by developing packages that extend its use.\nOf particular importance is the tidyverse. This is a set of packages for doing data science authored by the software development team at Posit. tidyverse packages share a principled underlying philosophy, syntax and documentation. Contained within the tidyverse is its data visualization package, ggplot2. This package predates the tidyverse and is one of the most widely-used toolkits for generating data graphics. As with other visualization toolkits it is inspired by Wilkinson (1999)’s The Grammar of Graphics, the gg in ggplot2 stands for Grammar of Graphics. We will cover some of the design principles behind ggplot2 (and tidyverse) in Chapter 3.\n\n1.2.3.2 Quarto for reproducible research\n\nIn recent years there has been much introspection into how science works – into how statistical claims are made from reasoning over evidence. This came on the back of, amongst other things, a high profile paper published in Science (Open Science Collaboration 2015), which found that of 100 contemporary peer-reviewed empirical papers in Psychology, the findings of only 39 could be replicated. The upshot is that researchers must now endeavour to make their work transparent, such that “all aspects of the answer generated by any given analysis [can] be tested” (Brunsdon and Comber 2020).\nA reproducible research project should be accompanied with code and data that:\n\nallows tables and figures presented in research outputs to be regenerated\ndoes what it claims (the code works)\ncan be justified and explained through proper documentation\n\nIf these goals are met, then it may be possible for others to use the code on new and different data to study whether the findings reported in one project might be consistent with another. Or alternatively to use the same data, but update the code to extend the original analysis. This model – generate findings, explore replicability in new contexts and re-analysis – is how knowledge development has always worked. However, to achieve this the data and procedures on which findings are generated must be made open and transparent.\nIn this setting proprietary data analysis software that support point-and-click interaction, used widely in the social sciences, are problematic. First, these software are often underpinned by code for implementing statistical procedures that is closed. It is not possible, and therefore less common, for the researcher to fully interrogate into the underlying processes that are being implemented and the results need to be taken more or less on faith. Second, replicating and updating anayses in light of new data is chellenging. It would be tedious to make notes describing all interactions performed when working with a dataset via a point-and-click- interface. As a declarative programming environment, it is very easy to provide such a provenance trail in R (with the tidyverse) since this necessarily exists in the analysis scripts. But also, the Integrated Development Environments (IDEs) through which R is accessed provide notebook environments that allow users to curate reproducible computational documents that blend input code, explanatory prose and outputs. Through the technical elements, we will prepare these sorts of notebooks using Quarto."
  },
  {
    "objectID": "01-intro.html#techniques",
    "href": "01-intro.html#techniques",
    "title": "1  Introduction",
    "section": "\n1.3 Techniques",
    "text": "1.3 Techniques\nReaders of this book might already have some familiarity with R and the RStudio IDE. If not, then this section is designed to quickly acclimatise readers with R and RStudio and to briefly introduce Quarto, R scripts and RStudio Projects. The accompanying template file, 01-template.qmd can be downloaded from the book’s companion website.\n\n1.3.1 R and RStudio\n\nInstall the latest version of R. Note that there are installations for Windows, macOS and Linux. Run the installation from the file you downloaded (an .exe or .pkg extension).\nInstall the latest version of RStudio Desktop. Note again that there are separate installations depending on operating system – for Windows an .exe extension, macOS a .dmg extension.\nOnce installed, open the RStudio IDE.\nOpen an R Script by clicking File > New File > R Script .\n\n\n\n\n\nFigure 1.1: The RStudio IDE\n\n\n\n\n\nYou should see a set of windows roughly similar to those in Figure 1.1. The top left pane is used either as a Code Editor (the tab named Untitled1) or Data Viewer. This is where you’ll write, organise and comment R code for execution or inspect datasets as a spreadsheet representation. Below this in the bottom left pane is the R Console, in which you write and execute commands directly. To the top right is a pane with the tabs Environment and History. This displays all objects – data and plot items, calculated functions – stored in-memory during an R session. In the bottom right is a pane for navigating through project directories, displaying plots, details of installed and loaded packages and documentation on their functions.\n\n1.3.2 Compute in the console\nYou will write and execute almost all code from the code editor pane. To start though let’s use R as a calculator by typing some commands into the Console. You’ll create an object (x) and assign it a value using the assignment operator (<-), then perform some simple statistical calculations using functions that are held within the base package.\n\n\n\n\n\n\nR package documentation\n\n\n\nThe base package exists as standard in R. Unlike other packages, it does not need to be installed and called explicitly. One means of checking the package to which a function you are using belongs is to call the help command (?) on that function: e.g. ?mean().\n\n\nType the commands contained in the code block below into your R Console. Notice that since you are assigning values to each of these objects they are stored in memory and appear under the Global Environment pane.\n\n# Create variable and assign a value.\nx <- 4\n# Perform some calculations using R as a calculator.\nx_2 <- x^2\n# Perform some calculations using functions that form baseR.\nx_root <- sqrt(x_2)\n\n\n1.3.3 Install some packages\nThere are two steps to getting packages down and available in your working environment:\n\n\ninstall.packages(\"<package-name>\") downloads the named package from a repository.\n\nlibrary(<package-name>) makes the package available in your current session.\n\nInstall tidyverse, the core collection of packages for doing Data Science in R, by running the code below:\n\ninstall.packages(\"tidyverse\")\n\nIf you have little or no experience in R, it is easy to get confused about downloading and then using packages in a session. For example, let’s say we want to make use of the simple features package (sf), which we will draw on heavily for performing spatial operations.\n\nlibrary(sf)\n\nUnless you’ve previously installed sf, you’ll probably get an error message that looks like this:\n\n> Error in library(sf): there is no package called ‘sf’\n\nSo let’s install it.\n\ninstall.packages(\"sf\")\n\nAnd now it’s installed, why not bring up some documentation on one of its functions (st_contains()).\n\n?st_contains()\n\nSince you’ve downloaded the package but not made it available to your session, you should get the message:\n\n> No documentation for ‘st_contains’ in specified packages and libraries\n\nSo let’s try again, by first calling library(sf).\n\nlibrary(sf)\n## Linking to GEOS 3.7.2, GDAL 2.4.1, PROJ 6.1.0\n?st_contains()\n\nNow let’s install some of the remaining core packages on which this book depends. Run the block below, which passes a vector of package names to the install.packages() function.\n\npkgs <- c(\n  \"devtools\",\"here\", \"quarto\",\"fst\",\"tidyverse\", \"lubridate\",\n  \"tidymodels\", \"gganimate\", \"ggforce\", \"distributional\", \"ggdist\"\n)\ninstall.packages(pkgs)\n\n\n\n\n\n\n\nR package visibility\n\n\n\nIf you wanted to make use of a package only very occasionally in a single session, you could access it without explicitly loading it via library(<package-name>), using this syntax: <package-name>::<function_name>, e.g. ?sf::st_contains().\n\n\n\n1.3.4 Experiment with Quarto\nQuarto documents are suffixed with the extension .qmd. They are computational notebooks that blend code with textual explanation and images, and so are a mechanism for supporting literate programming (Knuth 1984). They resemble Markdown, a lightweight language designed to minimise tedious markup tags (<header></header>) when preparing HTML documents. The idea is that you trade some flexibility in the formatting of your HTML for ease-of-writing. Working with Quarto documents feels very similar to Markdown. Sections are denoted hierarchically with hashes (#, ##, ###) and emphasis using * symbols (*emphasis* **added** reads emphasis added ).\nDifferent from standard Markdown, Quarto documents can also contain code chunks to be run when the document is rendered; they are a mechanism for producing reproducible, dynamic and interactive notebooks. Dynamic and reproducible because the outputs may change when there are changes to the underlying data; interactive because they can execute not just R code blocks, but also Jupyter Widgets, Shiny and Observable JS. Each chapter has an accompanying Quarto file. In later chapters you will use these to author computational notebooks that blend code, analysis prose and outputs.\nDownload the 01-template.qmd file for this chapter and open it in RStudio by clicking File > Open File ... > <your-downloads>/01-template.qmd. Note that there are two tabs that you can switch between when working with .qmd files. Source retains markdown syntax (e.g. #|##|### for headings); Visual renders these tags and allows you to, for example, perform formatting and build tables through point-and-click utilities.\nA quick anatomy of .qmd files :\n\n\nYAML - positioned at the head of the document and contains metadata determining amongst other things the author details and the output format when rendering.\nTEXT - incorporated throughout to document and comment on your analysis.\nCODE chunks - containing discrete blocks that are run when the .qmd file is rendered.\n\n\n\n\n\nFigure 1.2: The anatomy of .qmd files\n\n\n\n\n\nThe YAML section of an .qmd file controls how your file is rendered and consists of key : value pairs enclosed by ---. Notice that you can change the output format to generate for example .pdf, .docx files for your reports.\n---\nauthor: \"Roger Beecham\"\ntitle: \"Chapter 01\"\nformat: html\n---\nQuarto files are rendered with the Render button, annotated in the Figure above. This starts pandoc and executes all the code chunks and outputs in the case above an .html file. The markdown file can then be converted to many different output formats via pandoc.\n\nRender the 01-template.qmd file for this chapter by clicking the Render button.\n\nYou will notice that code chunks in Quarto can be customised in different ways. This is achieved by populating fields immediately after the curly brackets used to declare the code chunk:\n```{r}\n#| label: chunk-name\n#| echo: true\n#| eval: false\n\n# Any R code below is not run (evaluated) but printed (echoed)\n# in this position when the .qmd doc is rendered.\n```\nA quick overview of the parameters.\n\n\nlabel: <chunk-name> Chunks can be given distinct names. This is useful for navigating Quarto files. It also supports chaching – chunks with distinct names are only run once, important if certain chunks take some time to execute.\n\necho: <true|false> Determines whether the code is visible or hidden from the rendered file. If the output file is a data analysis report you may not wish to expose lengthy code chunks as these may disrupt the discursive text that appears outside of the code chunks.\n\neval: <true|false> Determines whether the code is evaluated (executed). This is useful if you wish to present some code in your document for display purposes.\n\ncache: <true|false> Determines whether the results from the code chunk are cached.\n\n1.3.5 R Scripts\nWhilst there are obvious benefits to working in .qmd documents when doing data analysis, there may be occasions where a script is preferable. R scripts are plain text files with the extension .R. They are typically used for writing discrete but substantial code blocks that are to be executed. For example, a set of functions that relate to a particular use case might be organised into an R script, and those functions referred to in a data analysis from a .qmd in a similar way as one might import a package. Below is an example script file with helper functions to support flow visualizations in R. The script is saved with the file name bezier_path.R. If it were stored in a sensible location, like a project’s code folder, it could be called from a .qmd file with source(./code/bezier_path). R Scripts can be edited in the same way as Quarto files in RStudio, via the Code Editor pane.\n\n# bezier_path.R\n#\n# Author: Roger Beecham\n##############################################################################\n\n# This function takes cartesian coordinates defining origin and destination\n# locations and returns a tibble representing a path for an asymmetric bezier\n# curve, which curves towards the destination location.\n#\n# The tibble has three rows representing an origin, destination and control\n# point for the bezier curve. The parameterisation follows that published in\n# Wood et al. 2011. doi: 10.3138/carto.46.4.239.\n\n# o_x, o_y : numeric coords of origin\n# d_x, d_y : numeric coords of destination\n# od_pair : text string identifying name of od-pair\n\nget_trajectory <- function (o_x, o_y, d_x, d_y, od_pair, curve_extent=-90, curve_position=6)\n{\n    curve_angle = get_radians(-curve_extent)\n    x = (o_x - d_x)/curve_position\n    y = (o_y - d_y)/curve_position\n    c_x = d_x + x * cos(curve_angle) - y * sin(curve_angle)\n    c_y = d_y + y * cos(curve_angle) + x * sin(curve_angle)\n    d <- tibble::tibble(x = c(o_x, c_x, d_x), y = c(o_y, c_y,\n        d_y), od_pair = od_pair)\n    return(d)\n}\n\n# Convert degrees to radians.\nget_radians <- function(degrees) { (degrees * pi) / (180) }\n\nR Scripts are more straightforward than Quarto files in that you don’t have to worry about configuring code chunks. They are really useful for quickly developing bits of code. This can be achieved by highlighting over the code that you wish to execute and clicking the Run icon at the top of the Code Editor pane or by typing ctrl + rtn on Windows, ⌘ + rtn on macOS.\n\n1.3.6 Create an RStudio Project\nThroughout the book we will use project-oriented workflows. This is where all files pertaining to a data analysis – data, code and outputs – are organised from a single root folder and where file path discipline is used such that all paths are relative to the project’s root folder (see Bryan & Hester 2020). You can imagine this self-contained project set-up is necessary for achieving reproducibility of your research. It allows anyone to take a project and run it on their own machines with minimal adjustment.\nWhen opening RStudio, the IDE automatically points to a working directory, likely the home folder for your local machine. RStudio will save any outputs to this folder and expect any data you use to be saved there. Clearly if you want to incorporate neat, self-contained project workflows then you will want a dedicated project folder rather than the default home folder for your machine. This can be achieved with the setwd(<path-to-your-project>) function. The problem with doing this is that you insert a path which cannot be understood outside of your local machine at the time it was created. This is a real pain. It makes simple things like moving projects around on your machine an arduous task and most importantly it hinders reproducibility if others are to reuse your work.\nRStudio Projects resolve these problems. Whenever you load an RStudio Project, R starts up and the working directory is automatically set to the project’s root folder. If you were to move the project elsewhere on your machine, or to another machine, a new root is automatically generated – so RStudio projects ensure that relative paths work.\n\n\n\n\nFigure 1.3: Creating an RStudio Project\n\n\n\n\n\nLet’s create a new Project for this book:\n\nSelect File > New Project > New Directory.\nBrowse to a sensible location and give the project a suitable name. Then click Create Project.\n\nYou will notice that the top of the Console window now indicates the root for this new project (~projects/vis4sds).\n\nIn the root of your project, create folders called reports, code, data, figures.\nSave this session’s 01-template.qmd file to the reports folder.\n\nYour project’s folder structure should now look like this:\nvis4sds\\\n  vis4sds.Rproj\n  code\\\n  data\\\n  figures\\\n  reports\\\n    01-template.qmd"
  },
  {
    "objectID": "01-intro.html#conclusions",
    "href": "01-intro.html#conclusions",
    "title": "1  Introduction",
    "section": "\n1.4 Conclusions",
    "text": "1.4 Conclusions\nVisual data analysis approaches are necessary for exploring complex patterns in data and to make and communicate claims under uncertainty. This is especially true of social data science applications, where: datasets are repurposed for research often for the first time; contain complex structure and geo-spatial relations that cannot be easily captured by statistical summaries alone; and, consequently, where the types of questions that can be asked and the techniques deployed to answer them cannot be specified in advance. This is demonstrated in the book as we explore (Chapter 4 and 5), model under uncertainty (Chapter 6) and communicate (Chapter 7 and 8) with various social science datasets. All technical activity is completed in R, making use of tools and software libraries that form part of the R ecosystem: the tidyverse for doing modern data science and Quarto for authoring reproducible research projects."
  },
  {
    "objectID": "02-data.html",
    "href": "02-data.html",
    "title": "2  Data Fundamentals",
    "section": "",
    "text": "By the end of this chapter you should gain the following knowledge and practical skills."
  },
  {
    "objectID": "02-data.html#introduction",
    "href": "02-data.html#introduction",
    "title": "2  Data Fundamentals",
    "section": "\n2.1 Introduction",
    "text": "2.1 Introduction\nThis chapter covers the basics of how to describe and organise data. Whilst this might sound prosaic, there are several reasons why being able to consistently describe a dataset is important. First, it is the initial step in any analysis and helps delimit the analytical procedures that can be deployed. This is especially relevant to data science-type workflows, where it is common to apply the same analysis templates for working over many different datasets. Describing data using a consistent vocabulary enables you to identify which analysis templates to reuse. Second relates to the point in Chapter 1 that social data science projects usually involve repurposing datasets for the first time. It is often not obvious whether the data contain sufficient detail and structure to characterise the behaviours being researched and the target populations they are assumed to represent. This leads to additional levels of uncertainty and places greater importance on the initial steps of data processing, description and exploration.\nThrough the chapter we will learn vocabulary for describing and thinking about data, as well as some of the most important data processing and organisation techniques in R . We will do so using data from New York’s Citibike scheme.\n\n\n\n\n\n\n\nData vocabulary\n\n\n\nApplying a consistent vocabulary to describing your data is especially useful to learning modern visualization toolkits (ggplot2, Tableau, vega-lite), and will be covered in some detail in Chapter 3 as we introduce Visualization Fundamentals and the Grammar of Graphics (Wilkinson 1999)."
  },
  {
    "objectID": "02-data.html#concepts",
    "href": "02-data.html#concepts",
    "title": "2  Data Fundamentals",
    "section": "\n2.2 Concepts",
    "text": "2.2 Concepts\n\n2.2.1 Data frames\nThroughout this book we will work with data frames. These are spreadsheet-like representations where rows are observations and columns are variables. In an R data frame, variables are vectors that must be of equal length. Where observations have missing values for certain variables, the missing values must be substituted with something, usually with NA or similar. This constraint can cause difficulties. For example, when working with variables that contain many values of different length for an observation. In these cases we create a special class of column, a list-column. Organising data according to this simple structure – rows as observations, columns as variables – is especially useful for applying analysis templates typical of the tidyverse ecosystem.\n\n2.2.2 Types of variable\nA familiar classification for describing variables is that developed by Stevens (1946) when cosidering the level of measurement of a variable. Stevens (1946) organises variables into two classes: variables that describe categories of things and variables that describe measurements of things.\nCategories include attributes like gender, titles, ranked orders (1st, 2nd, 3rd largest etc.). Measurements include quantities like distance, age, travel time. Categories can be further subdivided into those that are unordered (nominal) from those that are ordered (ordinal). Measurements can also be subdivided. Interval measurements are quantities where the computed difference between two values is meaningful. Ratio measurements have this property, but also have a meaningful 0, where 0 means the absence of something, and the ratio of two values can be computed.\n\nWhy is this useful? The measurement level of a variable determines the types of data analysis operations that can be performed and therefore allows us to make quick decisions when working with a dataset for the first time (Table 4.1).\n\n\n\n\n\nTable 2.1:  Breakdown of variable types \n \n Measurement \n    Example \n    Operators \n    Midpoint \n    Spread \n  \n\nCategories\n\n Nominal \n    Political parties; street names \n    =  ≠ \n    mode \n    entropy \n  \n\n Ordinal \n    Terrorism threat levels \n    =  ≠  \n    median \n    percentile \n  \nMeasures\n\n Interval \n    Temperatures; years \n    =  ≠  +  - \n    mean \n    variance \n  \n\n Ratio \n    Distances; prices \n    =  ≠  +  - | × ÷ \n    mean \n    variance \n  \n\n\n\n\n\n\n\n\n\n\n\nTask\n\n\n\nComplete the data description table below identifying the measurement level of each variable in the New York bikeshare stations dataset below.\n\n\nVariable name\nVariable value\nMeasurement level\n\n\n\nname\n“Central Park”\n\n\n\ncapacity\n80\n\n\n\nrank_capacity\n45\n\n\n\ndate_opened\n“2014-05-23”\n\n\n\nlongitude\n-74.00149746\n\n\n\nlatitude\n40.74177603\n\n\n\n\n\n\n\n2.2.3 Types of observation\nObservations either together form an entire population or a sample that we expect is representative of a target population. In social data science applications we often work with datasets that are so-called population-level. The Citibike dataset is a complete, population-level dataset in that every journey made through the scheme is recorded. Whether or not this is truly a population-level dataset, however, depends on the analysis purpose. When analysing the bikeshare dataset are we interested only in describing use within the Citibike scheme? Or are we taking the patterns observed through our analysis to make claims and inferences about cycling more generally? If the latter, then there are problems as the level of detail we have on our sample is pretty trivial compared to traditional, actively-collected datasets, where data collection activities are designed with a specified target population in mind. It may therefore be difficult to gauge how representative Citibike users and Citibike cycling is of New York’s general cycling population. The flipside is that so-called passively-collected data do not suffer from the same problems such as non-response bias and social-desirability bias as traditional, actively-collected data.\n\n2.2.4 Tidy data\nWe will be working with data frames organised such that columns always and only refer to variables and rows always and only refer to observations. This arrangement, called tidy (Wickham 2014), has two key advantages. First, if data are arranged in a consistent way, then it is easier to apply and re-use tools for wrangling them due to data having the same underlying structure. Second, placing variables into columns, with each column containing a vector of values, means that we can take advantage of R’s vectorised functions for transforming data – we will demonstrate this in the technical element of the chapter.\nThe three rules for tidy data:\n\nEach variable forms a column.\nEach observation forms a row.\nEach type of observational unit forms a table.\n\nThe concept, and its usefulness for tidyverse-style operations, is best explained through example. The technical element to this chapter is therefore comparatively lengthy and demonstrates key coding templates used extensively through this book for organising and re-organising data."
  },
  {
    "objectID": "02-data.html#techniques",
    "href": "02-data.html#techniques",
    "title": "2  Data Fundamentals",
    "section": "\n2.3 Techniques",
    "text": "2.3 Techniques\nThe technical element to this chapter imports, describes, transforms and tidies data from New York’s Citibike scheme.\n\nDownload the 02-template.qmd file and save it to the reports folder of your vis4sds project, created in Chapter 1.\nOpen your vis4sds project in RStudio and load the template file by clicking File > Open File ... > reports/02-template.qmd.\n\n\n2.3.1 Import\nIn the template file there is documentation on how to setup your R session with key packages – tidyverse , fst, lubridate, sf. The data were collected using the bikedata package, and in the template are documented code chunks demonstrating how to download and process data using bikedata’s API. A subset of data from New York’s bikeshare scheme, Citibike, were collected for this chapter and can be downloaded from the book’s acccompanying data repository.\n\n\nThe code for reading in these data may be familiar to most readers. The here package, which reliably creates paths relative to a project’s root, is used to pass as a parameter to read_csv() and read_fst() the locations at which the New York trips and stations data are stored. Notice that we use assignment (<-), so data are loaded as objects and appear in the Environment pane of your RStudio window.\n\n\n# Read in these local copies of the trips and stations data.\nny_trips <- read_fst(here(\"data\", \"ny_trips.fst\"))\nny_stations <- read_csv(here(\"data\", \"ny_stations.csv\"))\n\n\n\n\n\n\n\nNote\n\n\n\nfst is a special class of file that implements in the background various operations to speed-up reading and writing of data. This makes it possible to work with large datasets in-memory in R rather than connecting to a database and serving up summaries/subsets.\n\n\n\nInspecting the layout of the stations data with View(ny_stations) you will notice that the top line is the header and contains column (variable) names.\n\n\n\n\nFigure 2.1: ny_trips and ny_stations as they appear when calling View().\n\n\n\n\n\nThe glimpse() function can be used to quickly describe a data frame’s dimensions. We have 500,000 trip observations in ny_trips and 11 variables; the 500,000 represents a random sample of the c.1.9 million trips recorded in June 2020. The function also prints out the object type for each of these variables, with the variables either of type int, chr or dbl.\nIn this case the assignment needs correcting. start_time and stop_time may be better represented in date-time format; the station identifier variables are more efficient when converted to int types; and the geographic coordinates, currently stored as text strings (chr) are better converted as floating points (dbl) or POINT geometry types. In the 02-template.qmd file are code chunks for doing these conversions. There are some slightly more involved data transform procedures in this code, which you may wish to ignore at this stage.\n\nglimpse(ny_trips)\n## Rows: 500,000\n## Columns: 11\n## $ id               <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21…\n## $ city             <chr> \"ny\", \"ny\", \"ny\", \"ny\", \"ny\", \"ny\", \"ny\", \"ny\", \"ny\", \"ny\", \"ny\", \"ny\", \"…\n## $ trip_duration    <dbl> 1062, 3810, 1017, 226, 1437, 355, 99, 1810, 87, 2714, 2096, 1611, 529, 69…\n## $ start_time       <chr> \"2020-06-01 00:00:03\", \"2020-06-01 00:00:03\", \"2020-06-01 00:00:09\", \"202…\n## $ stop_time        <chr> \"2020-06-01 00:17:46\", \"2020-06-01 01:03:33\", \"2020-06-01 00:17:06\", \"202…\n## $ start_station_id <chr> \"ny3419\", \"ny366\", \"ny389\", \"ny3255\", \"ny367\", \"ny248\", \"ny3232\", \"ny3263…\n## $ end_station_id   <chr> \"ny3419\", \"ny336\", \"ny3562\", \"ny505\", \"ny497\", \"ny247\", \"ny390\", \"ny496\",…\n## $ bike_id          <chr> \"39852\", \"37558\", \"37512\", \"39674\", \"21093\", \"39594\", \"43315\", \"16571\", \"…\n## $ user_type        <chr> \"Customer\", \"Subscriber\", \"Customer\", \"Customer\", \"Customer\", \"Subscriber…\n## $ birth_year       <chr> \"1997\", \"1969\", \"1988\", \"1969\", \"1997\", \"1990\", \"1938\", \"1995\", \"1971\", \"…\n## $ gender           <dbl> 2, 0, 2, 0, 2, 1, 2, 2, 2, 1, 1, 0, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n\n\nglimpse(ny_stations)\n## Rows: 1,010\n## Columns: 6\n## $ id        <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 2…\n## $ city      <chr> \"ny\", \"ny\", \"ny\", \"ny\", \"ny\", \"ny\", \"ny\", \"ny\", \"ny\", \"ny\", \"ny\", \"ny\", \"ny\", \"n…\n## $ stn_id    <chr> \"ny116\", \"ny119\", \"ny120\", \"ny127\", \"ny128\", \"ny143\", \"ny144\", \"ny146\", \"ny150\",…\n## $ name      <chr> \"W 17 St & 8 Ave\", \"Park Ave & St Edwards St\", \"Lexington Ave & Classon Ave\", \"B…\n## $ longitude <chr> \"-74.00149746\", \"-73.97803415\", \"-73.95928168\", \"-74.00674436\", \"-74.00297088\", …\n## $ latitude  <chr> \"40.74177603\", \"40.69608941\", \"40.68676793\", \"40.73172428\", \"40.72710258\", \"40.6…\n\n\n\n\n\n2.3.2 Transform\nTransform with dplyr and pipes (|>)\ndplyr is perhaps the package within the tidyverse. It provides a grammar of data manipulation, with access to functions that can be variously combined to support most data processing and transformation tasks. Once familiar with dplyr functions you will find yourself generating analysis templates to re-use whenever working on a dataset.\nAll dplyr functions operate in a consistent way:\n\nStart with a data frame.\nPass arguments to a function performing some updates to the data frame.\nReturn the updated data frame.\n\nSo every dplyr function expects a data frame and will always return a data frame.\n\n\n\n\n\nTable 2.2:  dplyr funcitions (verbs) for manipulating data frames. \n \n function() \n    Description \n  \n\n\n filter() \n    Picks rows (observations) if their values match a specified criteria \n  \n\n arrange() \n    Reorders rows (observations) based on their values \n  \n\n select() \n    Picks a subset of columns (variables) by name (or name characteristics) \n  \n\n rename() \n    Changes the name of columns in the data frame \n  \n\n mutate() \n    Adds new columns \n  \n\n group_by() \n    Chunks the dataset into groups for grouped operations \n  \n\n summarise() \n    Calculates single-row (non-grouped) or multiple-row (if grouped) summary values \n  \n\n ..and more \n     \n  \n\n\n\n\n\ndplyr is most effective when its functions are chained together – you will see this shortly as we explore the New York bikeshare data. This chaining of functions can be achieved using the pipe operator (|>). Pipes are used for passing information in a program. They take the output of a set of code (a dplyr specification) and make it the input of the next set (another dplyr specification). Pipes can be easily applied to dplyr functions and the functions of all packages that form the tidyverse. We mentioned in Chapter 1 that ggplot2 provides a framework for specifying a layered grammar of graphics (more on this in Chapter 3). Together with the pipe operator (|>), dplyr supports a layered grammar of data manipulation. You will see this throughout the book as we develop and re-use code templates for performing some data manipulation that is then piped to a ggplot2 specification for visual analysis.\n\ncount() and summarise() over rows\nLet’s use and combine some dplyr functions to generate statistical summaries on the New York bikeshare data. First we’ll count the number of trips made in June 2020 by gender. dplyr has a convenience function for counting, so we could run the code below, also in the 02-template.qmd for this chapter.\n\n# Take the ny_trips data frame.\nny_trips |> \n  # Run the count function over the data frame \n  # and set the sort parameter to TRUE.\n  count(user_type, sort=TRUE) \n##    user_type      n\n## 1 Subscriber 347204\n## 2   Customer 152796\n\nThere are a few things happening in the count() function. It takes the gender variable from ny_trips, organises or groups the rows in the data frame according to its values (female | male | unknown), counts the rows and then orders the summarised output descending on the counts. Often you will want to do more than simply counting and you may also want to be more explicit in the way the data frame is grouped for computation. We’ll demonstrate this here with a more involved analysis of the usage data and with some key aggregate functions (Table 2.3).\nA common workflow is to combine group_by() and summarise(), and in this case arrange() to replicate the count() example.\n\n# Take the ny_trips data frame.\nny_trips |> \n  # Group by user_type.\n  group_by(user_type) |> \n    # Count the number of observations per group.\n    summarise(count=n()) |>\n    # Arrange the grouped and summarised (collapsed) rows \n    # according to count. \n    arrange(desc(count)) \n## # A tibble: 2 × 2\n##   user_type   count\n##   <chr>       <int>\n## 1 Subscriber 347204\n## 2 Customer   152796\n\nIn ny_trips there is a variable measuring trip duration in seconds (trip_duration) and distinguishing casual users from those formally registered to use the scheme (user_type - customer vs. subscriber). It may be instructive to calculate some summary statistics to see how trip duration varies between these groups. The code below uses group_by(), summarise() and arrange() in exactly the same way, but with the addition of other aggregate functions summarises the trip_duration variable according to central tendency (mean and standard deviation) and by user_type.\n\n# Take the ny_trips data frame.\nny_trips |> \n  mutate(trip_duration=trip_duration/60) |>\n  # Group by user type.\n  group_by(user_type) |> \n  # Summarise over the grouped rows, \n  # generate a new variable for each type of summary.\n  summarise( \n    count=n(),\n    avg_duration=mean(trip_duration),\n    median_duration=median(trip_duration),\n    sd_duration=sd(trip_duration),\n    min_duration=min(trip_duration),\n    max_duration=max(trip_duration)\n    ) |>\n  # Arrange on the count variable.\n  arrange(desc(count)) \n\n## # A tibble: 2 × 7\n##   user_type   count avg_duration median_duration sd_duration min_duration max_dur…¹\n##   <chr>       <int>        <dbl>           <dbl>       <dbl>        <dbl>     <dbl>\n## 1 Subscriber 347204         20.3            14.4        116.         1.02    33090.\n## 2 Customer   152796         43.3            23.1        383.         1.02    46982.\n## # … with abbreviated variable name ¹​max_duration\n\nAs each line is commented you hopefully get a sense of what is happening in the code above. You will notice that dplyr functions read like verbs, and this is a very deliberate design decision. With the code laid out as above – each dplyr verb occupying a single line, separated by a pipe (|>) – you can generally understand the code with a cursory glance. There are obvious benefits to this. Once you are familiar with dplyr it becomes very easy to read, write and share code.\n\n\n\n\n\nTable 2.3:  A breakdown of aggregate functions commonly used with summarise(). \n \n Function \n    Description \n  \n\n\n n() \n    Counts the number of observations \n  \n\n n_distinct(var) \n    Counts the number of unique observations \n  \n\n sum(var) \n    Sums the values of observations \n  \n\n max(var)|min(var) \n    Finds the min|max values of observations \n  \n\n mean(var)|median(var)| ... \n    Calculates central tendency of observations \n  \n\n ... \n    Many more \n  \n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nRemembering that pipes (|>) take the output of a set of code and make it the input of the next set, you will notice separate lines for each call to the pipe operator. This is good practice for supporting readability of your code.\n\n\nManipulate dates with lubridate\n\nLet’s continue this investigation of trips by user-type by profiling how usage varies over time. To do this we will need to work with date-time variables. The lubridate package provides various convenience functions for this.\nIn the code block below we extract the day of week and hour of day from the start_time variable using lubridate’s day accessor functions. Documentation on these can be accessed in the usual way (?<function-name>). Next we count the number of trips made by hour of day, day of week and user-type. The summarised data frame will be re-used several times in our analysis, so we store it as an object with a suitable name (ny_temporal) using the assignment operator.\n\n# Create an hour of day and day of week summary by gender and assign it the \n# name \"ny_temporal\". \nny_temporal <- ny_trips |>  \n  mutate(\n    # Create a new column identify dow.\n    day=wday(start_time, label=TRUE), \n    # Create a new column identify hod.\n    hour=hour(start_time)) |> \n  # Group by day, hour, user_type.\n  group_by(user_type, day, hour) |> \n  # Count the grouped rows.\n  summarise(count=n()) |> \n  ungroup()\n\n\n\n\n\n\n\nNote\n\n\n\nWhether or not to store derived data tables, like the newly assigned ny_temporal, in a session is not an easy decision. You should avoid cluttering the Environment pane with many data objects. Often when generating charts it is necessary to create these sorts of derived tables as input data to ggplot2. Assigning derived data tables to the Environment pane each time an exploratory plot is created risks an unhelpfully large number of such tables. A general rule: if the derived table is to be used more than three times in a data analysis or is computationally intensive, assign it (<-) a named object.\n\n\nIn Figure 2.2, the derived data are plotted (code for creating the graphic is in the template file). The plot demonstrates a familiar weekday-weekend pattern of usage. Trip frequencies peak in the morning and evening rush hours during weekdays and mid/late-morning and afternoon during weekends, with the weekday afternoon peak much larger than the morning peak. There are obvious differences in the type of trips made by subscribers versus customers – the temporal signature for subscribers appears to match more closely what one would expect of commuting behaviour.\n\n\n\n\nFigure 2.2: Line charts generated with ggplot2. Plot data computed using dplyr and lubridate.\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThis analysis is based on data from June 2020, a time when New York residents were emerging from lockdown. It would be instructive to compare with data from a non-Covid year. The fact that bikeshare is collected continuously makes this sort of behavioural change analysis possible.\n\n\nRelate tables with join()\n\nTrip distance is not recorded directly in the ny_trips table, but may be important for profiling usage behaviour. Since ny_stations contains coordinates corresponding to station locations, distances can be easily calculated by linking these station coordinates to the origin and destination stations recorded in ny_trips. To relate the two tables, we need to specify a join between them.\nA sensible approach is to:\n\nSelect all uniquely cycled trip pairs (origin-destination pairs) that appear in the ny_trips table.\nBring in the corresponding coordinate pairs representing the origin and destination stations by joining on the ny_stations table.\nCalculate the distance between the coordinate pairs representing the origin and destination.\n\nThe code below is one way of achieving this.\n\n# Take the ny_trips data frame.\nod_pairs <- ny_trips |> \n  # Select trip origin and destination (OD) station columns \n  # and extract unique OD pairs.\n  select(start_station_id, end_station_id) |>  unique() |> \n  # Select lon, lat columns from ny_stations and join on origin column.\n  left_join(\n    ny_stations |> select(stn_id, longitude, latitude), \n    by=c(\"start_station_id\"=\"stn_id\")\n    ) |> \n  # Rename new lon, lat columns -- associate with origin station.\n  rename(o_lon=longitude, o_lat=latitude) |>  \n  # Select lon, lat columns from ny_stations and join on destination column.\n  left_join(\n    ny_stations |> select(stn_id, longitude, latitude), \n    by=c(\"end_station_id\"=\"stn_id\")\n    ) |> \n  # Rename new lon, lat columns and associate with destination station.\n  rename(d_lon=longitude, d_lat=latitude) |>  \n  # Compute distance calculation one row-at-a-time.\n  rowwise() |> \n  # Calculate distance and express in kms.\n  mutate(dist=geosphere::distHaversine(c(o_lat, o_lon), c(d_lat, d_lon))/1000) |> \n  ungroup()\n\nThe code block above introduces some new functions: select() to pick or drop variables, rename() to rename variables and a convenience function for calculating straight line distance from polar coordinates (distHaversine()). The key function to emphasise is the left_join(). If you’ve worked with relational databases, dplyr’s join functions will be familiar to you. In a left_join, all the values from the first (left-most) table are retained, ny_trips in this case, and variables from the table on the right , ny_stations, are added. We specify the variable on which tables should be joined with the by= parameter, station_id in this case. If there is a station_id in ny_trips that doesn’t exist in ny_stations then corresonding cells are filled out with NA.\n\n\n\n\n\nTable 2.4:  A breakdown of dplyr join functions. \n\n*_join(x, y) ...\n\n\n\n\n left_join() \n    all rows from table x \n  \n\n right_join() \n    all rows from table y \n  \n\n full_join() \n    all rows from both table x and y \n  \n\n `semi_join()` \n    all rows from table x where there are matching values in table y, keeping just columns from table x \n  \n\n inner_join() \n    all rows from table x where there are matching values in table y, returning all combinations where there are multiple matches \n  \n\n anti_join \n    all rows from table x where there are not matching values in table y, never duplicating rows of table x \n  \n\n\n\n\n\n\n\n\n\nFigure 2.3: Citibike trip distance (straight-line km). Histograms generated with ggplot2. Plot data computed using dplyr and lubridate\n\n\n\n\n\nFrom the newly created distance variable, we can calculate the average (mean) trip distance for the 500,000 trips – 1.6km. This might seem very short, but remember that these are straight-line distances between pairs of docking stations. Ideally we would calculate network distances derived from cycle trajectories. A separate reason, discovered when generating a histogram on the dist variable, is that there are a large number of trips that start and end at the same docking station. Initially these might seem to be unsuccessful hires – people failing to undock a bike for example. We could investigate this further by paying attention to the docking stations at which same origin-destination trips occur, as in the code block below.\n\nny_trips |> \n  filter(start_station_id==end_station_id) |> \n  group_by(start_station_id) |>  summarise(count=n()) |> \n  left_join(\n    ny_stations |>   select(stn_id, name), \n    by=c(\"start_station_id\"=\"stn_id\")\n    ) |> \n  arrange(desc(count))\n## # A tibble: 958 x 3\n##    start_station_id count name\n##    <chr>            <int> <chr>\n##  1 ny3423            2017 West Drive & Prospect Park West\n##  2 ny3881            1263 12 Ave & W 125 St\n##  3 ny514             1024 12 Ave & W 40 St\n##  4 ny3349             978 Grand Army Plaza & Plaza St West\n##  5 ny3992             964 W 169 St & Fort Washington Ave\n##  6 ny3374             860 Central Park North & Adam Clayton Powell Blvd\n##  7 ny3782             837 Brooklyn Bridge Park - Pier 2\n##  8 ny3599             829 Franklin Ave & Empire Blvd\n##  9 ny3521             793 Lenox Ave & W 111 St\n## 10 ny2006             782 Central Park S & 6 Ave\n## # … with 948 more rows\n\nAll of the top 10 docking stations are either in parks, near parks or located along the river. This coupled with the fact that same origin-destination trips occur in much greater relative number for casual users, associated with discretionary leisure-oriented cycling, than regular users (Customer vs Subscriber) is further evidence that these are valid trips.\nWrite functions of your own\nThere may be times where you need to make functions of your own. Most often this is when you find yourself copy-pasting the same chunks of code with minimal adaptation.\nFunctions have three key characteristics:\n\nThey are (usually) named – the name should be expressive and communicate what the function does (e.g. dplyr functions read like verbs).\nThey have brackets <function()> usually containing arguments – inputs, which determine what the function does and returns.\nImmediately followed by <function()> are {} used to contain the body – code that performs a distinct task, described by the function’s name.\n\nEffective functions are short, perform single, discrete operations and are intuitive.\nYou will recall that in the ny_trips table there is a variable called birth_year. From this we can derive cyclists’ approximate age. Below is a function called get_age(). The function expects two arguments: yob – a year of birth as type chr; and yref – a reference year. In the body, lubridate’s as.period() function is used to calculate the time in years that elapsed between these dates.\n\n# get_age() depends on lubridate.\nlibrary(lubridate)\n# Function for calculating time elapsed between two dates in years (age).\nget_age <- function(yob, yref) {\n    period <- as.period(interval(yob, yref),unit = \"year\")\n    return(period$year)\n}\n\nny_trips <- ny_trips |> \n  # Calculate age from birth_date.\n  mutate(\n    age=get_age(\n      as.POSIXct(birth_year, format=\"%Y\"), \n      as.POSIXct(\"2020\", format=\"%Y\")\n    ) \n  )\n\nWe can use the two new derived variables – distance travelled and age – in our analysis. In Figure 2.4, we explore how approximate travel speeds vary by age, trip distance and customer type. The code to generate the summary data and plot is in the template file. Again the average speed calculation should be treated cautiously as it is based on straight line distances and it is likely that this will vary depending on whether the trip is made for ‘utilitarian’ or ‘leisure’ purposes. Additionally, due to the heavy subsetting, data become a little volatile for certain age groups and so the age variable is aggregated into 5-year bands.\nThere are some notable patterns in Figure 2.4. Subscribers make faster trips than do Customers, although this gap narrows as trip distance increases. Trips with a straight-line distance of 4.5km are non-trivial and so may be classed as ‘utilitarian’ even for non-regular Customers. There is a very slight effect of decreasing trip speed by age cycled for the longer trips. The volatility in the older age groups for trips >4.5km suggests more data and a more involved analysis is required to confidently establish this. For example, it may be that the comparatively rare occurrence of trips in the 65-70 age group is made by only a small subset of cyclists; with a larger dataset we may expect a regression to the mean effect and negate any noise caused by outlier individuals.\n\n\n\n\nFigure 2.4: Citibike average trip speeds (approximate) by age, customer type and trip distance. Line charts generated with ggplot2. Plot data computed using dplyr and lubridate.\n\n\n\n\n\n\n2.3.3 Tidy\n\n\nThe ny_trips and ny_stations data already comply with the rules for tidy data (Wickham 2014). Each row in ny_trips is a distinct trip and each row in ny_stations a distinct station. However, it is common to encounter datasets that are untidy and must be reshaped. In the book’s data repository are two examples: ny_spread_rows and ny_spread_columns. ny_spread_rows is so-called because the variable summary_type is spread across the rows (observations); ny_spread_columns because multiple variables are stored in single columns – the dist_weekday, duration_weekday columns.\n\nny_spread_rows\n## # A tibble: 411,032 × 6\n##    o_station d_station wkday   count summary_type  value\n##        <dbl>     <dbl> <chr>   <dbl> <chr>         <dbl>\n##  1        72       116 weekend     1 dist           1.15\n##  2        72       116 weekend     1 duration      18.2 \n##  3        72       127 weekend     4 dist           7.18\n##  4        72       127 weekend     4 duration     122.  \n##  5        72       146 weekend     4 dist           9.21\n##  6        72       146 weekend     4 duration     122.  \n##  7        72       164 weekend     1 dist           2.66\n##  8        72       164 weekend     1 duration      12.5 \n##  9        72       173 weekend     2 dist           2.13\n## 10        72       173 weekend     2 duration      43.6 \n## # … with 411,022 more rows\n\nny_spread_columns\n## # A tibble: 156,449 × 8\n##    o_station d_station count_weekend count_weekday dist_w…¹ dist_…² durat…³ durat…⁴\n##        <dbl>     <dbl>         <dbl>         <dbl>    <dbl>   <dbl>   <dbl>   <dbl>\n##  1        72       116             1             3     1.15    3.45    18.2    49.9\n##  2        72       127             4             4     7.18    7.18   122.    101. \n##  3        72       146             4             2     9.21    4.61   122.     64.1\n##  4        72       164             1             1     2.66    2.66    12.5    43.2\n##  5        72       173             2            13     2.13   13.9     43.6   189. \n##  6        72       195             1             4     2.56   10.2     24.7    98.3\n##  7        72       212             3             3     4.83    4.83    40.3    54.0\n##  8        72       223             1            NA     1.13   NA       21.1    NA  \n##  9        72       228             2             1     4.97    2.49    30.2    13.6\n## 10        72       229             1            NA     1.22   NA       39.2    NA  \n## # … with 156,439 more rows, and abbreviated variable names ¹​dist_weekend,\n## #   ²​dist_weekday, ³​duration_weekend, ⁴​duration_weekday\n\nTo re-organise the table in tidy form, we should identify what constitutes a distinct observation – an origin-destination pair summarising counts, distances and durations of trips that occur during the weekday or weekend. From here, the table’s variables are:\n\n\no_station: station id of the origin station\n\nd_station: station id of the destination station\n\nwkday: trip occurs on weekday or weekend\n\ncount: count of trips for observation type\n\ndist: total distance in kms (cumulative) of trips for observation type\n\nduration: total duration in minutes (cumulative) of trips for observation type\n\nThere are two functions for reshaping untidy data, from the tidyr package: pivot_longer() and pivot_wider(). pivot_longer() is used to tidy data in which observations are spread across columns; pivot_wider() to tidy data in which variables are spread across rows. The functions are especially useful in visual data analysis to fix messy data, but also flexibly reshape data supplied to ggplot2 specifications (more on this in Chapters 3 and 4).\nTo fix ny_spread_rows, we use pivot_wider() and pass to the function’s arguments the name of the problematic column and the column containing values used to populate the newly created columns.\n\nny_spread_rows |> \n   # names_from is column containing what will be column names in new layout (dist, duration).\n   # values_from column containing what will be values in new layout (recorded dists, durations).\n   pivot_wider(names_from=summary_type, values_from=value)\n\n## # A tibble: 205,516 × 6\n##    o_station d_station wkday   count  dist duration\n##        <dbl>     <dbl> <chr>   <dbl> <dbl>    <dbl>\n##  1        72       116 weekend     1  1.15     18.2\n##  2        72       127 weekend     4  7.18    122. \n##  3        72       146 weekend     4  9.21    122. \n##  4        72       164 weekend     1  2.66     12.5\n##  5        72       173 weekend     2  2.13     43.6\n##  6        72       195 weekend     1  2.56     24.7\n##  7        72       212 weekend     3  4.83     40.3\n##  8        72       223 weekend     1  1.13     21.1\n##  9        72       228 weekend     2  4.97     30.2\n## 10        72       229 weekend     1  1.22     39.2\n## # … with 205,506 more rows\n\nTo fix ny_spread_columns requires a little more thought. First we pivot_longer() on columns that are muddled with multiple variables. This results in a long and thin dataset similar to ny_spread_rows – each row is the origin-destination pair of trip with either a count, distance or duration recorded for trips occuring at weekdays or weekdays. The muddled variables, for example dist_weekend duration_weekday, now appear in the rows of a new column with the default title name. This column is separated on the _ mark to create two new columns, summary_type and wkday, used in pivot_wider().\n\nny_spread_columns |> \n  pivot_longer(cols = count_weekend:duration_weekday) |> \n  separate(col = name, into = c(\"summary_type\", \"wkday\"), sep = \"_\") |> \n  pivot_wider(names_from = summary_type, values_from = value)\n\n## # A tibble: 312,898 × 6\n##    o_station d_station wkday   count  dist duration\n##        <dbl>     <dbl> <chr>   <dbl> <dbl>    <dbl>\n##  1        72       116 weekend     1  1.15     18.2\n##  2        72       116 weekday     3  3.45     49.9\n##  3        72       127 weekend     4  7.18    122. \n##  4        72       127 weekday     4  7.18    101. \n##  5        72       146 weekend     4  9.21    122. \n##  6        72       146 weekday     2  4.61     64.1\n##  7        72       164 weekend     1  2.66     12.5\n##  8        72       164 weekday     1  2.66     43.2\n##  9        72       173 weekend     2  2.13     43.6\n## 10        72       173 weekday    13 13.9     189. \n## # … with 312,888 more rows"
  },
  {
    "objectID": "02-data.html#conclusions",
    "href": "02-data.html#conclusions",
    "title": "2  Data Fundamentals",
    "section": "\n2.4 Conclusions",
    "text": "2.4 Conclusions\nDeveloping the vocabulary and technical skills to systematically describe and organise data is crucial to modern data analysis. This chapter has covered the fundamentals: that data consist of observations and variables of different types (Stevens 1946) and that in order to work effectively with datasets, these data must be organised according to the rules of tidy data (Wickham 2014). Most of the content was dedicated to the techniques that enable these concepts to be operationalised. We covered how to download, transform and reshape a reasonably large dataset from New York’s Citibike scheme. In doing so, we generated insights that might inform further data collection and analysis activity. In the next chapter we will extend this conceptual and technical knowledge as we introduce the fundamentals of visual data analysis and ggplot2’s grammar of graphics."
  },
  {
    "objectID": "03-visual.html",
    "href": "03-visual.html",
    "title": "3  Visualization Fundamentals",
    "section": "",
    "text": "By the end of this chapter you should gain the following knowledge and practical skills."
  },
  {
    "objectID": "03-visual.html#introduction",
    "href": "03-visual.html#introduction",
    "title": "3  Visualization Fundamentals",
    "section": "\n3.1 Introduction",
    "text": "3.1 Introduction\nThis chapter outlines the fundamentals of visualization design. It offers a position on what effective data graphics should do, before discussing the processes that take place when creating data graphics. It will outline a framework – a vocabulary and grammar – for supporting this process which, combined with established knowledge around visual perception, can be used to describe, evaluate and create effective data graphics. Talking about a vocabulary and grammar of data and graphics may sound somewhat abstract. However, through an analysis of 2019 General Election results data the chapter will demonstrate how these concepts are fundamental to visual data analysis."
  },
  {
    "objectID": "03-visual.html#concepts",
    "href": "03-visual.html#concepts",
    "title": "3  Visualization Fundamentals",
    "section": "\n3.2 Concepts",
    "text": "3.2 Concepts\n\n3.2.1 Effective data graphics\nData graphics take numerous forms and are used in many different ways by scientists, journalists, designers and many more. Whilst the intentions of those producing data graphics varies, those that are effective generally have the following characteristics:\n\nRepresent complex datasets, exposing structure, connections and comparisons that could not be achieved easily via other means.\nAre data rich, presenting many numbers in a small space.\nReveal patterns at several levels of detail, from broad overview to fine structure.\nHave elegance, emphasising dimensions of a dataset without extraneous details.\nGenerate an aesthetic response, encouraging people to engage with the data or question.\n\nGiven these characteristics, consider the data graphic below (Figure 3.1), which presents an analysis of the 2016 US Presidential Election, or the Peaks and Valleys of Trump and Clinton’s Support. The map is reproduced from an article in the The Washington Post. Included in the bottom margin is a choropleth map coloured according to party majority, more standard practice for reporting county-level voting.\nThe Washington Post graphic is clearly data rich, encoding many more data items than does the standard choropleth. It is not simply the data density that makes the graphic successful; there are careful design choices which help support comparison and emphasise complex structure. By varying the height of triangles according to the number of votes cast, the thickness according to whether or not the result for Trump/Clinton was a landslide and rotating the map 90 degrees, the very obvious differences between metropolitan, densely populated coastal counties that voted emphatically for Clinton and the vast number of suburban, provincial town and rural counties (everywhere else) that voted Trump, are exposed.\n\n\n\n\nFigure 3.1: Map of 2016 US presidential election results. Note that for copyright reasons this is a re-implementation of the original Washington Post piece in ggplot2.\n\n\n\n\n\n\n\n\n\n3.2.2 Grammar of Graphics\n\nData graphics visually display measured quantities by means of the combined use of points, lines, a coordinate system, numbers, symbols, words, shading, and color.\nTufte (1983)\n\nIn evidence in the Washington Post graphic is a judicious mapping of data to visuals and a clear understanding of analysis context. This act of carefully considering how best to leverage visual systems given the available data and analysis priorities is key to designing effective data graphics. Leland Wilkinson’s Grammar of Graphics (1999) captures this process of turning data into visuals. Wilkinson (1999)’s thesis is that if graphics can be described in a consistent way according to their structure and composition, then the process of generating graphics of different types can be systematised. This has obvious benefits for building visualization toolkits. Different chart types and combinations can be specified systematically, thereby formalising data visualization design.\n\n\nWilkinson (1999)’s grammar separates the construction of a data graphic into a series of components. Below are the components of the Layered Grammar of Graphics on which ggplot2 is based (Wickham 2010), adapted from Wilkinson (1999)’s original work. The components in Figure 3.2 are together used to create ggplot2 specifications. Those to highlight at this stage are in emphasis: the data containing the variables of interest, the marks used to represent data and the visual channels through which variables are encoded.\n\n\n\n\nFigure 3.2: Components of Wickham (2010)’s Layered Grammar of Graphics.\n\n\n\n\n\nTo demonstrate this, let’s generate some scatterplots based on the 2019 General Election data. Two variables worth exploring for association here are: con_1719, the change in Conservative vote share by constituency between 2017-2019, and leave_hanretty, the size of the Leave vote in the 2016 EU referendum, estimated at Parliamentary Constituency level (see Hanretty 2017).\nIn Figure 3.3 are three plots, accompanied by ggplot2 specifications used to generated them. Reading-off the graphics and the associated code, you should get a feel for how ggplot2 specifications are constructed:\n\nStart with a data frame, in this case 2019 General Election results for UK Parliamentary Constituencies. The data are passed to the ggplot2 spec using the pipe operator (|>). Also at this stage, we consider the variables to encode and their measurement type – both con_1719 and leave_hanretty are ratio scale variables.\nNext is the encoding (mapping=aes()), which determines how the data are to be mapped to visual channels. A scatterplot is a 2D representation in which horizontal and vertical position varies in a meaningful way, in response to the values of a dataset. Here the values of leave_hanretty are mapped along the x-axis and the values of con_1719 are mapped along the y-axis.\nFinally, we represent individual data items with marks using the geom_point geometry.\n\nIn the middle plot, the grammar is updated such that the points are coloured according to winning_party, a variable of type categorical nominal. In the bottom plot constituencies that flipped from Labour-to-Conservative between 2017-19 are emphasised by varying the transparency (alpha) of points. flipped is labelled an ordinal variable, but strictly it is a nominal (binary) variable. In the plot, constituencies that flipped are given greater visual emphasis and so it is appropriate to call flipped an ordinal variable.\n\n\n\n\nFigure 3.3: Plots, grammars and underlying ggplot2 specifications for the scatterplot.\n\n\n\n\n\n\n\n3.2.3 Marks and visual channels\n\nYou might have noticed that in our descriptions we introduced marks as another term for geometry and visual encoding channels as another term for aesthetics. We also paid special attention to the data types that are being encoded. Marks are graphical elements such as bars, lines, points, ellipses that can be used to represent data items. In ggplot2, these are accessed through the functions prefaced with geom_*. Visual channels are attributes such as colour, size, position that, when mapped to data, control the appearance of marks in response to the values of a dataset.\nMarks and channels are terms used widely in Information Visualization, an academic discipline devoted to the study of data graphics, and most notably by Tamara Munzner (2014) in her textbook Visualization Analysis and Design. Munzner (2014)’s work synthesises over foundational research in Information Visualization and Cognitive Science testing how effective different visual channels are at supporting different tasks. Figure 3.4 is taken from Munzner (2014) and lists the main visual channels with which data might be encoded. The grouping and order of the figure is meaningful. Channels are grouped according to the tasks to which they are best suited and then ordered according to their effectiveness at supporting those tasks. To the left are magnitude:order channels – those that are best suited to tasks aimed at quantifying data items. To the right are identity:category channels – those that are most suited to supporting tasks that involve isolating, grouping and associating data items.\n\n\n\n\nFigure 3.4: Visual channels to which data items can be encoded, as they appear in Munzner (2014).\n\n\n\n\n\n\n\n3.2.4 Evaluating designs\nThe effectiveness rankings of visual channels in Figure Figure 3.4 are not simply based on Munzner’s preference. They are informed by detailed experimental work, Cleveland and McGill (1984) and later replicated by Heer and Bostock (2010), which involved conducting controlled experiments testing people’s ability to make judgements from graphical elements. We can use Figure 3.4 to help make decisions around which data item to encode with which visual channel. This is particularly useful when designing data-rich graphics, where several data items are to be encoded simultaneously (e.g. Beecham et al. 2021). Figure 3.4 also offers a low cost way of evaluating different designs against their encoding effectiveness.\nTo illustrate this, we can use Munzner’s ranking of channels to evaluate the Washington Post graphic discussed in Figure 3.1. Table 3.2 provides a summary of the encodings used in the graphic. US counties are represented using a peak-shaped mark. The key purpose of the graphic is to depict the geography of voting outcomes, and the most effective quantitative channel – position on an aligned scale – is used to order the county marks with a 2D geographic arrangement. With the positional channels taken, the two quantitive measures are encoded with the next highest ranked channel, 1D length: height varies according to number of total votes cast and width according to margin size. The marks are additionally encoded with two categorical variables: whether the county-level result was a landslide and also the winning party. Since the intention is to give greater visual saliency to counties that resulted in a landslide, this as an ordinal variable, encoded with a quantitative channel: 2D area. The winning party, a categorical nominal variable, is encoded using colour hue.\n\n\n\n\nEncoding effectiveness for Washington Post graphic that emphasises vote margin and size of counties using triangle marks.\n \n Data item \n    Type \n    Channel \n    Rank \n  \n\nMagnitude:Order\n\n County location \n    interval \n    position in x,y \n    1. quant \n  \n\n Total votes cast \n    ratio \n    length \n    3. quant \n  \n\n Margin size \n    ratio \n    length \n    3. quant \n  \n\n Is landslide \n    ordinal \n    area \n    5. quant \n  \nIdentity:Category\n\n Winning party \n    nominal \n    colour hue \n    2. cat \n  \n\n\n\n\nEach of the encoding choices follow conventional wisdom in that data items are encoded using visual channels appropriate to their measurement level. Glancing down the “rank” column, the graphic has high effectiveness. Whilst technically spatial region is the most effective channel for encoding nominal data, it is already in use as the marks are arranged by geographic position. Additionally, it makes sense to distinguish Republican and Democrat wins using the colours with which they are always represented. Given the fact that the positional channels represent geographic location, length to represent votes cast and vote margin, the only superior visual channel to 2D area that could be used to encode the landslide variable is orientation. There are very good reasons for not varying the orientation of the arrow marks. Most obvious is that this would undermine perception of length encodings used to represent the vote margin (width) and absolute vote size (height).\n\n\n\n\n\n\nVisualization design trade-offs\n\n\n\nData visualization design almost always involves trade-offs. When deciding on a design configuration, it is necessary to prioritise data and analysis tasks, then match representations and encodings that are most effective to the tasks that have the greatest priority. This constrains the encoding options for less important data items and tasks. Good visualization design is sensitive to this interplay between tasks, data and encoding.\n\n\n\n3.2.5 Symbolisation\n\nSymbolization is the process of encoding something with meaning in order to represent something else. Effective symbol design requires that the relationship between a symbol and the information that symbol represents (the referent) be clear and easily interpreted.\nWhite (2017)\n\nImplicit in the discussion above, and when making design decisions, is the importance of symbolisation. From the original Washington Post article, the overall pattern that can be discerned is of population-dense coastal and metropolitan counties voting Democrat – densely-packed, tall, wide and blue  marks – contrasted against population-sparse rural and small town areas voting Republican – short, wide and red  marks. The graphic evokes a distinctive landscape of voting behaviour, emphasised by its caption: “The peaks and valleys of Trump and Clinton’s support”.\nSymbolisation is used equally well in a variant of the graphic emphasising two-party Swing between the 2012 and 2016 elections (Figure 3.5). Each county is represented as a | mark. The Swing variable is then encoded by continuously varying mark angles: counties swinging Republican are angled to the right /; counties swinging Democrat are angled to the left \\. Although angle is a less effective channel at encoding quantities than is length, there are obvious links to the political phenomena in the symbolisation – angled right for counties that moved to the right politically. There are further useful properties in this example. Since county voting is spatially auotocorrelated, we quickly assemble from the graphic dominant patterns of Swing to the Republicans (Great Lakes, rural East Coast), predictable Republican stasis (the mid west) and to detect more isolated, locally exceptional Swings to the Democrats (rapidly urbanising counties in the deep south).\n\n\n\n\n\nFigure 3.5: Map of swing in 2016 US presidential election results. Note that for copyright reasons this is a re-implementation of the original Washington Post piece in ggplot2.\n\n\n\n\n\n\n\n\n\n\n\nTask\n\n\n\nComplete the description table below identifying each data item that is encoded in Figure 3.5 along with its measurement level, visual mark and visual channel and the effectiveness rank, according to Munzner (2014), of this encoding.\n\n\n\n\n\n\n\n\n\nData item\nMeasurement level\nVisual mark\nVisual channel\nRank\n\n\n\nCounty location\n\n\n\n\n\n\n...\n...\n...\n...\n...\n\n\n...\n...\n...\n...\n...\n\n\n...\n...\n...\n...\n...\n\n\n...\n...\n...\n...\n...\n\n\n...\n...\n...\n...\n...\n\n\n\n\n\n\n\n\n3.2.6 Colour\n\nThe ultimate rule when using colour in data graphics is to use properties of colour that match the properties of the data. Categorical nominal data – data that cannot be easily ordered – should be encoded using discrete colours with no obvious order; so colour hue. Categorical ordinal data – data whose categories can be ordered – should be encoded with colours that contain an intrinsic order; saturation or brightness, usually allocated into gradients. Quantitative data – data that can be ordered and contain values on a continuous scale – should also be encoded with saturation or brightness, expressed on a continuous scale. As we will discover shortly, these principles are applied by default in ggplot2, along with access to perceptually uniform schemes.\n\n\n\n\n\n\nOn colour\n\n\n\nThere are very many considerations when using colour to support visual data analysis and communication. Lisa Charotte-Rost’s Guide to Colours in Data Visualization is an excellent outline of the decision-space."
  },
  {
    "objectID": "03-visual.html#sec-techniques",
    "href": "03-visual.html#sec-techniques",
    "title": "3  Visualization Fundamentals",
    "section": "\n3.3 Techniques",
    "text": "3.3 Techniques\nThe technical component of this chapter analyses data from the 2019 UK General Election, reported at Parliamentary Constituency level. After importing and describing the dataset, we will generate data graphics that expose patterns in voting behaviour by writing ggplot2 specifications.\n\nDownload the 03-template.qmd file for this chapter and save it to the reports folder of your vis4sds project.\nOpen your vis4sds project in RStudio and load the template file by clicking File > Open File ... > reports/03-template.qmd.\n\n\n3.3.1 Import\nThe template file lists the required packages – tidyverse, sf and also parlitools. Installing parlitools brings down the 2019 UK General Election dataset, along with other constituency-level datasets. Loading it with library(parlitools) makes these data available to your R session.\nThe data frame containing 2019 UK General Election data is called bes_2019. This stores results data released by House of Commons Library. We can get a quick overview with a call to glimpse(<dataset-name>). bes_2019 contains 650 rows, one for each parliamentary constituency, and 118 columns. Contained in the columns are variables reporting vote numbers and shares for the main political parties for 2019 and 2017 General Elections, as well as names and codes (IDs) for each constituency and the county, region and country in which they are contained.\nThe technical component replicates some of the visual data analysis in Beecham (2020). For this we need to calculate an additional variable, Butler Swing (Butler and Van Beek 1990); the average change in share of the vote won by two parties contesting successive elections. Code for calculating this variable (named swing_con_lab) is in the 03-template.qmd. The only other dataset to load is a .geojson file containing simplified geometries of constituencies, collected originally from ONS Open Geography Portal. This is a special class of data frame containing a Simple Features geometry column.\n\n\n3.3.2 Summarise\nYou may be familiar with the result of the 2019 General Election: a landslide Conservative victory that confounded expectations. To start, we can quickly compute some summary statistics around the vote. In the code block below, we count the number of seats won and overall vote share by party. For the latter, the code is a little more elaborate than we might wish at this stage. We needed to reshape the data frame using pivot_wider() such that each row represents a vote for a party in a constituency. From here the vote share for each party can be easily computed.\nWhilst the Conservative party hold 56% of constituencies, they won only 44% of the vote share. The equivalent figures for Labour are 31% and 32% respectively. Incidentally, whilst the Conservatives increased their share of constituencies from 2017 (where they had just 317, 49% of constituencies) their vote share increase was reasonably small – in 2017 they gained 43% of the vote. This fact is interesting as it may suggest some shift in where, which constituencies, the Conservative party gained majorities.\n\n# Number of constituencies won by party.\nbes_2019 |>\n  group_by(winner_19) |>\n  summarise(count=n()) |>\n  arrange(desc(count))\n## # A tibble: 11 x 2\n##    winner_19                        count\n##    <chr>                            <int>\n##  1 Conservative                       365\n##  2 Labour                             202\n##  3 Scottish National Party             48\n##  4 Liberal Democrat                    11\n##  5 Democratic Unionist Party            8\n##  6 Sinn Fein                            7\n##  7 Plaid Cymru                          4\n##  8 Social Democratic & Labour Party     2\n##  9 Alliance                             1\n## 10 Green                                1\n## 11 Speaker                              1\n\n# Share of vote by party.\nbes_2019 |>\n  # Select cols containing vote counts by party.\n  select(constituency_name, total_vote_19, con_vote_19:alliance_vote_19, region) |>\n  # Pivot to make each row a vote for a party in a constituency.\n  pivot_longer(cols=con_vote_19:alliance_vote_19, names_to=\"party\", values_to=\"votes\") |>\n  # Use some regex to pull out party name.\n  mutate(party=str_extract(party, \"[^_]+\")) |>\n  # Summerise over parties.\n  group_by(party) |>\n  # Calculate vore share for each party.\n  summarise(vote_share=sum(votes, na.rm=TRUE)/sum(total_vote_19)) |>\n  # Arrange parties descending on vote share.\n  arrange(desc(vote_share))\n\n## # A tibble: 12 x 2\n##    party    vote_share\n##    <chr>         <dbl>\n##  1 con         0.436\n##  2 lab         0.321\n##  3 ld          0.115\n##  4 snp         0.0388\n##  5 green       0.0270\n##  6 brexit      0.0201\n##  7 dup         0.00763\n##  8 sf          0.00568\n##  9 pc          0.00479\n## 10 alliance    0.00419\n## 11 sdlp        0.00371\n## 12 uup         0.00291\n\nBelow are some summary statistics computed over the newly created swing_con_lab variable. As the Conservative and Labour votes are negligible in Northern Ireland, it makes sense to focus on Great Britain for our analysis of Conservative-Labour Swing and so the first step in the code is to create a new data frame filtering out Northern Ireland.\n\ndata_gb <- bes_2019 |>\n  filter(region != \"Northern Ireland\") |>\n  mutate(\n    swing_con_lab=0.5*((con_19-con_17)-(lab_19-lab_17)),\n    # Recode to 0 Chorley (incoming speaker),\n    # Buckingham (outgoing speaker) as uncontested seat.\n    swing_con_lab=if_else(constituency_name %in% c(\"Chorley\", \"Buckingham\"),0,swing_con_lab)\n  )\n\ndata_gb |>\n  summarise(\n    min_swing=min(swing_con_lab),\n    max_swing=max(swing_con_lab),\n    median_swing=median(swing_con_lab),\n    num_swing=sum(swing_con_lab>0),\n    num_landslide_con=sum(con_19>50, na.rm=TRUE),\n    num_landslide_lab=sum(lab_19>50, na.rm=TRUE)\n    )\n\n## # A tibble: 1 x 6\n##   min_swing max_swing median_swing num_swing num_landslide_con num_landslide_lab\n##       <dbl>     <dbl>        <dbl>     <int>             <int>             <int>\n## 1     -6.47      18.4         4.44       599               280               120\n\n\n3.3.3 Plot distributions\n\n\n\n\nFigure 3.6: Histograms of Butler two-party Labour-Conservative Swing.\n\n\n\n\nLet’s start with ggplot2 specifications by plotting some of these variables. Below is the code for plotting a histogram of the Swing variable.\n\ndata_gb |>\n  ggplot(mapping=aes(swing_con_lab)) +\n  geom_histogram()\n\nA reminder of the general form of the ggplot2 specification:\n\nStart with some data: data_gb.\nDefine the encoding: mapping=aes(), into which we pass the swing_con_lab variable.\nSpecify the marks to be used: geom_histogram() in this case.\n\nDifferent from the scatterplot example, there is more happening in the internals of ggplot2 when creating a histogram. The Swing variable is partitioned into bins and observations in each bin are counted. The x-axis (bins) and y-axis (counts by bin) is therefore derived from the supplied swing_con_lab variable.\nBy default the histogram’s bars are given a grey colour. To set them to a different colour, add a fill= argument to geom_histogram(). In the code block below, colour is set using hex codes – \"#003c8f\", based on the theme for this book. The term set and not map or encode is used for principled reasons. Any part of a ggplot2 specification that involves encoding data – mapping a data item to a visual channel – should be specified through the mapping=aes() argument. Anything else, for example changing the default colour, thickness and transparency of marks, needs to be set outside of this argument.\n\ndata_gb |>\n  ggplot(mapping=aes(swing_con_lab)) +\n  geom_histogram(fill=\"#003c8f\") +\n  labs(\n    title=\"Butler two-party Labour-Conservative Swing\",\n    subtitle=\"-- GB Constituencies 2019 versus 2017 election\",\n    caption=\"Data published by House of Commons Library, accessed via `parlitools`\",\n    x=\"Swing\", y=\"count\"\n  )\n\nYou will notice that different elements of a ggplot2 specification are added (+) as layers. In the example above, the additional layer of labels (labs()) is not intrinsic to the graphic. However, often you will add layers that do affect the graphic itself. For example, the scaling of encoded values (e.g. scale_*_continuous()) or whether the graphic is to be conditioned on another variable to generate small multiples for comparison (e.g. facet_*()). Adding a call to facet_*(), we can compare how Swing varies by region (Figure 3.7). The plot is annotated with the median value for Swing (4.4) by adding a vertical line layer (geom_vline()) set with an x-intercept at this median value. From this, there is some evidence of a regional geography to the 2019 vote: London and Scotland are particularly distinctive in containing relatively few constituencies swinging greater than the expected midpoint; North East, Yorkshire & The Humber, and to a lesser extent West and East Midlands, appear to show the largest relative number of constituencies swinging greater than the midpoint.\n\n\n\n\n\n\nFigure 3.7: Histograms of Swing variable, grouped by region.\n\n\n\n\n\n\n\n\n\n\n\nTask\n\n\n\nUpdate the earlier ggplot2 specification to produce a set of histograms of the Swing variable faceted by region, similar to that in Figure 3.7.\n\n\n\n3.3.4 Plot ranks/magnitudes\n\n\n\n\nFigure 3.8: Plots of vote shares by party.\n\n\n\n\n\nPreviously we calculated overall vote share by Political Party. We could continue the exploration of votes by region, re-using this code to generate plots displaying quantities but also comparing by region, using marks and encoding channels that are suitable for magnitudes.\nTo generate a bar chart similar to the left of Figure 4.3 the ggplot2 specification would be:\n\ndata_gb |>\n  # The code block summarising vote by party.\n  <some dplyr code> |>\n  # To be piped to ggplot2.\n  <...>  |>\n  # Ordinal x-axis (party, reordered), Ratio y-axis (vote_share).\n  ggplot(aes(x=reorder(party, -vote_share), y=vote_share)) +\n  # Set bar colour to the book theme.\n  geom_col(fill=\"#003c8f\") +\n  coord_flip()\n\nA quick breakdown of the specification:\n\n\nData: This is the summarised data frame in which each row is a political party and the column describes the vote share recorded for that party.\n\nEncoding: We have dropped the call to mapping=. ggplot2 always looks for aes() and so can save some code clutter. In this case we are mapping party to the x-axis, a categorical variable made ordinal by the fact that we reorder the axis left-to-right descending according to vote_share. vote_share is mapped to the y-axis – so encoded using bar length on an aligned scale, an effective channel for conveying magnitudes.\n\nMarks: geom_col() for generating the bars.\n\nSetting: Again, we’ve set bar colour according to the book theme and included titles and captions. Optionally we add a coord_flip() layer in order to display the bars horizontally. This makes the category axis labels easier to read and also seems more appropriate for the visual “ranking” of bars.\n\nFaceting by region\n\n\n\n\nFigure 3.9: Plots of vote shares by party and region.\n\n\n\n\n\nIn Figure 3.9 the graphic is faceted by region. This requires an updated derived dataset grouping by vote_share and region and of course adding a faceting layer (geom_facet(~region)) to the ggplot2 specification. The graphic is more data-rich, but additional cognitive effort is required in relating the bars representing political parties between different graphical subsets. We can assist this associative task by encoding the bars with an appropriate visual channel: colour hue. The ggplot2 specification for this is as you would expect; we add a mapping to geom_col() and pass the variable name party to the fill argument (aes(fill=party)).\n\ndata_gb |>\n  # The code block summarising vote by party and region.\n  <some dplyr code> |>\n  # To be piped to ggplot2.\n  <...>  |>\n  ggplot(aes(x=reorder(party, vote_share), y=vote_share)) +\n  geom_col(aes(fill=party)) +\n  coord_flip() +\n  facet_wrap(~region)\n\nTrying this for yourself, you will observe that the ggplot2 internals do some thinking for us. Since party is a categorical variable, a categorical hue-based colour scheme is automatically applied. Try passing a quantitative variable (fill=vote_share) and see what happens; a quantitative colour gradient scheme is applied.\nClever as this is, when encoding political parties with colour, symbolisation is important. It makes sense to represent political parties using colours with which they are most commonly associated. We can override ggplot2’s default colour by adding a scale_fill_manual() layer into which a vector of hex codes describing the colour of each political party is passed (party_colours). We also need to tell ggplot2 which element of party_colours to apply to which value of the party variable. In the code below, a derived table is generated summarising vote_share by political party and region. In the final line the party variable is recoded as a factor. You might recall from the last chapter that factors are categorical variables of fixed and orderable values – levels. The call to mutate() recodes party as a factor variable and orders the levels according to overall vote share.\n\n# Generate derived data.\ntemp_party_shares_region <- data_gb |>\n  select(constituency_name, region, total_vote_19, con_vote_19:alliance_vote_19) |>\n  pivot_longer(cols=con_vote_19:alliance_vote_19, names_to=\"party\", values_to=\"votes\") |>\n  mutate(party=str_extract(party, \"[^_]+\")) |>\n  group_by(party, region) |>\n  summarise(vote_share=sum(votes, na.rm=TRUE)/sum(total_vote_19)) |>\n  filter(party %in% c(\"con\", \"lab\", \"ld\", \"snp\", \"green\", \"brexit\", \"pc\")) |>\n  mutate(party=factor(party, levels=c(\"con\", \"lab\", \"ld\", \"snp\", \"green\", \"brexit\" \"pc\")))\n\nNext, a vector of objects is created containing the hex codes for the colours of political parties (party_colours). This is a named vector, with names assigned from the levels of the party variable that was just created.\n\n# Define colours.\ncon <- \"#0575c9\"\nlab <- \"#ed1e0e\"\nld <- \"#fe8300\"\nsnp <- \"#ebc31c\"\ngreen <- \"#78c31e\"\npc <- \"#4e9f2f\"\nbrexit <- \"#25b6ce\"\nother <- \"#bdbdbd\"\n\nparty_colours <- c(con, lab, ld, snp, green, brexit, pc)\nnames(party_colours) <- levels(temp_party_shares_region |> pull(party))\n\nThe ggplot2 specification is then updated with the scale_fill_manual() layer:\n\ntemp_party_shares_region |>\n  ggplot(aes(x=reorder(party, vote_share), y=vote_share)) +\n  geom_col(aes(fill=party)) +\n  scale_fill_manual(values=party_colours) +\n  coord_flip() +\n  facet_wrap(~region)\n\n\n\n\n\n\n\nGrammar of Graphics-backed visualization toolkits\n\n\n\nThe idea behind visualization toolkits such as ggplot2 is to insert visual data analysis approaches into the Data Scientist’s workflow. Rather than being overly concerned with low-level aspects of drawing, mapping to screen coordinates and scaling factors, the analyst instead focuses on aspects relevant to the analysis – the variables in a dataset and how they will be variously encoded and conditioned using visuals to expose structure and effect comparison. Hadley Wickham talks about the type of workflow used throughout this book – bits of dplyr to prepare data for charting before being piped (|>) to a ggplot2 specification – as equivalent to a grammar of interactive graphics.\nThe process of searching for, defining and inserting manual colour schemes for creating Figure 3.9 might seem inimical to this. Indeed we were reluctant to include this code so early – there is some reasonably advanced dplyr and a little regular expression in the data preparation code that you should not be overly concerned with. Having control of these slightly more low-level properties is sometimes necessary even for exploratory analysis, in this case for enabling appropriate and sensible symbolisation.\n\n\n\n3.3.5 Plot relationships\n\n\n\n\nFigure 3.10: Plots of 2019 versus 2017 vote shares.\n\n\n\n\n\n\nTo continue the investigation of change in vote shares for the major parties between 2017 and 2019, Figure 3.10 contains a scatterplot of Conservative vote share in 2019 (y-axis) against vote share in 2017 (x-axis). The graphic is annotated with a diagonal line. If constituencies voted in 2019 in exactly the same way as 2017, the points would all converge on the diagonal, points above the diagonal indicate a larger Conservative vote share than 2017, those below the diagonal represent a smaller Conservative vote share than 2017. Points are coloured according to the winning party in 2019 and constituencies that flipped from Labour to Conservative are emphasised using transparency and shape.\nThe code for generating most of the scatterplot comparing Conservative vote shares is below.\n\ndata_gb |>\n  mutate(winner_19=case_when(\n           winner_19 == \"Conservative\" ~ \"Conservative\",\n           winner_19 == \"Labour\" ~ \"Labour\",\n           TRUE ~ \"Other\"\n         )) |>\n  ggplot(aes(x=con_17, y=con_19)) +\n  geom_point(aes(colour=winner_19), alpha=.8) +\n  geom_abline(intercept = 0, slope = 1) +\n  scale_colour_manual(values=c(con,lab,other)) +\n  ...\n\nHopefully there is little surprising here:\n\n\nData: The data_gb data frame. Values of winner_19 that are not Conservative or Labour are recoded to Other using a conditional statement. This is because points are eventually coloured according to winning party, but the occlusion of points adds visual complexity and so the two main parties are retained and remaining parties recoded to Other.\n\nEncoding: Conservative vote share in 2017 and 2019 are mapped to the x- and y- axes respectively and winner_19 to colour. scale_colour_manual() is used for customising the colours.\n\nMarks: geom_point() for generating the points of the scatterplot; geom_abline() for drawing the reference diagonal.\n\n\n\n\n\n\n\nTask\n\n\n\nThe code block above doesn’t exactly reproduce the graphic in Figure 3.10. Try updating the ggplot2 specification to emphasise constituencies that flipped allegiance from Labour to Conservative. Hint: you may wish to generate a variable recording constituencies that flipped between 2017 and 2019 and encode some visual channel in the graphic on this.\n\n\n\n\n\n\n\n\nOn conditionals\n\n\n\ncase_when allows you to avoid writing multiple if_else() statements. It wasn’t strictly necessary here, as a single if_else() would have sufficed:\ndata_gb |>\n  mutate(\n    winner_19=\n    if_else(!winner_19 %in% c(\"Conservative\", \"Labour\"), \"Other\", winner_19)\n  )\nA general point from the code blocks in this chapter is of the importance of proficiency in dplyr. Throughout the book you will find yourself needing to calculate new variables, recode variables, and reorganise data frames before passing through to ggplot2.\n\n\n\n3.3.6 Plot geography\n\n\n\n\nFigure 3.11: Choropleth of elected parties in 2019 General Election.\n\n\n\n\n\nImplied in the data graphics above is a geography to voting; certainly to observed changes in voting comparing the 2017 and 2019 elections (e.g. Figure 3.7). We end the chapter by generating thematic maps of the results data.\nTo do this we need to define a join on the boundary data (cons_outline):\n\n# Join constituency boundaries.\ndata_gb <- cons_outline |>\n  inner_join(data_gb, by=c(\"pcon19cd\"=\"ons_const_id\"))\n# Check class.\n## [1] \"sf\"         \"data.frame\"\n\nThe code for generating the Choropleth maps of winning party by constituency in Figure 3.11:\n\n# Recode winner_19 as a factor variable for assigning colours.\ndata_gb <- data_gb |>\n  mutate(\n    winner_19=if_else(winner_19==\"Speaker\", \"Other\", winner_19),\n    winner_19=as_factor(winner_19))\n\n# Create a named vector of colours\nparty_colours <- c(con, lab, ld, green, other, snp, pc)\nnames(party_colours) <- levels(data_gb |> pull(winner_19))\n\n# Plot map.\ndata_gb |>\n  ggplot(aes(fill=winner_19)) +\n  geom_sf(colour=\"#eeeeee\", linewidth=0.01)+\n  # Optionally add a layer for regional boundaries.\n  # geom_sf(\n  #   data=. |> group_by(region) |> summarise(),\n  #    colour=\"#eeeeee\", fill=\"transparent\", linewidth=0.08\n  # )+\n  coord_sf(crs=27700, datum=NA) +\n  scale_fill_manual(values=party_colours)\n\nA breakdown of the ggplot2 spec:\n\n\nData: The dplyr code updates data_gb by recoding winner_19 as a factor and defining a named vector of colours to supply to scale_fill_manual().\n\nEncoding: No surprises here – fill according to winner_19.\n\nMarks: geom_sf() is a special class of geometry. It draws objects depending on the contents of the geometry column. In this case MULTIPOLYGON, so read this as a polygon geometric primitive.\n\nCoordinates: coord_sf – we set the coordinate system (CRS) explicitly. In this case OS British National Grid.\n\nSetting: Constituency boundaries are subtly introduced by setting the geom_sf() mark to light grey (colour=\"#eeeeee\") with a thin (size=0.01) outline. On the map to the right outlines for regions are added as another geom_sf layer.\n\nIn the third map of Figure 3.11 the transparency (alpha) of filled constituencies is varied according to the Swing variable. This does demonstrate that constituencies swinging most dramatically for Conservative (darker colours) are in the midlands and North of England and not in London and the South East. The pattern is nevertheless a subtle one; transparency (colour luminance / saturation) is not a highly effective visual channel. It may be worth applying the same encoding to Butler two-party swing as that used in the Washington Post graphic when characterising Republican-Democrat swing in 2016 US Elections (e.g. Beecham (2020)).\nThis can be achieved by simply adding another ggplot2 layer, though the code is a little more involved. ggplot2’s geom_spoke() primitive draws line segments parameterised by a location (x,y position) and angle. With this we can encode constituencies with | marks that angle to the right / where the constituency swings towards Conservative and to the left where it swings towards Labour \\. This encoding better exposes the pattern of constituencies forming Labour’s “red wall” in the north of England, as well as parts of Wales and the Midlands, flipping to Conservative.\n\n\n\n\nFigure 3.12: Map of Butler Con-Lab Swing in 2019 General Election.\n\n\n\n\n\nAnd the ggplot2 specification:\n\n# Find the maximum Swing values, to pin the min and max angles to.\nmax_shift <- max(abs(data_gb |> pull(swing_con_lab)))\nmin_shift <- -max_shift\n\n# Re-define `party_colours`  to contain just three values: hex codes for\n# Conservative, Labour and Other.\nparty_colours <- c(con, lab, other)\nnames(party_colours) <- c(\"Conservative\", \"Labour\", \"Other\")\n\n# Plot Swing map.\ndata_gb |>\n  mutate(\n    is_flipped=seat_change_1719 %in% c(\"Conservative gain from Labour\",\"Labour gain from Conservative\"),\n    elected=if_else(!winner_19 %in% c(\"Conservative\", \"Labour\"), \"Other\", as.character(winner_19))\n    ) |>\n  ggplot()+\n  geom_sf(aes(fill=elected), colour=\"#636363\", alpha=.2, linewidth=.01)+\n  geom_spoke(\n    aes(x=bng_e, y=bng_n, angle=get_radians(map_scale(swing_con_lab,min_shift,max_shift,135,45)),\n    colour=elected, size=is_flipped),\n    radius=7000, position=\"center_spoke\"\n    )+\n  coord_sf(crs=27700, datum=NA)+\n  scale_size_ordinal(range=c(.3,.9))+\n  scale_colour_manual(values=party_colours)+\n  scale_fill_manual(values=party_colours)\n\nA breakdown:\n\n\nData: data_gb is updated with a boolean identifying whether or not the Constituency flipped Con-Lab/Lab-Con between successive elections (is_flipped), and a variable simplifying the party elected to either Conservative, Labour or Other.\n\nEncoding: geom_sf() is again filled by elected party. This encoding is made more subtle by adding transparency (alpha=.2). geom_spoke() is mapped to the geographic centroid of each Constituency (bng_e - easting, bng_n - northing), coloured according to elected party, sized according to whether the Constituency flipped its vote and tilted or angled according to the Swing variable. A convenience function (map_scale()) pins the maximum Swing values in either direction to 45 degrees (max Swing to the right, Conservative) and 135 degrees (max Swing to the left, Labour).\n\nMarks: geom_sf() for the Constituency boundaries, geom_spoke() for the angled line primitives.\n\nScale: geom_spoke() primitives are sized to emphasise whether constituencies have flipped. The size encoding is censored to two values with scale_size_ordinal(). Passed to scale_colour_manual() and scale_fill_manual() is the vector of party_colours.\n\nCoordinates: coord_sf – the CRS is OS British National Grid.\n\nSetting: The radius of geom_spoke() lines is a sensible default arrived at through trial and error, its position set using a newly created centre_spoke class.\n\nNote that there are helper functions that must be run to execute the ggplot2 spec above correctly. In order to position lines using geom_spoke() centred on their x,y location, we need to create a position subclass, as below. Once created, this can be easily used: geom_spoke(position = centre_spoke)’\n\n# Convert degrees to radians.\nget_radians <- function(degrees) {\n  (degrees * pi) / (180)\n}\n# Rescaling function.\nmap_scale <- function(value, min1, max1, min2, max2) {\n  return  (min2+(max2-min2)*((value-min1)/(max1-min1)))\n}\n# Position subclass for centred geom_spoke.\n# As per https://bit.ly/3yfXdKJ.\nposition_centre_spoke <- function() PositionCentreSpoke\nPositionCenterSpoke <-\n    ggplot2::ggproto('PositionCentreSpoke', ggplot2::Position,\n        compute_panel = function(self, data, params, scales)\n            {\n                data$x <- 2*data$x - data$xend\n                data$y <- 2*data$y - data$yend\n                data$radius <- 2*data$radius\n                data\n            }\n    )"
  },
  {
    "objectID": "03-visual.html#conclusions",
    "href": "03-visual.html#conclusions",
    "title": "3  Visualization Fundamentals",
    "section": "\n3.4 Conclusions",
    "text": "3.4 Conclusions\nVisualization design is ultimately a process of decision-making. Data must be filtered and prioritised before being encoded with marks, visual channels and symbolisation. The most successful data graphics are those that expose structure, connections and comparisons that could not be achieved easily via other, non-visual means. This chapter has introduced concepts – a vocabulary, framework and empirically-informed guidelines – that help support this decision-making process and that underpin modern visualization toolkits (ggplot2 included). Through an analysis of UK 2019 General Election data, we have demonstrated how these concepts can be applied in a real data analysis."
  },
  {
    "objectID": "04-explore.html",
    "href": "04-explore.html",
    "title": "4  Exploratory Data Analysis",
    "section": "",
    "text": "By the end of this chapter you should gain the following knowledge and practical skills."
  },
  {
    "objectID": "04-explore.html#introduction",
    "href": "04-explore.html#introduction",
    "title": "4  Exploratory Data Analysis",
    "section": "\n4.1 Introduction",
    "text": "4.1 Introduction\nExploratory Data Analysis (EDA) is an approach to analysis that aims to expose the properties and structure of a dataset, and from here suggest directions for analytic inquiry. In an EDA, relationships are quickly inferred, anomalies labelled, assumptions tested and new hypotheses and ideas formulated. EDA relies heavily on visual approaches to analysis; it is common to generate many dozens of (often throwaway) data graphics when exploring a dataset for the first time.\nThis chapter demonstrates how the concepts and principles introduced previously, of data types and their visual encoding, can be applied to support EDA. It does so by analysing STATS19, a dataset containing detailed information on every reported road traffic crash in Great Britain that resulted in personal injury. STATS19 is highly detailed, with many categorical variables. The chapter starts by revisiting commonly used chart types for summarising within-variable variation and between-variable co-variation in a dataset. It then focuses more directly on the STATS19 case, and how detailed comparison across many categorical variables can be effected using colour, layout and statistical computation."
  },
  {
    "objectID": "04-explore.html#concepts",
    "href": "04-explore.html#concepts",
    "title": "4  Exploratory Data Analysis",
    "section": "\n4.2 Concepts",
    "text": "4.2 Concepts\n\n4.2.1 Exploratory data analysis and statistical graphics\n\nIn Exploratory Data Analysis (EDA), graphical and statistical summaries are used to build knowledge and understanding of a dataset. The goal of EDA is to infer relationships, identify anomalies and test new ideas and hypotheses; it is a knowlegde-building activity. Rather than a formal set of techniques, EDA should be considered an outlook to analysis. It aims to reveal properties, patterns and relationships in a dataset, and from there expectations, codified via models, to be further investigated in context via more targeted data graphics and statistics.\nThe early stages of an EDA may be very data-driven. Datasets are described abstractly according to their measurement level and corresponding data graphics and summary statistics suggested from this (e.g. Table 4.1). As knowledge and understanding of the dataset increases, researchers might apply more targeted theory and prior knowledge when developing and evaluating models.\nVisual approaches play an important role in both these stages of analysis. For example, when first examining variables according to their shape and location (e.g. Table 4.1), data graphics help identify patterns that statistical summaries miss; such as whether variables are multi-modal, the extent and direction of outliers. When more specialised models are proposed, graphical summaries can add important detail around where, and by how much, the observed data depart from the model. It is for this reason that data visualization is seen as intrinsic to EDA (Tukey 1977).\n\n\n\n\n\n\nTable 4.1:  Breakdown of variable types. \n \n Measurement type \n    Statistic \n    Chart type \n  \n\nWithin-variable variation\n\n Nominal \n    mode | entropy \n    bar charts, dot plots ... \n  \n\n Ordinal \n    median | percentile \n    bar charts, dot plots ... \n  \n\n Continuous \n    mean | variance \n    histograms, box plots, density plots ... \n  \nBetween-variable variation\n\n Nominal \n    contingency tables \n    mosaic/spine plots ... \n  \n\n Ordinal \n    rank correlation \n    slope/bump charts ... \n  \n\n Continuous \n    correlation \n    scatterplots, parallel coordinate plots ... \n  \n\n\n\n\n\n\n4.2.2 Plots for continuous variables\nWithin-variable variation\nFigure 4.1 presents statistical graphics that are commonly used to show continuous variables measured on a ratio and interval scale – in this instance the age of casualty for a random sample of recorded road crashes (casualty_age).\n\n\n\n\nFigure 4.1: Univariate plots of dispersion in casualty ages from a sample of pedestrian-vehicle road crashes.\n\n\n\n\n\nIn the left-most row is a strip-plot. Every observation is displayed as a dot and mapped to y-position, with transparency and a random vertical perturbation applied to resolve occlusion due to overlapping observations. Although strip-plots scale poorly, the advantage is that all observations are displayed without the need to impose an aggregation. It is possible to visually identify the location of the distribution – denser dots towards 20-25 age range – but also that there is a degree of spread across the age values.\nHistograms partition observations into equal-range bins and observations in each bin are counted. These counts are encoded on an aligned scale using bar height. Increasing the size of the bins increases the resolution of the graphical summary. If reasonable decisions are made around choice of bin, histograms give distributions a shape that is expressive. It is easy to identify the location of a distribution and, in using length on aligned scale to encode frequency, estimate relative densities between different parts of the distribution. Different from the strip-plot, the histogram allows us to intuit that despite the heavy spread, the distribution of casualty_age is right-skewed, and we’d expect this given the location of the mean (36 years) relative to the median (33 years).\nA problem with histograms is the potential for discontinuities and artificial edge-effects around the bins. Density plots overcome this and can be thought of as smoothed histograms. They show the probability density function of a variable – the relative amount of probability attached to each value of casualty_age. From glancing at the density plots an overall shape to the distribution can be immediately derived. It is also possible to infer statistical properties: the mode of the distribution, the peak density; the mean and median, by sort of visual averaging and approximating the midpoint of the area under the curve.\n\nFinally, boxplots (McGill and Larsen 1978) encode the statistical properties inferred from strip-plots, histograms and density plots directly. The box is the interquartile range (IQR) of the casualty_age variable, the vertical line splitting the box is the median, and the whiskers are placed at observations \\leq 1.5*IQR. Whilst we lose important information around the shape of a distribution, box-plots are space-efficient and useful for comparing many distributions at once.\n\n\n\n\n\n\nNote\n\n\n\nSince the average age of road casualties is quite low, it may be instructive to explore the distribution of casualty_age conditioning on another variable and differentiating between variable values using colour. Figure 4.2 displays boxplots and density plots of the location and spread in casualty_age by vehicle and casualty_class for all crashes involving pedestrians. A noteworthy pattern is that riders of bicycles and motorcycles tend to be younger than the pedestrians they are contacting with, whereas for buses, taxis, HGVs and cars the reverse is true. Pedestrians involved in crashes with cars are especially skewed towards the younger ages. Although less space efficient, the density plots display richer information around the nature of the driver-pedestrian age distributions.\n\n\n\n\nFigure 4.2: Boxplots of casualty age by vehicle type and class.\n\n\n\n\n\n\n\nBetween-variable variation\nThe previous chapter included several scatterplots for displaying association between two quantitative variables. Scatterplots are used to check whether the association between variables is linear, but also to make inferences about the direction and intensity of linear correlation between variables – the extent to which values in one variable depend on the values of another – and also around the nature of variation between variables – the extent to which variation in one variable depends on another (heteroscedasticity). Although other chart types for displaying bivariate data exist, empirical studies in Information Visualization have demonstrated that aggregate correlation statistics can be reliably estimated from scatterplots (Rensink and Baldridge 2010; Harrison et al. 2014; Kay and Heer 2016; Correll and Heer 2017).\nThere are few variables in the STATS19 dataset measured on a continuous scale. In an EDA it is common to quickly compare associations between many quantitive variables in a dataset using scatterplot matrices or alternatively parallel coordinate plots. We will use both in chapter 6 when building models that attempt to formally structure and explain between-variable covariation.\n\n\n\n\n4.2.3 Plots for categorical variables\nWithin-variable variation\nFor categorical variables, variation is judged by how relative frequencies distribute across the variable’s categories. Bar charts are most commonly used. Length is an efficient visual channel for encoding quantities. Often it is useful to flip bar charts on their side so that category labels are arranged horizontally for ease of reading and, unless there is a natural ordering, arrange the bars in descending order based on their frequency, as in Figure 4.3.\n\n\n\n\nFigure 4.3: Bars displaying crash frequencies by vehicle type.\n\n\n\n\n\nBar charts are effective at conveying frequencies where the number of categories is reasonably small. For summarising frequencies across many categories, alternative chart types that minimise non-data-ink, such as Cleveland dot plots may be appropriate. The left plot in Figure 4.4 displays pedestrian crash counts for boroughs in London, ordered by crash frequency and grouped by whether boroughs are in inner- or outer- London. To the right, dots for crashes recorded between time periods – weekends and weekdays – are added. The addition of lines to connect dots is a useful stratgey for emphasising the differences between time periods. Although a consistent pattern is of greater crash counts during weekdays, the gap is less obvious four outer London boroughs; there may be in relative terms many more pedestrian crashes occurring in central London boroughs during weekdays. The third graphic is a heatmap with the same ordering and grouping of boroughs, but with columns coloured according to crash frequencies by vehicle type, further grouped by weekday and weekend times. Remembering Munzner (2014)’s ordering of visual channels, we trade-off some precision in estimation when encoding frequencies in heatmaps. A greater difficulty, irrespective of encoding channel, comes from the dominance of cars and weekdays; variation between vehicle types and time periods outside of this is almost imperceptible.\n\n\n\n\nFigure 4.4: Cleveland dot plots and heatmaps summarising crash frequencies by London borough, period of day and vehicle type.\n\n\n\n\n\nBetween-variable covariation: standardised bars and mosaic plots\n\n\n\n\n\nIn Figure 4.4 we began to make between-category comparison; we asked whether there are relatively greater or fewer crashes by time period or vehicle type in certain boroughs than others. There are chart types that explicitly support these sorts of analysis tasks. Figure 4.5 compares pedestrian crash frequencies in London by vehicle type involved and whether the crash occured on weekdays or weekends.\nFirst, stacked bars are ordered by frequency, distinguishing time period using colour lightness. Cars are by far the dominant travel mode, contributing the largest number of crashes resulting in injury to pedestrians. Whether or not pedestrian injuries involving cars occur relatively more on weekends than other modes is not clear from the left-most chart. Length encodes absolute crash counts effectively but relative comparison on time periods between vehicle types is challenging. In standardised bars the absolute length of bars is fixed and bars are split according to proportional weekday / weekend crashes (middle). The plot is also annotated according to the expected proportion of weekday crashes if crashes occurred by time period independently of vehicle type (22%). This encoding shows pedestrian crashes involving taxis occur in much greater frequency than would be expected at weekends, whilst the reverse is true of crashes involving vans, bikes and HGVs. However, we lose a sense of the absolute numbers involved.\nFailing to encode absolute number – the amount of information in support of some observed pattern – is a problem in EDA. Since proportional summaries are agnostic to sample size, they can induce false discoveries, overemphasising patterns that may be unlikely to replicate in out-of-sample tests. It is sometimes desirable, then, to update standardised bar charts so that they are weighted by frequency: to make more visually salient those categories that occur more often and visually downweight those that occur less often. This is possible using mosaic plots (Friendly 1992). Bar widths and heights are allowed to vary; so bar area is proportional to absolute number of observations and bars are further subdivided for relative comparison across category values. Mosaic plots are useful tools for exploratory analysis. That they are space-efficient and regularly sized, means they can be flexibly laid out for comparison.\n\n\n\n\nFigure 4.5: Bars (and Mosaic plot) displaying association between vehicle type and injury severity.\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe mosaic plot in Figure 4.5 was generated using the ggmosaic package, an extension to ggplot2.\n\n\n\n\n\nUpdating graphics to show deviation from expectation\nThe heatmaps in Figure 4.4 reflect the dominating effect of cars and weekdays. Any additional structure by time period and vehicle type in boroughs outside of this is visually unintelligible. An alternative approach could be to colour cells according to some relevant effect size statistic: for example, differences in the proportion of weekend crashes occurring in any vehicle-type and borough combination against the global average proportion, or expectation, of 22% of crashes occurring on weekends. Diminishing sample sizes in certain borough-vehicle type combinations nevertheless means such an encoding risks overstating large proportional effects based on negligible differences in absolute crash frequencies.\nThere are measures of effect size sensitive both to absolute and relative differences from expectation. Signed chi-score residuals (Visalingam 1981), for example, represent expected values as counts separately for each category combination in a dataset – in this case, pedestrian crashes recorded in a borough in a stated time period involving a stated vehicle type. Observed counts (O_i ... O_n) are then compared to expected counts (E_i ... E_n) as below:\n\\chi=\\frac{O_i - E_i}{\\sqrt{E_i}}\nThe way in which the differences between observed and expected values (residuals) are standardised in the denominator is important. If the denominator was simply the raw expected value, the residuals would express the proportional difference between each observation and its expected value. The denominator is instead transformed using the square root (\\sqrt{E_i}), which has the effect of inflating smaller expected values and squashing larger expected values in the denominator, thereby giving greater saliency to differences from expectation that are large in both relative and absolute number.\nFigure 4.6 updates the heatmaps with signed residuals encoded using a diverging colour scheme – red for cells with greater crash counts than expected, blue for cells with fewer crash counts than expected. The assumption in the first heatmap is that crash counts by borough distribute independently of vehicle type. Laying out the heatmap such that inner and outer London boroughs are grouped for comparison is instructive: fewer than expected crashes in inner London are recorded for cars; greater than expected for all other vehicle types but especially taxis and bicycles. This pattern is strongest (largest residuals) for very central boroughs, where pedestrian crash frequencies are also likely to be highest and where cars are comparatively less dominant as a travel mode. For almost all boroughs, again especially central London boroughs, there is a clear pattern of modes other than cars, taxis and buses overrepresented amongst crashes occurring on weekdays, again reflecting the transport dynamics of the city.\n\n\n\n\nFigure 4.6: Heatmaps of crashes by preiod of week and vehicle type for London Boroughs.\n\n\n\n\n\n\n\n4.2.4 Strategies for supporting comparison\nA key role for data graphics in EDA is in supporting comparison. Three strategies typically deployed are juxtaposition, superposition and direct encoding (see Gleicher and Roberts 2011). Table 4.2 defines each and identifies how they can be implemented in ggplot2.\n\n\n\n\n\n\nTable 4.2:  Implementing Gleicher et al.’s (2011) three comparison strategies in ggplot2. \n \n Strategy \n    Function \n    Use \n  \n\n\n Juxtaposition \n    faceting \n    Create separate plots in rows and/or columns by conditioning on a categorical variable. Each plot has same encoding and coordinate space. \n  \n\n Juxtaposition \n    patchwork pkg \n    Flexibly arrange plots of different data type, encoding and coordinate space. \n  \n\n Superposition \n    geoms \n    Layering marks on top of each other. Marks may be of different data types but must share the same coordinate space. \n  \n\n Direct encoding \n    NA \n    No strategy specialised to direct encoding. Often variables cross 0, so diverging schemes, or graphics with clearly annotated and symbolised thresholds are used. \n  \n\n\n\n\n\nAs with most visualization design, each involves trade-offs and so careful decision-making. In Figure 4.4 dotplots representing counts of weekend and weekday crashes are superposed on the same coordinate space, with connecting lines added to emphasise difference. This strategy is fine where two categories of similar orders of magnitude are compared, but if instead all eight vehicle types were to be encoded in the same way, superposed with categories differentiated using colour and a dominating modal category (cars), the plot would be substantially more challenging to process. In Figure 4.6, comparison by vehicle type is instead effected using direct encoding – residuals coloured above or below an expectation. Notice also the use of containment, juxtaposition and layout in both plots. By containing frequencies for inner- and outer- London boroughs in separate, juxtaposed plots, and within each laying out plot elements top-to-bottom on frequency, the systematic differences in the composition of vehicle types involved in crashes between inner- and outer- London can be inferred.\nComparison and layout\nThe effective use of layout is fundamental to visual data analysis. Judiciously laid out data graphics can help explore and test complex patterns that would be difficult to capture through numerical summaries alone. In Figure 4.7 mosaic plots of crash frequencies (bar height) and relative number of weekday/weekend crashes (dark bar width) are presented for each borough in London, with plots for vehicle type juxtaposed side-by-side for comparison. Rather than laying out boroughs top-to-bottom on frequency, boroughs are given an approximate spatial arrangement, generated using the gridmappr package. This arrangement enables several patterns to be quickly inferred. From it, we can observe that pedestrian crashes involving bicycles and taxis generally occur more in central London boroughs; those involving cars and taxis occur in greater relative number during weekends, especially so for those that take place in central London boroughs; and, different from other vehicle types, pedestrian crashes involving cars occur in reasonably large numbers in outer London boroughs.\n\n\n\n\n\n\n\nFigure 4.7: Mosaic plots of vehicle type and period for London Boroughs with geo-spatial arrangement."
  },
  {
    "objectID": "04-explore.html#techniques",
    "href": "04-explore.html#techniques",
    "title": "4  Exploratory Data Analysis",
    "section": "\n4.3 Techniques",
    "text": "4.3 Techniques\nThe technical element to this chapter continues with the STATS19 road crash data. Rather than a how-to guide for generating exploratory analysis plots in R, the section aims to demonstrate a workflow for exploratory visual data analysis:\n\nExpose pattern\nModel an expectation derived from pattern\nShow deviation from expectation\n\nIt does so by exploring the characteristics of individuals involved in pedestrian crashes, with a special focus on inequalities. Research suggests those living in more deprived neighbourhoods are at elevated risk of road crash injury than those living in less-deprived areas (Vidal Tortosa et al. 2021). A follow-up question is around the characteristics of those involved in crashes. To what extent do drivers share demographic characteristics with the pedestrians they crash into, and does this vary in expected ways by the location in which crashes take place?\n\n4.3.1 Import\n\nDownload the 04-template.qmd file for this chapter and save it to the reports folder of your vis4sds project.\nOpen your vis4sds project in RStudio and load the template file by clicking File > Open File ... > reports/04-template.qmd.\n\nThe presented analysis is based on that published in Beecham and Lovelace (2022) and investigates vehicle–pedestrian crashes in STATS19 between 2010 and 2019, where the Indeces of Multiple Deprivation (IMD) class of the pedestrian, driver and crash location is recorded.\nSTATS19 is a form used by the police to record road crashes that result in injury. Raw data are released by the Department for Transport, but can be also accessed via the stats19 R package. The data are organised into three tables:\n\n\nAccidents (or Crashes): Each observation is a recorded road crash with a unique identifier (accident_index), date (date), time (time) and location (longitude, latitude). Many other characteristics associated with the crashes are also stored in this table.\n\nCasualties: Each observation is a recorded casualty that resulted from a road crash. The Crashes and Casualties data can be linked via the accident_index variable. As well as casualty_severity (Slight, Serious, Fatal), information on casualty demographics and other characteristics is stored in this table.\n\nVehicles: Each observation is a vehicle involved in a crash. Again Vehicles can be linked with Crashes and Casualties via the accident_index variable. As well as the vehicle type and manoeuvre being made, information on driver characteristics is recorded in this table.\n\nAn .fst dataset containing pedestrian crashes linked to casualty and driver characteristics has been stored in the book’s accompanying data repository. There are simplifications and assumptions made when generating this linked dataset, detailed in the template file.\n\n\n\n4.3.2 Sample\n\nWe first create a new dataset – ped_veh_complete – identifying those linked crashes where the full IMD data are recorded:\n\n# Complete demographics for pedestrians, drivers and crash locations.\nped_veh_complete <- ped_veh |>\n  filter(\n    !is.na(casualty_quintile),\n    casualty_quintile != \"Data missing or out of range\",\n    driver_quintile != \"Data missing or out of range\"\n    )\n\nThe dataset contains c. 49,000 observations, 23% of recorded pedestrian crashes. Although this is a large number, there may be some systematic bias in the types of pedestrian crashes for which full demographic data are recorded, a challenge often encountered when datasets are repurposed for analysis. For brevity, we will not extensively investigate this bias, but below ‘record completeness rates’ are calculated for selected crash types. As anticipated, lower completeness rates appear for crashes coded as Slight in injury severity. Interestingly for our study, there are also lower completeness rates for crashes occurring in the highest deprivation quintile.\n\n\n\n\n\n\n\n\nTable 4.3:  Proportion of records – or completeness rates – where IMD status of pedestrian, driver and crash location are recorded. \n \n Crash category \n    Completeness rate \n  \n\nCasualty severity\n\n Fatal \n    26% \n  \n\n Serious \n    26% \n  \n\n Slight \n    20% \n  \nIMD class of crash location\n\n 5 Least deprived \n    25% \n  \n\n 4 Less deprived \n    25% \n  \n\n 3 Mid deprived \n    24% \n  \n\n 2 More deprived \n    23% \n  \n\n 1 Most deprived \n    19% \n  \n\n\n\n\n\nThis difference in record completeness may reflect genuine differences in recording behaviour for crashes occurring in high deprivation neighbourhoods, or it might be a function of some confounding context. For example, one might expect that crashes that are more serious in nature are reported in greater detail and so have higher completion rates; this might then explain the varying completeness rates if crashes resulting in slight injuries are overrepresented in high deprivation areas. To explore this further we can calculate completeness rates separately by crash injury severity (Figure 4.8). This demonstrates an expected difference in completion rates depending on injury severity, but that record completion rates are still lower for crashes taking place in the high deprivation neighbourhoods.\n\n\n\n\nFigure 4.8: Completeness rates by IMD class of crash location and crash injury severity.\n\n\n\n\nThe code:\n\nped_veh |>\n  mutate(\n    is_complete=accident_index %in% (ped_veh_complete |> pull(accident_index)),\n    is_ksi=if_else(accident_severity != \"Slight\", \"Fatal | Serious\", \"Slight\")\n  ) |>\n  group_by(crash_quintile, is_ksi) |>\n  summarise(prop_complete=mean(is_complete)) |>\n  ggplot() +\n  geom_point(aes(y=crash_quintile, x=prop_complete, colour=is_ksi), size=2) +\n  scale_colour_manual(values=c(\"#67000d\", \"#fb6a4a\")) +\n  scale_x_continuous(limits=c(0.1,.4))\n\nA breakdown of the ggplot2 spec:\n\n\nData: A boolean variable, is_completeness, is defined by checking accident_index against those contained in the ped_veh_complete dataset. Note that the pull() function extracts accident_index from the complete dataset as a vector of values. A boolean variable also to group Fatal and Serious injury outcomes. We then group on crash_quintile and is_ksi to calculate completeness rates by severity and crash location. Since is_complete is a boolean value (false=0, true=1), its mean is the proportion of true records, in this case those with a complete status.\n\nEncoding: Arrange dots vertically (y position) on crash_quintile and horizontally (x position) on prop_complete and colour on injury severity (is_ksi).\n\nMarks: geom_point() for drawing points.\n\nScale: Passed to scale_colour_manual() are hex values for dark and light red, according to ordinal injury severity.\n\n4.3.3 Abstract and relate\nMotivating this analysis is the characteristics, as measured by neighbourhood-level IMD, of pedestrians involved in road crashes, the drivers crashing into them and the locations in which crashes occur. We wish to ask to what extent drivers share demographic characteristics with the pedestrians they crash into, and does this vary by the location in which crashes take place?\nTo start, we abstract over the variables from which this profile is to be drawn: five IMD classes from high-to-low deprivation (IMD quintile 1–5) for pedestrians, drivers, and crash locations. Figure 4.9 summarises frequencies across these categories. Pedestrian crashes occur more frequently in higher deprivation neighbourhoods; those injured more often live in higher deprivation neighbourhoods; and the same applies to drivers involved in crashes. This high-level pattern is consistent with existing research (Vidal Tortosa et al. 2021) and is a function of various conditioning context. High deprivation areas are located in greater number in urban areas and so we would expect greater numbers of pedestrian crashes to occur in such areas. The shapes of the bars nevertheless suggest that are are inequalities in the characteristics of pedestrians and drivers involved in crashes: the frequencies are most skewed towards high deprivation for pedestrians and more uniform across deprivation classes for drivers. This may indicate an importing effect of drivers living in lower deprivation areas crashing into pedestrians living in higher deprivation areas.\n\n\n\n\nFigure 4.9: Frequencies of pedestrian crashes by IMD class of crash location, pedestrian injured and driver involved.\n\n\n\n\nThe code:\n\nped_veh_complete |>\n  inner_join(imd |> select(lsoa_code, total_pop), by=c(\"lsoa_of_accident_location\"=\"lsoa_code\"))  |>\n  select(driver_quintile, casualty_quintile, crash_quintile) |>\n  pivot_longer(cols=everything(), names_to=\"location_type\", values_to=\"imd\") |>\n  group_by(location_type, imd) |>\n  summarise(count=n()) |> ungroup() |>\n  separate(col=location_type, into=\"type\", sep=\"_\", extra = \"drop\") |>\n  mutate(\n    type=case_when(\n      type==\"casualty\" ~ \"pedestrian\",\n      type==\"crash\" ~ \"location\",\n      TRUE ~ type),\n    type=factor(type, levels=c(\"location\", \"pedestrian\", \"driver\"))\n  ) |>\n  ggplot() +\n  geom_col(aes(x=imd, y=count), fill=\"#003c8f\") +\n  scale_x_discrete(labels=c(\"most\",\"\", \"mid\", \"\", \"least\")) +\n  facet_wrap(~type)\n\nThe ggplot2 spec:\n\n\nData: First identify the IMD class of crash locations by joining the lsoa_code recorded for those locations on the imd dataset. Next, we calculate frequencies on which the plot depends: the IMD status of crash locations, pedestrians or drivers involved in pedestrian crashes. To do this, we pivot_longer() on the respective IMD variables so that each row is a crash record and IMD class. The dataset is then grouped on these variables in order to count frequencies of location, pedestrian and drivers by IMD class involved. Finally, we use mutate() to recode the type variable with more expressive labels for locations, pedestrians and drivers and cast the variable as a factor in order to control the order in which variables appear when type is conditioned on to generate a faceted plot.\n\nEncoding: Bars arranged vertically (y position) on frequency and horizontally (x position) on imd class.\n\nMarks: geom_col() for drawing bars.\n\nFacets: facet_wrap() for faceting on the type variable (location, pedestrian or driver).\n\n4.3.4 Model and residual: Pass 1\nTo explore how the characteristics of pedestrians and drivers co-vary more directly, we can compute the joint frequency of each permutation of driver-pedestrian IMD quintile group. This results in 5x5 combinations and in Figure 4.10 these combinations are represented in a heatmap. Cells of the heatmap are ordered left-to-right on the IMD class of pedestrian and bottom-to-top on the IMD class of driver. Arranging cells in this way encourages linearity in the association to be emphasised. The darker blues in the diagonals demonstrate that an association between pedestrian-driver IMD characteristics exists: drivers and passengers living in similar types of neighbourhoods are involved in crashes with one another with greater frequency than those living in different types of neighbourhoods.\n\n\n\n\nFigure 4.10: Pedestrian casualties by IMD quintile of pedestrian and driver.\n\n\n\n\nA consequence of the heavy concentration of crash counts, and thus colour, in the high-deprivation cells is that it is difficult to gauge variation and the strength of association between IMD class of driver and pedestrian in the lower deprivation cells. Again we can use exploratory models to support our analysis. In this case, our (naive) expectation is that crash frequencies distribute independently of the IMD class of the pedestrian-driver involved and compute signed chi-scores describing how different the observed number of crashes in each cell position is from this expectation. The second row of Figure 4.10 demonstrates how expectation is spread based on absolute numbers within rows and columns of the heatmap: expected values for each cell are derived from multiplying across these column and row marginals.\nThe observed-versus-expected plot highlights the largest positive residuals are in and around the diagonals and the largest negative residuals are those furthest from the diagonals: we see higher crash frequencies between drivers and pedestrians living in the same or similar IMD quintiles and fewer between those in different quintiles than would be expected given no association between pedestrian-driver IMD characteristics. That the bottom left cell – high-deprivation-driver + high-deprivation-pedestrian – is dark red can be understood when remembering that signed chi-scores emphasise effect sizes that are large in absolute as well as relative number. Not only is there an association between the characteristics of drivers and casualties, but larger crash counts are recorded in locations containing the highest deprivation and so residuals here are large. The largest positive residuals are nevertheless recorded in the top right of the heatmap – and this is more surprising. Against an expectation of no association between the IMD characteristics of drivers and pedestrians, there is a particularly high number of crashes between drivers and pedestrians living in neighbourhoods containing the lowest deprivation. An alternative phrasing: the IMD characteristics of those involved in pedestrian crashes are most narrow between drivers and pedestrians who live in the lowest deprivation quintiles.\nThe code:\n\nmodel_data <- ped_veh_complete |>\n  mutate(grand_total=n()) |>\n  group_by(driver_quintile) |>\n  mutate(row_total=n()) |> ungroup() |>\n  group_by(casualty_quintile) |>\n  mutate(col_total=n()) |> ungroup() |>\n  group_by(casualty_quintile, driver_quintile) |>\n  summarise(\n    observed=n(),\n    row_total=first(row_total),\n    col_total=first(col_total),\n    grand_total=first(grand_total),\n    expected=(row_total*col_total)/grand_total,\n    resid=(observed-expected)/sqrt(expected),\n  )\n\nmodel_data |>\n  ggplot() +\n  geom_tile(\n    aes(x=casualty_quintile, y=driver_quintile, fill=resid),\n    colour=\"#707070\", size=.2\n    ) +\n  scale_fill_distiller(\n    palette=\"RdBu\",\n    direction=-1,\n    limits=c(- max(model_data$resid), max(model_data$resid))\n    ) +\n  coord_equal()\n\nThe ggplot2 spec:\n\n\nData: We create a staged dataset for plotting. Observed values for each cell of the heatmap are computed, along with row and column marginals for calculating expected values. The way in which the signed chi-score residuals are calculated follows that described earlier in the chapter.\n\nEncoding: Cells of the heatmap are arranged in x and y on the IMD class of pedestrians and drivers and filled according to signed chi-score residuals .\n\nMarks: geom_tile() for drawing cells of the heatmap.\n\nScale: scale_fill_distiller() for continuous ColorBrewer scheme, diverging using the RdBu palette. To make the scheme centred on 0, the maximum absolute residual value in model_data is used.\n\n4.3.5 Model and residual: Pass 2\nAn obvious confounding factor, neglected in the analysis above, is the IMD class of the location in which crashes occur. To explore this, we can condition (or facet) on the IMD class of crash location, laying out the faceted plot left-to-right on the ordered IMD classes. Eyeballing this graphic of observed counts (Figure 4.11) we see again the association between IMD characteristics for crashes occurring in the least deprived quintile and elsewhere slightly more ‘mixing’. Few pedestrians living outside the most deprived quintile are involved in crashes that occur in that quintile. Given the dominating pattern is of crashes occurring in the most deprived quintiles, however, it is difficult to see too much variation from the diagonal cell in the less-deprived quintiles. An easy adjustment would be to apply a local colour scale for each faceted plot and compare relative ‘leakage’ from the diagonal for each deprivation level of crash location. The more interesting question, however, is whether this known association between pedestrian and driver characteristics is stronger for certain driver-pedestrian-location combinations than others: that is, net of the dominant pattern in Figure 4.11, in which cells are there greater or fewer crash counts?\nThe concept that we are exploring is whether crash counts vary depending on how different are the IMD characteristics of pedestrians and drivers from those of the locations in which crashes occur. We calculate a new variable measuring this distance: ‘geodemographic distance’, the euclidean distance between the IMD class of the driver-pedestrian-crash location, treating the IMD class as a continuous variable ranging from 1 to 5. The second row of Figure 4.11 encodes this directly. We then specify a Poisson regression model, modelling crash counts in each driver-pedestrian-crash location cell as a function of geodemographic distance for that cell. Since the range of the crash count varies systematically by IMD class, the model is extended with with a group-level intercept term that varies on the IMD class of the crash location. If regression modelling frameworks are new to you, don’t worry about the specifics. The residuals from this regression model are expressed in the same way as in the signed-chi-score model and show whether there are greater or fewer crash counts in any pedestrian-driver-location combination than would be expected given the amount of geodemographic difference between individuals and locations involved. Our expectation is that crash counts vary inversely with geodemographic distance. In EDA, we are not overly concerned with confirming this to be the case; instead we use our data graphics to explore where in the distribution, and by how much, the observed data depart from this expectation.\n\n\n\n\nFigure 4.11: Pedestrian casualties by IMD quintile of pedestrian, driver and crash location.\n\n\n\n\nThe vertical block of red in the left column of the left-most matrix (crashes occurring in high-deprivation areas) indicates higher than expected crash counts for pedestrians living and being hit in the most deprived quintile, both by drivers living in that high-deprivation quintile and the less-deprived quintiles (especially the lowest quintiles), as evidenced by the vertical strip. This pattern is important as it persists even after having modelled for ‘geodemographic distance’. There is much to unpick elsewhere in the graphic. Like many health issues, pedestrian road injuries have a heavy socio-economic element and our analysis has identified several patterns that may be worthy of more formal investigation.\n\n\nThe modelling is somewhat involved – a more gentle introduction to model-based visual analysis appears in Chapters 6 and 7 – but the code for generating the model and graphics in Figure 4.11 is:\n\nmodel_data <- ped_veh_complete |>\n  mutate(\n    # Derive numeric values from IMD classes (ordered factor variable).\n    across(\n      .cols=c(casualty_quintile, driver_quintile, crash_quintile),\n      .fns=list\n        (num=~as.numeric(factor(., levels=c(c(\"1 most deprived\", \"2 more deprived\", \"3 mid deprived\", \"4 less deprived\", \"5 least deprived\")))))\n    ),\n    # Calculate demog_distance.\n    demog_dist=sqrt(\n      (casualty_quintile_num-driver_quintile_num)^2 +\n      (casualty_quintile_num-crash_quintile_num)^2 +\n      (driver_quintile_num-crash_quintile_num)^2\n    )\n  ) |>\n  # Calculate over observed cells: each ped-driver IMD class combination.\n  group_by(casualty_quintile, driver_quintile, crash_quintile) |>\n  summarise(crash_count=n(), demog_dist=first(demog_dist)) |> ungroup()\n\n# Model crash count against demographic distance allowing the intercept to vary\n# on crash quintile.\nmodel <- lme4::glmer(crash_count ~ demog_dist + ( 1 | crash_quintile),\n                     data=model_data, family=poisson, nAGQ = 100)\n\n# Extract model residuals.\nmodel_data <- model_data %>%\n  mutate(ml_resids=residuals(model, type=\"pearson\"))\n\n# Plot.\nmodel_data |>\n  ggplot() +\n  geom_tile(\n    aes(x=casualty_quintile, y=driver_quintile, fill=ml_resids),\n    colour=\"#707070\", size=.2\n    ) +\n  scale_fill_distiller(\n    palette=\"RdBu\", direction=-1,\n    limits=c(-max(model_data$ml_resids %>% abs()), max(model_data$ml_resids %>% abs()))\n  )+\n  facet_wrap(~crash_quintile, nrow=1) +\n  coord_equal()"
  },
  {
    "objectID": "04-explore.html#conclusion",
    "href": "04-explore.html#conclusion",
    "title": "4  Exploratory Data Analysis",
    "section": "\n4.4 Conclusion",
    "text": "4.4 Conclusion\nExploratory data analysis (EDA) is an approach to analysis that aims to amplify knowledge and understanding of a dataset. The idea is to explore structure, and data-driven hypotheses, by quickly generating many often throwaway statistical and graphical summaries. In this chapter we discussed chart types for exposing distributions and relationships in a dataset, depending on data type. We also showed that EDA is not model-free. Data graphics help us to see dominant patterns and from here formulate expectations that are to be modelled. Different from so-called confirmatory data analysis, however, in an EDA the goal of model-building is not “to identify whether the model fits or not […] but rather to understand in what ways the fitted model departs from the data” (Gelman 2004). We covered visualization approaches to supporting comparison between data and expectation using juxtaposition, superimposition and direct encoding (Gleicher and Roberts 2011). The chapter did not provide an exhaustive survey of EDA approaches, and certainly not an exhaustive set of chart types and model formulations for exposing distributions and relationships. By locating the session closely in the STATS19 dataset, we learnt a workflow for EDA that is common to most effective data analysis and communication activity:\n\nExpose pattern\nModel an expectation derived from pattern\nShow deviation from expectation"
  },
  {
    "objectID": "05-network.html",
    "href": "05-network.html",
    "title": "5  Networks",
    "section": "",
    "text": "By the end of this chapter you should gain the following knowledge and practical skills."
  },
  {
    "objectID": "05-network.html#introduction",
    "href": "05-network.html#introduction",
    "title": "5  Networks",
    "section": "\n5.1 Introduction",
    "text": "5.1 Introduction\nNetworks are a special class of data used to represent things, entities, and how they relate to one another. Network data consist of two types of element: nodes, the entities themselves, and edges, the connections between nodes. Both nodes and edges can have additional information attached to them – counts, categories and directions. Network data are cumbersome to work with in R as they are not represented well by flat data frames. A common workflow is to split the data across two tables – one representing nodes and one representing edges (Wickham, Navarro, and Lin Pedersen 2020).\nA category of network data used heavily in geospatial analysis is origin-destination (OD) data describing, for example, flows of bikes (Beecham et al. 2023) and commuters (Beecham and Slingsby 2019) around a city. These data consist of nodes, origin and destination locations, and edges, flows between those origins and destinations. Whilst statistics from Network Science can and have (Yang et al. 2022) been deployed in the analysis of geospatial OD data, visualization techniques provide much assistance in exposing the types of complex structural patterns and relations that result from locating OD flows within geographic context.\nIn this chapter we will work with an accessible and widely used OD network dataset: Census travel-to-work data recording counts of individuals commuting between Census geographies of the UK based on their home and workplace. Specifically, we will work with data in London recording travel-to-work between the city’s 33 boroughs."
  },
  {
    "objectID": "05-network.html#concepts",
    "href": "05-network.html#concepts",
    "title": "5  Networks",
    "section": "\n5.2 Concepts",
    "text": "5.2 Concepts\n\n5.2.1 Node summary\nThe nodes in this dataset are London’s 33 boroughs and the edges are directed OD pairs between these boroughs. In Figure 5.1 frequencies of the number of jobs available in each borough and workers living in each borough (the nodes) are represented. Note that job-rich boroughs in central London – Westminster, City of London, Camden, Tower Hamlets – contain many more jobs than workers residing in them. We can infer that there is a high level of in-commuting to those boroughs and the reverse, a high level of out-commuting for boroughs containing large numbers of workers but comparatively fewer jobs.\n\n\n\n\nFigure 5.1: Barchart of jobs and workers contained in London boroughs.\n\n\n\n\n\n5.2.2 Node-link representations\nThe most common class of network visualization used to represent network data are node-link diagrams. These depict graphs in two dimensions as a force-directed layout. Nodes are positioned such that those sharing greater connection – edges with greater frequencies – are closer than those that are less well-connected – that do not share edges with such large frequencies. Edges are drawn as lines connecting nodes, and so node-link diagrams.\nFigure 5.2 uses a force-directed layout to represent the travel-to-work data. This shows large numbers of flows between job-rich boroughs, Westminster (Wst), and boroughs containing large resident populations: Wandsworth (Wnd), Lambeth (Lmb), Southwark (Sth) close to central London; Barnet (Barnt), Brent, Haringey (Hrn) in outer north London; Croydon (Cry), Bromley (Brm) and Sutton (Stt) in outer south London. That these boroughs are close to one another in force-directed space suggests that, as expected, between-borough commuting has a spatially dependent element.\nTo investigate this more directly it makes sense to position nodes with a geographic arrangement. In the right column of Figure 5.2, nodes (boroughs) are placed in their exact geographic position (geometric centroid of boroughs) and line width and colour is used to encode edge frequency. In the plot on the bottom row, nodes are represented with circles sized according to frequency (the total number of jobs and workers contained in the borough) and flow direction is encoded by making flow lines asymmetric (following Wood, Slingsby, and Dykes 2011): the straight ends are origins, the curved ends destinations.\n\n\n\n\nFigure 5.2: Flowlines with edges showing frequencies between London boroughs.\n\n\n\n\nThe geopraphic positioning of nodes adds context and the encoding of direction provides further detail. For example, the pattern of commuting into central London boroughs versus more peripheral boroughs – asymmetric flows into Westminster, more symmetric pattern between outer London boroughs. However, there are problems that affect the usefulness of the graphic. Self-contained flows – counts of those living and working in the same borough – are not shown; longer flows are more visually dominant than are shorter flows; the graphic is cluttered with a ‘hairball’ effect due to multiple overlapping lines; and aggregating to the somewhat arbitrary geometric centre of boroughs and drawing lines between these locations implies an undue level of spatial precision – the geographic pattern of commuting would likely look very different were individual flows encoded with precise OD locations of home and workplace.\n\n5.2.3 Matrix representations\nAn alternative way to represent these edge frequencies is origin-destination matrices, as in Figure 5.4. The columns are destinations, London boroughs into which residents commute for work; the rows are origins, London boroughs from which residents commute out for work. Edge frequencies are encoded using colour value – the darker the colour, the larger the number of commutes between those boroughs. Boroughs are ordered left-to-right and top-to-bottom according to the total number of jobs accessed in each borough.\nWhilst using colour rather than line width to show flow magnitude is a less effective encoding channel (following Munzner 2014), there are obvious advantages. The salience bias of longer flows is removed – every OD pair, 1039 in total (33^2), is given equal graphic saliency. Ordering cells of the matrix by destination size (number of jobs accessed in each borough) helps to emphasise patterns in the job-rich boroughs, but also encourages within and between borough comparison. For example, the lack of colour outside of the diagonals in the less job-rich boroughs, which also tend to be in outer London, suggests that labour markets there might be more self-contained: that is, people are less likely to commute-in from outside of those boroughs for work. By applying a local scaling on destination (right plot), a separate colour scale is created for each destination borough, we can explore commutes into individual boroughs in a more involved way. The vertical strips of blue for other job-rich central and inner London boroughs (Hammersmith & Fulham and Kensington & Chelsea), suggesting reasonably high-levels of in-commuting to access jobs there.\n\n\n\n\nFigure 5.3: Origin-destination matrices ordered according to borough size on number of jobs.\n\n\n\n\n\n5.2.4 OD maps\nThe OD matrices expose new structure that could not be so easily inferred from the node-link visualizations. Although their layout is space-efficient, clearly for phenomena such as commuting geographic context is highly relevant. OD maps (Wood, Dykes, and Slingsby 2010) make better use of layout and position to support the spatial dimension of analysis. They take a little to get your head around, but the idea is elegant.\nOD maps contains exactly the same cells as an OD matrix, but the cells are re-ordered with an approximate geographic arrangement, as in the bottom row of Figure 5.4. So, for example, we may be interested in focussing on destination, or workplace, boroughs. In the first highlighted example, commutes into Westminster are considered (the left-most column of the OD matrix). Cells in the highlighted column are coloured according to the number of workers resident in each borough that travel into Westminster for work. In the inset map to the right these cells are then re-ordered with an approximate spatial arrangement, generated via the gridmappr package. The geographic ordering allows us to see that residents access jobs in Westminster in large numbers from many boroughs in London, but especially from Wandsworth (Wns), Lambeth (Lmb) and Southwark (Sth) to the south of Westminster (Wst). In the second example, we focus on origins: commutes out of Hackney are considered (the middle row). Cells in the highlighted row are coloured according to the number of jobs accessed in each borough by residents travelling out of Hackney for work. Cells are again reordered in the inset map. This demonstrates that patterns of commuting are reasonably localised. The modal destination/workplace borough remains Westminster, but relatively large numbers of jobs are accessed in Camden (Cmd), Islington (Isl), Tower Hamlets (TwH) and the City of London (CoL).\n\n\n\n\nFigure 5.4: Origin-destination matrices: highlighted destination (Westminster) and origin (Hackney) with geospatial arrangement.\n\n\n\n\nOD maps extend this idea by displaying all cells of the OD matrix with a geographic arrangement. This is achieved via a ‘map-within-map’ layout. In the destination-focussed example in Figure 5.5, each larger (reference) cell identifies destinations and the smaller cells are coloured according to origins – the number of residents in each borough commuting into the reference cell for work. The map uses a local colour scaling, with same origin-destination cells removed. Flow counts are summarised over each reference borough (destination in this case) and normalised according to the maximum flow count for that reference borough. The local scaling allows us to characterise the geography of commuting into boroughs in some detail. The two job-rich boroughs, Westminster and City of London, clearly draw workers in large proportions across London boroughs and to a lesser extent this is the case for other central/inner boroughs such as Islington (Isl), Camden (Cmd) and Tower Hamlets (TwH). For outer London boroughs, commuting patterns are more localised – large numbers of available jobs are filled by workers living in immediately neighbouring boroughs (if same origin-destination boroughs were included this pattern would be reinforced). Readers familiar with London’s geography may notice that inner London boroughs south of the river – Lambeth (Lam), Wandsworth (Wnd), Southwark (Sth) – tend to draw workers in greater number from boroughs that are also south of the river.\n\n\n\n\n\n\nFigure 5.5: Destination-focussed OD map of commutes between London boroughs with local scaling.\n\n\n\n\n\n\n\n\n\n\n\nOD maps versus node-link diagrams\n\n\n\nThe analysis above might give the impression that OD maps should be used in preference to node-link diagrams. As always, this depends on dataset and analysis task. In Figure 5.6 is a map displaying bikeshare flow data for the London Cycle Hire Scheme (LCHS), collected via the bikedata package. The LCHS consists of c.700 docking stations in London – so c. 700^2 grid cells if the grid-within-grid layout of an OD map were to be used, which would certainly present a design challenge. If a synoptic overview of spatial patterns is necessary, the more intuitive node-link representation is perhaps more successful than the OD map in any case. Additionally, different from Census commutes, there is geographic precision in the spatial coordinates representing origins and destinations in the bikeshare dataset, and distance judgements seem important when exploring cycling trips – an argument against the spatial distortion required by OD maps.\n\n\n\n\n\nFigure 5.6: Spatial node-link graphic depicting morning peak-time travel from London Cycle Hire Scheme trips. Data by Transport for London, parks and river outline via OpenStreetMap"
  },
  {
    "objectID": "05-network.html#techniques",
    "href": "05-network.html#techniques",
    "title": "5  Networks",
    "section": "\n5.3 Techniques",
    "text": "5.3 Techniques\nThe technical element to this chapter continues in our analysis of 2011 Census travel-to-work data. After importing the dataset, we will organise the flow data into nodes and edges, before creating graphics that summarise over the nodes (London boroughs in this case) and reveal spatial structure in the edges (OD flows between boroughs). A focus for the analysis is on how the geography of travel-to-work varies by occupation type.\n\n5.3.1 Import\n\nDownload the 05-template.qmd file for this chapter and save it to the reports folder of your vis4sds project.\nOpen your vis4sds project in RStudio and load the template file by clicking File > Open File ... > reports/05-template.qmd.\n\nA .csv file containing Census travel-to-work data in London has been stored in the book’s accompanying data repository. Code for downloading the data is in the template file. The data can then be read into your session in the usual way.\n\n# Read in local copies of the Census travel-to-work data.\nod_pairs <- read_csv(here(\"data\", \"london_ttw.csv\"))\n\nIn order to generate an approximate geographic arrangement of London boroughs we will use the gridmappr R package. The development version can be downloaded with:\n\n# Uncomment the line below if you do not already have devtools.\n# install.packages(\"devtools\")\ndevtools::install_github(\"rogerbeecham/gridmappr\")\n\nThe od_pairs dataset has the following structure. This is effectively an edges table. Each observation is a unique OD pair summarising the total number of recorded commuters between a pair of London boroughs for a stated occupation type.\n\n\n\n\n\nTable 5.1:  Census OD travel-to-work data: edges (OD flows) table. \n \n o_bor \n    d_bor \n    occ_type \n    count \n    is_prof \n  \n\n\n Barnet \n    Westminster \n    1_managers_senior \n    2733 \n    TRUE \n  \n\n Barnet \n    Westminster \n    2_professional \n    4055 \n    TRUE \n  \n\n Barnet \n    Westminster \n    3_associate_professional \n    2977 \n    TRUE \n  \n\n Barnet \n    Westminster \n    4_administrative \n    2674 \n    FALSE \n  \n\n Barnet \n    Westminster \n    5_trade \n    687 \n    FALSE \n  \n\n Barnet \n    Westminster \n    6_caring_leisure \n    755 \n    FALSE \n  \n\n Barnet \n    Westminster \n    7_sales_customer \n    1255 \n    FALSE \n  \n\n Barnet \n    Westminster \n    8_machine_operatives \n    257 \n    FALSE \n  \n\n Barnet \n    Westminster \n    9_elementary \n    1309 \n    FALSE \n  \n\n\n\n\n\nNodes in the dataset are the 33 London boroughs. We can express commutes between these nodes in different ways, according to whether nodes are destinations or origins. In the code below, two tables are generated with OD data grouped by destination (nodes_d) and origin (nodes_o) and commutes into- and out of- boroughs counted respectively. These two data sets are then joined with bind_rows() and distinguished via the variable name type.\n\nnodes_d <- od_pairs |>\n  group_by(d_bor, occ_type) |>\n  summarise(\n    count = sum(count),\n    is_prof = first(is_prof)\n  ) |>\n  ungroup() |>\n  rename(la = d_bor) |>\n  mutate(type=\"jobs\")\n\nnodes_o <- od_pairs |>\n  group_by(o_bor, occ_type) |>\n  summarise(\n    count = sum(count),\n    is_prof = first(is_prof)\n  ) |>\n  ungroup() |>\n  rename(la = o_bor) |>\n  mutate(type=\"workers\")\n\nnodes  <- bind_rows(nodes_o, nodes_d)\n\n\n\n\n\n\nTable 5.2:  Census OD travel-to-work data: nodes (boroughs) table. \n \n la \n    is_prof \n    count \n    type \n  \n\n\n Barking and Dagenham \n    TRUE \n    3745 \n    workers \n  \n\n Barking and Dagenham \n    TRUE \n    7841 \n    workers \n  \n\n Barking and Dagenham \n    TRUE \n    5243 \n    workers \n  \n\n Barking and Dagenham \n    FALSE \n    8592 \n    workers \n  \n\n Barking and Dagenham \n    FALSE \n    3990 \n    workers \n  \n\n Barking and Dagenham \n    FALSE \n    6635 \n    workers \n  \n\n Barking and Dagenham \n    FALSE \n    5797 \n    workers \n  \n\n Barking and Dagenham \n    FALSE \n    4396 \n    workers \n  \n\n Barking and Dagenham \n    FALSE \n    7998 \n    workers \n  \n\n Barking and Dagenham \n    TRUE \n    2667 \n    jobs \n  \n\n\n\n\n\n\n5.3.2 Gridmap layout\nWe will analyse over the nodes and edges (flows) data by laying out data graphics with a geo-spatial arrangement. Such arrangements can be automatically created using the gridmappr R package. Given a set of point locations, the package creates a two-dimensional grid of user-specified dimensions and allocates points to the grid such that the distance between points in geographic and grid space is minimised. The main function to call is points_to_grid(). This takes a tibble of geographic points and returns grid cell positions (row and column identifiers). In the code below an 8x8 grid is used. The allocation is also constrained by a compactness parameter which determines the extent to which points are allocated to cells in the centre (compactness = 1), edges (0) or scaled geographic location (0.5) within the grid.\n\nlibrary(gridmappr)\nn_row <- 8\nn_col <- 8\npts <- london_boroughs |>\n  st_drop_geometry() |>\n  select(area_name, x = easting, y = northing)\nsolution <- points_to_grid(pts, n_row, n_col, compactness = .6)\n\nOnce a layout is generated, we can create a corresponding polygon object so that it can be plotted. This is achieved with make_grid(). This function takes an sf data frame containing polygons with ‘real’ geography and returns a sf data frame representing a grid, with variables identifying column and row IDs (bottom left is origin) and geographic centroids of grid squares. The gridded object can then be joined on a gridmap solution returned from (points_to_grid()) in order to create an object in which each grid cell corresponds to a gridmap allocation position.\n\ngrid <- make_grid(london_boroughs, n_row, n_col) |>\n  inner_join(solution)\n\nTo evaluate different layouts that could be generated from differently specified grid dimensions and/or compactness values, it can be useful to encode explicitly the geographic distortion introduced when moving centroids to regularly sized grid cells. In the example below, displacement vectors are drawn connecting the centroid of each borough in London in real and grid space. This is achieved with get_trajectory() from the odvis package.\n\n\n\n\nFigure 5.7: Displacement vectors showing distortion introduced by candidate gridmap layouts\n\n\n\n\nThe code is slightly more advanced. Some concepts, for example functional-style programming with map(), are discussed in a more involved way in later chapters. First, we combine the real and grid geographies in a single data frame. Then we map() over each real-to-grid location pair calling get_trajectory() to generate a data frame of trajectories – origins, destinations and control points, which affect the path of the vectors so that they curve towards the destination. Finally trajectories are plotted with geom_bezier() from the ggforce package, with separate lines (group=) for each real-to-grid OD pair.\n\n# Install odvis.\ndevtools::install_github(\"rogerbeecham/odvis\")\n\n# Combine the grid and london_boroughs (real geography)\n# objects into a single simple features data frame.\nlon_geogs <- bind_rows(\n  london_boroughs |> mutate(type = \"real\") |>\n    select(area_name, x = easting, y = northing, type),\n  grid |>  mutate(type = \"grid\") |>\n    select(area_name, x, y, type, geometry = geom)\n)\n\n# Create points for drawing trajectories\n# -- origin, destination and control point locations.\ntrajectories <- lon_geogs |>\n  st_drop_geometry() |>\n  filter(!is.na(ward_name)) |>\n  pivot_wider(names_from = type, values_from = c(x, y)) |>\n  mutate(id = row_number()) |>\n  nest(data = c(ward_name, x_real, y_real, x_grid, y_grid)) |>\n  mutate(\n    trajectory = map(\n      data,\n      ~get_trajectory(.x$x_real, .x$y_real, .x$x_grid, .x$y_grid, .x$ward_name)\n    )\n  ) |>\n  select(trajectory) |>\n  unnest(cols = trajectory)\n\n# Plot displacement vectors.\nggplot() +\n  geom_sf(\n    data = lon_geogs |> mutate(type = factor(type, levels = c(\"real\", \"grid\"))),\n    aes(fill = type, colour = type), linewidth = .2\n    ) +\n  ggforce::geom_bezier(\n    data = trajectories,\n    aes(x = x, y = y, group = od_pair),\n    colour = \"#08306b\", linewidth = .4\n    ) +\n  scale_fill_manual(values = c(\"#f0f0f0\", \"transparent\"), guide = \"none\") +\n  scale_colour_manual(values = c(\"#FFFFFF\", \"#525252\"), guide = \"none\")\n\n\n5.3.3 Analysing over the nodes (boroughs)\nIn Figure 5.8 are gridmaps summarising over the nodes (boroughs). The number of workers living in each borough (left column) and jobs available in each borough (right column) is encoded using circle size, with circles positioned in x, y at the centroids of the geospatial grid layout. Frequencies are shown separately for professional and non-professional occupation types. If you are familiar with London’s social geography, the patterns make sense. There are comparatively more non-professional workers living in the somewhat more affordable boroughs in outer and east London; and the importance of job-rich central London boroughs particularly for supplying professional jobs (Westminster Wst, Camden Cmd, City of London CoL, Tower Hamlets TwH).\n\n\n\n\nFigure 5.8: Workers and jobs in London borough by occupation class\n\n\n\n\nThe code:\n\ngrid |>\n  inner_join(\n    nodes |>\n      group_by(la, is_prof, type) |>\n      summarise(count=sum(count)),\n    by = c(\"area_name\" = \"la\")\n    ) |>\n  mutate(\n    is_prof = factor(\n        if_else(\n          is_prof, \"professional\", \"non-professional\"),\n          levels = c(\"professional\", \"non-professional\")\n        ),\n    type = factor(type, levels = c(\"workers\", \"jobs\")),\n  ) |>\n  ggplot(aes(x = x, y = y)) +\n  geom_sf(fill = \"#EEEEEE\") +\n  geom_point(\n    aes(size = count, colour = is_prof), alpha = .3\n    ) +\n  geom_point(\n    aes(size = count, colour = is_prof),\n    fill = \"transparent\", pch = 21, stroke = .5\n    ) +\n  facet_grid(is_prof ~ type) +\n  scale_fill_manual(values = c(\"#67000d\", \"#08306b\")) +\n  scale_colour_manual(values = c(\"#67000d\", \"#08306b\"))\n\nThe ggplot2 spec:\n\n\nData: From the derived nodes tibble, we count workers and jobs (type) by borough, collapsed over professional or non-professional occupation types (is_prof). Note that we also start by joining on grid in order to bring in the polygon file and coordinates of the generated gridmap. Casting is_prof and type to factor variables (the mutate()) gives us control over the order in which they appear in the plot.\n\nEncoding: the proportional symbols are positioned at the centroids of borough grid cells (x, y), sized according to count of jobs or workers and coloured according to occupation type (is_prof).\n\nMarks: geom_point() for proportional symbols and geom_sf() for grid outline – remember our dataset is now of class sf as we joined on the grid object.\n\nScale: scale_fill/colour_manual() for associating occupation type.\n\nFacets: facet_wrap() on workers/jobs summary type and high-level occupation type (is_prof).\n\nIn Figure 5.8, we collapsed over nine occupation types in order to plot proportional-symbol maps. Since gridmaps consist of regularly-sized cells, we can introduce more complex graphical summaries with a geographical arrangement. For example, Figure 5.9 uses bar charts to analyse the number of workers (left-pointing bars) and jobs (right-pointing bars) by occupation type across the the full nine occupation classes. In the selected examples below, jobs and workers are differentiated by varying the direction of bars: pointing to the right for jobs, to the left for workers. The counts are locally (borough-level) scaled. For each borough, its modal category count of jobs/workers is found and bar length is scaled relative to this modal category. This encoding allows us to distinguish between job-rich boroughs with longer bars pointing to the right (Westminster); resident/worker-rich boroughs with longer bars pointing to the left (Wandsworth); and outer London boroughs that are more self-contained (Hillingdon).\n\n\n\n\nFigure 5.9: Workers and jobs in selected London boroughs by full occupation classes\n\n\n\n\nThe code:\n\nplot_data <- solution |>\n  inner_join(nodes, by = c(\"area_name\" = \"la\")) |>\n  group_by(area_name) |>\n  mutate(count = count / max(count)) |>\n  ungroup() |>\n  mutate(\n    count = if_else(type == \"jobs\", count, -count),\n    occ_name = factor(occ_type),\n    occ_type = as.numeric(fct_rev(factor(occ_type)))\n  )\n\nplot_data |>\n  filter(area_name %in% c(\"Wandsworth\", \"Westminster\", \"Bexley\", \"Hillingdon\")) |>\n  ggplot(aes(x = occ_type, y = count)) +\n  geom_col(aes(fill = is_prof), alpha = .5, width = 1) +\n  geom_hline(yintercept = 0, linewidth = .4, colour = \"#ffffff\") +\n  facet_wrap(~area_name) +\n  scale_y_continuous(limits = c(-1, 1)) +\n  scale_fill_manual(values = c(\"#08306b\", \"#67000d\"), guide = \"none\") +\n  coord_flip()\n\nThe ggplot2 spec:\n\n\nData: We create a staged dataset for plotting (plot_data). The different bar directions for workers/jobs is achieved by a slight hack – changing the polarity of counts by occupation depending on the summary type. Additionally in this staged dataset counts are further locally (borough-level) scaled. Note that we filter() on some selected boroughs.\n\nEncoding: Bars whose length (y=) varies according to count and categorical position (x=) according to occ_type, filled on high-level (professional / non-professional – is_prof) occupation type.\n\nMarks: geom_col() for bars.\n\nScale: scale_fill_manual() for associating occupation type, scale_x_continuous() for making sure workers/jobs bars use the same scale.\n\nFacets: facet_wrap() on borough (area_name).\n\nSetting: coord_flip() for bars that are oriented horizontally.\n\nAdding a geospatial arrangement – Figure 5.10 – is instructive at exposing the geography to these different categories of borough. More balanced boroughs to the east (Barking and Dagenham BaD) and west (Hillingdon Hil); worker-rich boroughs (left-pointing bars) with large proportions of professional workers in west and south west London boroughs (Wandsworth Wnd, Richmond Upon Thames RuT); job-rich boroughs (right-pointing bars) in central London (Westminster Wst, Camden Cmd).\n\n\n\n\nFigure 5.10: Workers and jobs in London boroughs by full occupation classes\n\n\n\n\nDifferent from the proportional-symbol maps, the spatial arrangement in Figure 5.10 is generated using ggplot2’s in-built faceting rather than a spatial polygon file. This can be understood when remembering that gridmap layouts created by points_to_grid() define row and column identifiers for each spatial unit. The only update to the bar chart spec is that we supply row and col identifiers to facet_grid(), with a slight hack on the row variable (-row) as gridmappr’s origin [min-row, min-col] is the bottom-left cell in the grid whereas for facet_grid() the origin is the top-left.\nThe code:\n\nplot_data |>\n  ggplot(aes(x = occ_type, y = count)) +\n  geom_col(aes(fill = is_prof), alpha = .5, width = 1) +\n  geom_hline(yintercept = 0, linewidth = .4, colour = \"#ffffff\") +\n  facet_grid(-row ~ col, scales = \"free\") +\n  scale_y_continuous(limits = c(-1, 1)) +\n  scale_fill_manual(values = c(\"#08306b\", \"#67000d\")) +\n  coord_flip()\n\n\n5.3.4 Analysing over the edges (flows between boroughs)\nTo study the geography of flows between boroughs, we can update our ggplot2 specification to generate a full OD map. In the example in Figure 5.11, there is a little more thinking around patterns in the data that we wish to explore, borrowing from the ideas introduced in the previous chapter.\nWe’ve identified differences in where professional jobs and workers are located in London and it would reasonable to expect that flows between boroughs also have an uneven geography. To explore this, we can set up a model that assumes that commuter flows between boroughs distribute uniformly across London. Of all commutes between London boroughs, 51% are to access professional jobs (global_prof). Under an assumption of uniformity, were we to randomly sample an OD (borough-borough) commute pair, we would expect to see this proportion when counting up the number of professional and non-professional occupation types present in that commute. For each origin-destination pair (OD), we therefore generate expected counts by multiplying the total number of commutes present in an OD pair by this global_prof, and from here signed residuals (resid) identifying whether there are greater or fewer professionals commuting that OD pair than would be expected. Note that these are like the signed chi-scores in the previous chapter in that rather than expressing differences in observed counts as a straight proportion of expected counts (dividing by expected counts), we apply a power transform to the denominator. This has the effect of also giving saliency to differences that are large in absolute terms. You could try varying this exponent (maybe between 0.5-1.0) to see its effect on residuals when encoded using colour in the OD map.\nFigure 5.11 is a D-OD map; the large reference cells are destination boroughs (workplaces) and the small cells origins (residences) from which workers travel to access jobs in the reference cell. From this we observe that job-rich boroughs in central London are associated more with professional occupations and draw professional commuters especially from ‘residential’ boroughs such as Wandsworth (Wnd), Hammersmith and Fulham (HaF). Note that the darker colours indicate that these job-rich boroughs also attract workers in large number from boroughs across London. By contrast, boroughs in outer London do not draw workers from other boroughs in such large number and the very dark blues in the reference cells suggest that, as might be expected, the labour market for non-professional jobs is more localised.\n\n\n\n\nFigure 5.11: Commutes between London boroughs: difference maps by occupation type assuming professional and non-professional distribute uniformly across London\n\n\n\n\nThe code:\n\nplot_data <- edges |> mutate(non_prof = commutes-is_prof) |>\n  rename(prof = is_prof) |>\n  ungroup() |>\n  mutate(global_prof = sum(prof) / sum(prof + non_prof)) |>\n  # Group by o_bor for an O-OD map.\n  group_by(d_bor) |>\n  mutate(\n    count = prof + non_prof,\n    obs = prof,\n    exp = (global_prof * count),\n    resid = (obs - exp) / (exp^.7)\n    ) |>\n  ungroup() |>\n  # Join on d_bor for an O-OD map.\n  left_join(grid |> select(area_name), by = c(\"o_bor\" = \"area_name\")) |>\n  mutate(\n    bor_label = if_else(o_bor == d_bor, d_bor, \"\"),\n    bor_focus = o_bor == d_bor\n  ) |>\n  st_as_sf()\n\nbbox_grid <- st_bbox(grid)\n\nplot <- plot_data |>\n  ggplot() +\n  geom_sf(aes(fill = resid), colour = \"#616161\", size = 0.15, alpha = 0.9) +\n  geom_sf(data = . %>% filter(bor_focus), fill = \"transparent\", colour = \"#373737\", size = 0.3) +\n  geom_text(\n    data = plot_data %>% filter(bor_focus),\n    aes(x = o_x, y = o_y, label = str_extract(o_bor, \"^.{1}\")),\n    colour = \"#252525\", alpha = 0.9, size = 2.1,\n    hjust = \"centre\", vjust = \"middle\"\n  ) +\n  geom_text(\n    data = plot_data %>% filter(bor_focus),\n    aes(x = bbox_grid$xmax, y = bbox_grid$ymin, label = abbreviate(o_bor, 3)),\n    colour = \"#252525\", alpha = 0.9, size = 3.5,\n    hjust = \"right\", vjust = \"bottom\"\n  ) +\n  coord_sf(crs = st_crs(plot_data), datum = NA) +\n  facet_grid(-d_row ~ d_col, shrink = FALSE) +\n  scale_fill_distiller(palette = \"RdBu\", direction = -1 )\n\nThe ggplot2 spec:\n\nData:\n\nCalculate the proportion of professional jobs in the dataset (global_prof).\nThen for each destination (workplace) borough calculate the expected number of commutes for any OD pair (flow), by multiplying the number of jobs contained in that flow by global_prof and express the difference between the actual number of professional jobs as rate (with power transform) ((obs-exp) / (exp^.7)).\nTake the staged dataset and join twice on the gridmap dataset.\nThen join the with the gridded polygon file (grid) on o_bor – in this OD map the origins are the small maps.\nFinally in the mutate() we generate a new variable identifying the borough in focus (bor_focus), destination in this case, and a text label variable for annotating plots on this (bor_label).\n\n\nEncoding:\n\nGridmap cells are coloured according to the calculated residuals (fill=resid).\nText labels for destination (focus) boroughs are drawn in the bottom-right corner of larger cells. Note that the coordinate space here is that from the gridmap dataset and so the x,y location of borough labels are derived from the bounding box object (bbox_grid), calculated during data staging. Single letter annotations are also positioned where small origin cells match the focus (destination) borough. These additional labels are positioned using the grid centroids o_x, o_y from plot_data.\n\n\nMarks: geom_sf() for drawing the small gridcell maps; geom_text() for drawing the labels.\nScale: scale_fill_distiller() for a diverging colour scheme using the ColorBrewer RdBu palette and made symmetrical on 0 by manually setting limits().\nFacets: facet_grid() for effecting the map-within-map layout.\n\nOnce the data staging and ggplot2 spec for the OD map is generated, it is very easy to adapt and extend the code to explore different assumptions and elements of the dataset. For example, the assumption of a uniform distribution across London in the relative number of commutes by occupation type is a flawed one since we know that there is some variation in the proportion of professional jobs available in each borough. In the City of London (CoL) 74% of jobs are in professional occupations whereas in Bexley (Bxl), Havering (Hvr) and Barking and Dagenham (BaD) that figure is c.30%. We can easily adapt the data staging code to instead generate local expectations for each destination borough by moving the assignment of global_prof into the destination borough group_by. The expectation is now that the relative number of professional commutes present in any OD pair should be proportionally equivalent to the number of professional jobs available at that OD pair’s reference, destination, borough. Colouring cells of the OD map according to this new quantity (Figure 5.12) exposes patterns that relate to London’s social geography. Higher than expected non-professional workers from more affordable boroughs to the east of London and into job-rich boroughs in central London and a reverse pattern for origin boroughs supplying higher than expected professional workers.\n\n\n\n\nFigure 5.12: Commutes between London boroughs: difference maps by occupation type assuming professional and non-professional distribute uniformly within boroughs"
  },
  {
    "objectID": "05-network.html#conclusion",
    "href": "05-network.html#conclusion",
    "title": "5  Networks",
    "section": "\n5.4 Conclusion",
    "text": "5.4 Conclusion\nNetwork data – data which describe relations between entities – are challenging to represent, work with and analyse. It is for this reason that visual approaches are often used in their analysis. A common pitfall in many network visualizations is that they simply re-present that complexity inherent to network data without exposing useful structure or insight into the phenomena being analysed. Through an analysis of 2011 Census travel-to-work data in London, the chapter demonstrated approaches to analysing and inferring structure in a category of network data common to geographers: spatial origin-destination data. Spatially-arranged node-link diagrams are highly intuitive and can support a kind of synoptic overview of a network, but were of limited success in representing detailed patterns in travel-to-work within and between London boroughs. Instead we used matrix-based views, including spatially arranged matrices or OD Maps. As ever, the appropriateness of either approach – node-link based or matrix-based representations – depends on data, analysis purpose and audience."
  },
  {
    "objectID": "06-model.html",
    "href": "06-model.html",
    "title": "6  Models",
    "section": "",
    "text": "By the end of this chapter you should gain the following knowledge and practical skills."
  },
  {
    "objectID": "06-model.html#introduction",
    "href": "06-model.html#introduction",
    "title": "6  Models",
    "section": "\n6.1 Introduction",
    "text": "6.1 Introduction\nSo far the analysis presented has been quite data-driven. Having described data in a consistent way (Chapter 2), visual analysis approaches have been applied to expose structure in datasets, informed by established visualization guidelines. Chapters 4 and 5 involved model building, but these were largely value-free models based on limited prior theory.\nThis chapter works through a dataset with a more explicit and theoretically-informed motivation. We will explore variation in voting behaviour in the UK’s 2016 referendum on leaving the EU. You might remember that whilst there was a slight majority for Leave (51.9%), the vote varied between different parts of the country. There were many theories and explanations offered around for why particular places voted the way they did, often related to the demographic composition of those areas. We will explore whether the sorts of compositional demographic factors discussed vary systematically with area-level Leave voting. Using regression frameworks, we will model the relative effect of each of these compositional factors in structuring variation in the vote and construct data graphics that allow these models and parameters to be considered and evaluated in detail.\n\n\n\n\n\n\nNote\n\n\n\nThis chapter assumes some basic familiarity with linear regression modelling. For a clear and detailed overview, with excellent social science examples, you may wish to try Bartholomew et al. (2008)."
  },
  {
    "objectID": "06-model.html#concepts",
    "href": "06-model.html#concepts",
    "title": "6  Models",
    "section": "\n6.2 Concepts",
    "text": "6.2 Concepts\n\n6.2.1 Quantifying and exploring variation\n\nIn Figure 6.1 is a map and bar chart displaying total variation in vote shares for Leave in Great Britain (GB), estimated at Parliamentary Constituency level (see Hanretty 2017). The values themselves are the difference in estimated vote shares from an expectation that the Leave vote by constituency (y_{i}), our outcome of interest, is uniformly distributed across the country and so equivalent to the overall vote share for Leave, which for constituencies is 51.9%. Although a slightly contrived formulation, we could express this as an intercept-only linear regression model, where the estimated slope (\\beta_{1}) is ‘turned off’ (takes the value 0) and the intercept (\\beta_{0}) is the GB average vote share for Leave (\\bar{y}):\n\\begin{align*}\n       y_{i}= \\beta_{0} + \\beta_{1} + \\varepsilon_{i}\n\\end{align*}\nSo we estimate the Leave vote in each constituency (y_{i}) as a function of:\n\n\n\\beta_{0}, the intercept, the GB average vote share (\\bar{y}) +\n\n\n\\beta_{1}=0, a negated slope, +\n\n\n\\varepsilon_{i}, a statistical error term capturing the difference between y_{i}, the observed Leave vote in a constituency, and the unobservable ‘true population’ value of the Leave vote in each constituency\n\nHow does this relate to the idea of characterising variation? The length and colour of each bar in Figure 6.1 is scaled according to model residuals: the difference between y_{i}, the observed value, and the expected value of the Leave vote under the uniform model. The sum of these bar lengths is therefore the total variance that we later try to account for by updating our regression model to generate new expected values using information on the demographic composition of constituencies.\nFigure 6.1 is similar to the maps that were published widely in press reports in the aftermath of the vote, and demonstrates that there is indeed substantial variation in Leave voting between different parts of the country. The intercept-only model, which assumes a uniform distribution of Leave across the country, consistently underestimates the vote in Scotland and most of London. Outside of this, constituencies voting in smaller proportions than would be expected for Leave are distributed more discretely in the country: the dark red dot with surrounding red area in the east of England is Cambridge and Cambridgeshire, constituencies in Bristol (south west), Manchester and Liverpool (north west), Brighton (south), are also reasonably strong red.\n\n\n\n\nFigure 6.1: Residuals from uniform model comparing constituency Leave vote to GB average\n\n\n\n\nWhen evaluating the effectiveness of modelled values, there are various checks that can be performed. An obvious one here is wether there is bias in the residuals – whether they have any underlying structure that suggests that they are grouped in a way not captured by the model. Given the motivation behind our analysis, it is no surprise that there is a geographic pattern to the residuals in Figure 6.1, but also the non-symmetrical shape of the ‘signed’ bars is instructive. There are more constituencies with positive values than negative – the Leave vote is underestimated by the uniform model for 57% of constituencies – and certain constituencies with extreme negative values – the strongest vote for Leave was Boston and Skegness (76%) but the strongest for Remain was Hackney North and Stoke Newington (80%).\n\n6.2.2 Quantifying and exploring co-variation\nMore interesting is whether the pattern of variation in Figure 6.1 is correlated with compositional factors that we think explain this variation; and also whether bias or structure in residuals exists even after accounting for these compositional factors.\nIn Table 6.1 is a list of candidate explanatory variables describing the demographic composition of constituencies. Each variable is expressed as a proportion of the constituency’s population. So the degree educated variable describes the proportion of residents in the constituency educated at least to degree-level. Comparison across these variables is challenging due to the fact that their ranges differ: the EU-born variable ranges from 0.6% to 17%; the white variable from 14% to 98%. There are also obvious ceilings that limit how successful explanatory variables are likely to be at discriminating variation. Common practice for addressing the range problem is to z-score transform the variables so that each value is expressed in standard deviation units from its variable’s mean.\n\n\n\n\n\nTable 6.1:  Breakdown of variable types \n \n Census variable \n    Constituency % \n  \n\npost-industrial / knowledge economy\n\n degree-educated \n    with degrees + \n  \n\n professional occupations \n    ns-sec manager/professional \n  \n\n younger adults \n    adults aged <44 \n  \n\n heavy industry \n    manufacturing and transport \n  \ndiversity/values/outcomes\n\n not good health \n    reported fair, bad, very bad \n  \n\n white \n    ethnicity white British/Irish \n  \n\n Christian \n    Christian \n  \n\n EU-born \n    EU-born (not UK) \n  \nmetropolitan / 'big city'\n\n own home \n    own home \n  \n\n no car \n    don't own a car \n  \n\n\n\n\n\nFigure 6.2 presents scatterplots from which the extent of linear association between these demographics and Leave voting in each constituency can be inferred. Each dot is a constituency, arranged on the x-axis according to the value of each candidate explanatory variable and the y-axis according to the share of Leave vote. The scatterplots are faceted by explanatory variable and ordered left-to-right and top-to-bottom according to correlation coefficient. The variable most heavily correlated with Leave voting is that measuring levels of degree education: as the share of a constituency’s population educated at least to degree-level increases, the share of Leave vote in that constituency decreases. An association in the same direction, but to a lesser extent, is observed for variables representing similar concepts: professional occupations, younger adults, EU-born, no-car and the reverse for Christian, not-good health and heavy industry.\n\n\n\n\nFigure 6.2: Scatterplots of constituency Leave vote against candidate explanatory variables\n\n\n\n\nIt is of course possible, and likely, that there is some some autocorrelation in the compositional characteristics of constituencies and Leave voting. Parallel coordinate plots are useful for visually exploring this multivariate correlation. Whereas in scatterplots, observations are represented as points located in x- and y- axes that are orthogonal, in a parallel coordinate plot axes are parallel and the values of an observation encoded via a line connecting the multiple parallel axes, as in Figure 6.3. Each of the thin lines is a constituency coloured according to whether the constituency voted majority Remain (red) or Leave (blue); a high Remain and Leave constituency is highlighted. The first variable encoded is the size of the Leave vote (z-score transformed) and variables are then ordered according to linear association with Leave. Note that we have reversed the polarity of variables such as degree-educated and professional so that we expect more Leave (blue lines) towards the right locations of the parallel axis. That the blue and red lines are reasonably separated suggests that there is autocorrelation in the demographics of Leave and Remain constituencies.\n\n\n\n\n\nFigure 6.3: Parallel coordinate plot of constituency Leave vote and candidate explanatory variables\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nAlthough parallel coordinate plots enable some aspects of association between multiple variables to be inferred, they have several deficiencies. Association can only really be directly inferred by comparing variables that are immediately adjacent in parallel space; the order of parallel variables can affect the visual appearance and patterns of plots; and relatedly, visual patterns of the plot that are salient may not always be important to the phenomena being investigated.\n\n\n\n\n6.2.3 Modelling for co-variation\nLinear regression provides a framework for systematically describing the associations implied by the scatterplots and parallel coordinate plot, and with respect to the constituency-level variation identified in Figure 6.1. Having seen this new data, the candidate demographic variables in Figure 6.2, we can derive expected values closer to those observed and so better account for constituency-level variation in Leave voting.\nTo express this in equation form, we update the uniform model such that Leave vote is a function of the candidate explanatory variables. For single-variable linear regression, we might select the proportion of residents educated at least to degree-level ({d_{i1}}):\n\\begin{align*}\n       y_{i}&= \\beta_{0} + \\beta_{1}{d_{i1}} + \\varepsilon_{i}  \\\\\n\\end{align*}\nSo we now estimate the Leave vote in each constituency (y_{i}) as a function of:\n\n\n\\beta_{0}, the intercept, the GB average vote share (\\bar{y}) +\n\n\n\\beta_{1}=\\beta_{1}{d_{i1}}, the slope, indicating in which direction and to what extent degree-educated is associated with Leave, +\n\n\n\\varepsilon_{i}, the difference between y_{i} (the observed value) and the unobservable ‘true population’ value of the Leave vote in that constituency (statistical error)\n\nThere are different algorithms that can be used to estimate these parameters. Most obvious is ordinary least squares (OLS), which aims to minimise the sum of the (squared) residuals between the observed Leave vote in a constituency, y_{i}, and that expected given the association with the degree-educated explanatory variable d_{i1}. Eyeballing the scatterplots in Figure 6.2 we can already infer many of the parameters estimated via OLS regression. Degree-educated is most consistently associated with Leave voting and, according to our linear regression model, explains 60% of the total variation (the summed bars in Figure 6.1) in constituency-level Leave voting. Also from the scatterplots, especially those associating Leave with degree-educated and professionals, there is a grouping of constituencies where the Leave vote is lower than we might expect given that constituency’s population characteristics – and this is reflected when inspecting the 1D distribution of residuals in histograms (not included here).\nIt is of course possible, and likely, that some of these variables account for different elements of the variation in the Leave vote than others. You will be aware that the linear regression model can be extended to include many explanatory variables:\n\\begin{align*}\n      y_{i}&= \\beta_{0} +\\beta_{1}x_{i1} + ... + \\beta_{k}x_{ik} + \\varepsilon_{i}  \\\\\n\\end{align*}\nSo this results in separate \\beta_{k} coefficients for separate explanatory variables. These coefficients can be interpreted as the degree of association between the explanatory variable k and the outcome variable, keeping all the other explanatory variables constant – or the distinct correlation between an explanatory variable k and the outcome variable, net of the other correlated variables.\nIn Figure 6.4 are regression coefficients (\\beta_{k}) from a multiple regression model with degree-educated, no car, white, heavy industry, EU-born and not good health selected as explanatory variables. Coefficients are reported as dots with estimates of uncertainty represented as lines encoding 95% confidence intervals.  Most variables’ coefficients are in the direction that would be expected given the associations in Figure 6.2. Net of variation in the other compositional factors, increased levels of degree education in a constituency has the effect of reducing the Leave vote. The two exceptions are EU-born and white: after controlling for variation in the other demographic variables, increased proportions of residents identifying as white reduces the Leave vote and increased proportions of residents that are EU-born increases the Leave vote. Since the confidence interval for white crosses zero, this coefficient is subject to much uncertainty; further exploration may allow us to identify whether these counter-intuitive effects are genuine or the result of a poorly-specified model.\n\n\n\n\nFigure 6.4: Outputs from multiple regression model of Leave vote by demographic composition of constituency\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nGiven the spirit of this book, you might have wondered about the reasonably abbreviated discussion of techniques for representing model outputs and their uncertainty estimates (via Confidence Intervals). More involved coverage of this is in Chapter 7.\n\n\n\n6.2.4 Evaluating model bias\nThe new expected values for Leave in derived from the multivariate model are much closer to the observed in the data; the model accounts for a reasonably large share (c.76%) of the variation in constituency-level Leave voting. However, our analysis becomes more interesting when we start to explore and characterise model bias: any underlying structure to the observations which are better or less well accounted for by the model.\nFor area-level regression models such as ours, it is usual for residuals to exhibit some spatial autocorrelation structure. For certain parts of a country, a model will overestimate an outcome given the relationship implied by associations between explanatory and outcome variables; for other parts, the outcome will be underestimated. This might occur due to:\n\n\nSpatial dependence in variable values over space. We know that the geography of GB is quite socially distinctive, so it is reasonable to expect, for example, the range in variables like heavy industry and white to be bounded to economic regions and metropolitan-versus-peripheral regional contexts.\n\nSpatial nonstationarity in processes over space. It is possible that associations between variables might be grouped over space – that the associations vary for different parts of the country. For example, high levels of EU-born migration might affect political attitudes, and thus area-level voting, differently in different parts of a country.\n\nWe can test for and characterise spatial autocorrelation by performing a graphical inference test, a map line-up (Beecham et al. 2017; Wickham et al. 2010) against a null hypothesis of complete spatial randomness (CSR) in residuals. Here a plot of real data, the true map of residuals, is hidden amongst a set of decoys, in this case maps with the residual values randomly permuted around constitutencies. If the real map can be correctly identified from the decoys, then this lends statistical credibility to the claim that the observed data are not consistent with the null of CSR. Graphical line-up tests have been used in various domains, also to test regression assumptions (Loy, Hofmann, and Cook 2017). The map line-up in Figure 6.5 demonstrates that there is very obviously spatial (and regional) autocorrelation in residuals, and therefore structure that our regression model misses.\n\n\n\n\nFigure 6.5: Map line-up of residuals in which the ‘real’ dataset is presented alongside 8 decoy plots generated by randomly permuting the observed residuals around constituencies.\n\n\n\n\nThere are different ways of updating our model according to this geographic context. We have talked about patterning in residuals as being spatial, with values varying smoothly and continuously depending on location. This might be the case, and spatial autocorrelation is present in almost all datasets. But given the phenomena we are studying, it also plausible that distinct contexts are linked to regions. The residuals in Figure 6.5 – the real being plot 5 – do seem to be grouped by regional boundaries, particularly Scotland looks categorically different. This suggests that geographic context might be usefully represented as a category rather than continuous variable (location in x,y). We will therefore update our model representing geographic context as a regional grouping and cover approaches both to modelling spatial dependence in values and spatial non-stationarity in processes.\n\n6.2.5 Geographic context as grouped nuisance term\nA common approach to treating geographic dependence in the values of variables is to model geographic context as a Fixed Effect (FE). A dummy variable is created for each group (region in our case), and every region receives a constant. Any group-level sources of variation in the outcome are collapsed into the FE variable, which means that regression coefficients are not complicated by this more messy variation – they now capture the association between demographics and Leave after adjusting for systematic differences in the Leave vote due to region. So, for example, we know that Scotland is politically different from the rest of GB and that this appears to drag down the observed Leave vote for its constituencies, and so the constant term on region adjusts for this and prevents the estimated regression coefficients (inferred associations between variables) from being affected. The constant term also the ‘base level’ of the outcome for each grouping variable to be estimated – e.g. net of demographic composition, the expected Leave vote in a particular region.\nThe linear regression model an be easily extended with the FE term ({\\gamma_{j}}). For a single variable model:\n\\begin{align*}\n       y_{i}&= {\\gamma_{j}}  + \\beta_{1}x_{i1} + \\varepsilon_{i}  \\\\\n\\end{align*}\nSo we now estimate the Leave vote in each constituency (y_{i}) as a function of:\n\n\n{\\gamma_{j}}, a constant term similar to an intercept for region j, +\n\n\n\\beta_{1}=\\beta_{1}x_{i1}, the slope, indicating in which direction and to what extent some explanatory variable measured at constituency i is associated with Leave, +\n\n\n\\varepsilon_{i}, the difference between y_{i} (the observed value) at constituency i and the unobservable true population value of the Leave vote in that constituency (statistical error)\n\nPresented in Figure 6.6 are updated regression coefficients for a multivariate model fit with a FE on region. In the left panel are the FE constants. Together these capture the variance in Leave vote between regions after accounting for demographic composition. These coefficients have useful properties: they are the estimated size of the Leave vote for a constituency in a region net of demographic composition. London is in interesting here. When initially analysing variation in the vote, constituencies in Scotland and London were distinctive in voting in much smaller proportions than the rest of the country for Leave. Given the associations we observe with Leave voting and demographic composition, however, if we were to randomly sample two constituencies that contain the same demographic characteristics, one in London and one in another region (say North West), on average we would expect Leave from the constituency in London to be higher (~60%) than that sampled from North West (~51%). A separate, and more anticipated pattern is that Scotland would have a lower Leave vote (~38%) – that is, net of demographics there is some additional context in Scotland that means Leave is lower than in other regions.\nIn the right panel are the regression coefficients net of this between-region variation. In the previous model, the white variable was shown counterintuitively to have a slight negative association with Leave (although there was high uncertainty here). Now the white variable has a direction of effect that conforms to expectation – net of variation in other demographics increased proportions of white residents is associated with increased Leave voting. For the other variable with a counterintuitive effect – EU born – the coefficient still suggests a positive association with Leave.\n\n\n\n\nFigure 6.6: Output from multiple regression model of Leave vote by demographic composition of constituency with FE on region\n\n\n\n\nFigure 6.7 is a map line-up of the residuals from this updated model. Certainly adding fixed effect on region has addressed systematic over- and under- estimation of the vote between regions. The residuals nevertheless exhibit obvious spatial autocorrelation, plot 7 being the real data. Further investigative analysis, for example of the constituencies for which Leave is particularly over- and under- represented, may be instructive.\n\n\n\n\n\nFigure 6.7: Map line-up of residuals from model with fixed effect on region. The ‘real’ dataset is presented alongside 8 decoy plots generated by randomly permuting the observed residuals around constituencies.\n\n\n\n\n\n6.2.6 Geographic context as grouped effects\nThe benefit of the FE adjustment is that it provides coefficient estimates that are not affected by between-region variation. The FE constants themselves also allow group differences to be quantified net of differences in demographics. However, they simply identify the fact that this variation exists – they do not permit non-stationarity in process. It is conceivable that the strength and direction of association between Leave and the candidate demographic variables may vary between regions. For example, that increased levels of EU-born (non-UK) residents might affect area-level voting differently in certain regions than others.\nRather than simply allowing a constant term to vary, we can update the linear regression model with an interaction term ({\\beta_{1j}}{x_{i1}}) that allows the coefficient estimates to vary depending on region. This means we get a separate constant term and coefficient estimate of the effect of each variable on Leave for every region.\n\\begin{align*}\n       y_{i}&= {\\gamma_{j}} + {\\beta_{1j}}x_{i1} + \\varepsilon_{i}  \\\\\n\\end{align*}\n\n\n{\\gamma_{j}}, a constant term similar to an intercept for region j, +\n\n\n{\\beta_{1j}}x_{i1}, the region-specific slope, indicating in which direction and to what extent some demographic variable at constituency i and in region j is associated with Leave, +\n\n\n\\varepsilon_{i}, the difference between y_{i} (the observed value) at constituency i and the unobservable true ‘population’ value of the Leave vote in that constituency (statistical error)\n\nIn Figure 6.8 are region-specific coefficients derived from a multivariate model with an interaction term introduced on region. In each region, degree-educated has a negative coefficient and with reasonably tight uncertainty estimates, or at least CIs that do not cross 0. The other variables are subject to more uncertainty. The no-car variable is also negatively associated with Leave, a variable we thought may separate metropolitan versus peripheral contexts, but the strength of negative association, after controlling for variation in other demographic factors, does vary by region. The heavy industry variable, previously identified as being strongly associated with Leave, has a clear positive association only for London and to a much lesser extent for North West and Wales (small coefficients). The EU-born variable is again the least consistent as it flips between positive and negative association when analysed at the regional-level: after controlling for variation in other demographic characteristics it is positively associated with Leave for North West, Scotland, South West, but negatively associated with Leave for the North East (though with coefficients that are subject to much variation).\n\n\n\n\nFigure 6.8: Output from multiple regression model of Leave vote by demographic composition of constituency with FE and interaction on region.\n\n\n\n\n\n6.2.7 Estimate volatility and alternative modelling structures\nOur treatment of regression frameworks has in this chapter been reasonably breezy. Introducing FE and interaction terms without adding data, for example, reduces statistical power as data are heavily partitioned. This risks overfitting as our coefficients may begin to fit noise rather than true effects. Given the fact that our data are hierarchically structured (constituencies sit within regions) hierarchical multi-level modelling may be more appropriate to modelling this sort of regional grouping. Multi-level modelling uses partial pooling, borrowing data, to make estimated coefficients more conservative, less locally biased, where there are comparatively few observations in particular groupings (see Gelman and Hill 2006). There are also many ways in which associations between values can be modelled continuously over space. In the case of geographically weighted regression (GWR) (Brunsdon, Fortheringham, and Charlton 2002), Local regression coefficients, for each spatial unit. If applied to our dataset, separate regression coefficients for each constituency are estimated that take into account observed values for Leave and the demographic variables in nearby constituencies. GW-statistics enables spatial non-stationarity in process to be flexibly explored and characterised. As GWR involves generating many hundreds of parameter estimates, visual analysis are often used in its interpretation (e.g. Dykes and Brunsdon 2007)."
  },
  {
    "objectID": "06-model.html#techniques",
    "href": "06-model.html#techniques",
    "title": "6  Models",
    "section": "\n6.3 Techniques",
    "text": "6.3 Techniques"
  },
  {
    "objectID": "07-uncertainty.html",
    "href": "07-uncertainty.html",
    "title": "7  Uncertainty",
    "section": "",
    "text": "By the end of this chapter you should gain the following knowledge and practical skills."
  },
  {
    "objectID": "07-uncertainty.html#concepts",
    "href": "07-uncertainty.html#concepts",
    "title": "7  Uncertainty",
    "section": "7.1 Concepts",
    "text": "7.1 Concepts"
  },
  {
    "objectID": "07-uncertainty.html#techniques",
    "href": "07-uncertainty.html#techniques",
    "title": "7  Uncertainty",
    "section": "7.2 Techniques",
    "text": "7.2 Techniques"
  },
  {
    "objectID": "08-storytelling.html",
    "href": "08-storytelling.html",
    "title": "8  Data Storytelling",
    "section": "",
    "text": "By the end of this chapter you should gain the following knowledge and practical skills."
  },
  {
    "objectID": "08-storytelling.html#concepts",
    "href": "08-storytelling.html#concepts",
    "title": "8  Data Storytelling",
    "section": "8.1 Concepts",
    "text": "8.1 Concepts"
  },
  {
    "objectID": "08-storytelling.html#techniques",
    "href": "08-storytelling.html#techniques",
    "title": "8  Data Storytelling",
    "section": "8.2 Techniques",
    "text": "8.2 Techniques"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Bartholomew, David J., Fiona Steele, J Galbraith, and Irini Moustaki.\n2008. Analysis of Multivariate Social Science Data. London, UK:\nCRC Press.\n\n\nBeecham, R. 2020. “Using Position, Angle and Thickness to Expose\nthe Shifting Geographies of the 2019 UK\nGeneral Election.” Environment and\nPlanning A: Economy and Space 52 (5): 833–36.\n\n\nBeecham, R., J. Dykes, L. Hama, and N. Lomax. 2021. “On the Use of\n‘Glyphmaps’ for Analysing the Scale and Temporal Spread of\nCOVID-19 Reported Cases.” ISPRS International Journal of\nGeo-Information 10 (4). https://doi.org/10.3390/ijgi10040213.\n\n\nBeecham, R., J. Dykes, W. Meulemans, A. Slingsby, C. Turkay, and J.\nWood. 2017. “Map Line-Ups: Effects of Spatial Structure on\nGraphical Inference.” IEEE Transactions on Visualization\n& Computer Graphics 23 (1): 391–400.\n\n\nBeecham, R., and R. Lovelace. 2022. “A Framework for Inserting\nVisually-Supported Inferences into Geographical Analysis Workflow:\nApplication to Road Safety Research.” https://doi.org/10.1111/gean.12338.\n\n\nBeecham, R., and A. Slingsby. 2019. “Characterising Labour Market\nSelf-Containment in London with Geographically Arranged\nSmall Multiples.” Environment and Planning A: Economy and\nSpace 51 (6): 1217–24.\n\n\nBeecham, R., Y. and Yang, C. Tait, and R. Lovelace. 2023.\n“Connected Bikeability in London: Which Localities Are Better\nConnected by Bike and Does This Matter?” Environment and\nPlanning B: Urban Analytics and City Science 0 (0):\n23998083231165122. https://doi.org/10.1177/23998083231165122.\n\n\nBrunsdon, C., and A. Comber. 2020. “Opening Practice: Supporting\nReproducibility and Critical Spatial Data Science.” Journal\nof Geographical Systems.\n\n\nBrunsdon, C., M. Fortheringham, and M. Charlton. 2002.\n“Geographically Weighted Summary Statistics: A Framework for\nLocalised Exploratory Data Analysis.” Computers, Environment\nand Urban Systems 26: 501–24.\n\n\nButler, D., and S. Van Beek. 1990. “Why Not Swing?\nMeasuring Electoral Change.” Political Science\n& Politics 23 (2): 178–84.\n\n\nCleveland, W., and R. McGill. 1984. “Graphical\nPerception: Theory,\nExperimentation, and Application to the\nDevelopment of Graphical\nMethods.” Journal of the American Statistical\nAssociation 79 (387): 531–54.\n\n\nCorrell, M., and J. Heer. 2017. “Regression by Eye:\nEstimating Trends in Bivariate\nVisualizations.” In ACM\nHuman Factors in Computing\nSystems (CHI). http://idl.cs.washington.edu/papers/regression-by-eye.\n\n\nDonoho, D. 2017. “50 Years of Data\nScience.” Journal of Computational and Graphical\nStatistics 26 (6): 745–66. https://doi.org/10.1080/10618600.2017.1384734.\n\n\nDykes, J., and C. Brunsdon. 2007. “Geographically Weighted\nVisualization: Interactive Graphics for Scale-Varying\nExploratory Analysis.” IEEE Transactions on Visualization and\nComputer Graphics 13 (6): 1161–68.\n\n\nFriendly, M. 1992. “Mosaic Displays for Loglinear Models.”\nIn ASA, Proceedings of the\nStatistical Graphics\nSection, 61–68.\n\n\nGelman, A. 2004. “Exploratory Data\nAnalysis for Complex\nModels.” Journal of Computational and Graphical\nStatistics 13 (4): 755–79. https://doi.org/10.1198/106186004X11435.\n\n\nGelman, A., and J Hill. 2006. Data Analysis Using Regression and\nMultilevel/Hierarchical Models. Cambridge University Press.\n\n\nGleicher, Albers, M., and J. Roberts. 2011. “Visual Comparison for\nInformation Visualization. Information\nVisualization.” Information Visualization\n10 (4): 289–309.\n\n\nHanretty, C. 2017. “Areal Interpolation and the UK’s\nReferendum on EU Membership.” Journal of\nElections, Public Opinion and Parties 37 (4): 466–83.\n\n\nHarrison, L., F. Yang, S. Franconeri, and R. Chang. 2014. “Ranking\nVisualizations of Correlation Using Weber’s\nLaw.” IEEE Conference on Information\nVisualization (InfoVis) 20 (12): 1943–52.\n\n\nHealy, K. 2018. Data Visualization: A\nPractical Introduction. Princeton:\nPrinceton University Press.\n\n\nHeer, J., and M. Bostock. 2010. “Crowdsourcing\nGraphical Perception: Using\nMechanical Turk to Assess\nVisualization Design.” In\nACM Human Factors in\nComputing Systems, 203–12. https://doi.org/10.1145/1753326.1753357.\n\n\nKay, M., and J. Heer. 2016. “Beyond Weber’s\nLaw: A Second Look\nat Ranking Visualizations of\nCorrelation.” IEEE Trans. Visualization &\nComp. Graphics (InfoVis) 22 (1): 469–78.\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.\n\n\nLovelace, R., J. Nowosad, and J. Muenchow. 2019. Geocomputation with\nR. London, UK: CRC Press.\n\n\nLoy, A., H. Hofmann, and D. Cook. 2017. “Model Choice\nand Diagnostics for Linear\nMixed-Effects Models\nUsing Statistics on Street\nCorners.” Journal of Computational and Graphical\nStatistics 26 (3): 478–92. https://doi.org/10.1080/10618600.2017.1330207.\n\n\nMcGill, Tukey, R., and W. A. Larsen. 1978. “Variations of Box\nPlots.” The American Statistician 32: 12–16.\n\n\nMunzner, T. 2014. Visualization Analysis and\nDesign. AK Peters\nVisualization Series. Boca Raton, FL: CRC\nPress.\n\n\nOpen Science Collaboration. 2015. “Estimating the Reproducibility\nof Psychological Science.” Science 349 (6251): aac4716.\nhttps://doi.org/10.1126/science.aac4716.\n\n\nRensink, R., and G. Baldridge. 2010. “The Perception of\nCorrelation in Scatterplots.” Computer Graphics Forum 29\n(3): 1203–10.\n\n\nStevens, S. 1946. “On the Theory of Scales of Measurement.”\nScience 103 (2684): 677–80.\n\n\nTufte, E. 1983. The Visual Display of\nQuantitative Information. Cheshire, CT:\nGraphics Press.\n\n\nTukey, J. W. 1977. Exploratory Data\nAnalysis. Reading, MA, USA: Addison-Wesley.\n\n\nTukey, John W. 1962. “The Future of Data\nAnalysis.” The Annals of Mathematical Statistics\n33 (1): 1–67. https://doi.org/10.1214/aoms/1177704711.\n\n\nVidal Tortosa, Eugeni, Robin Lovelace, Eva Heinen, and Richard P. Mann.\n2021. “Socioeconomic Inequalities in Cycling Safety: An Analysis\nof Cycling Injury Risk by Residential Deprivation Level in\nEngland.” Journal of Transport & Health 23: 101291.\nhttps://doi.org/https://doi.org/10.1016/j.jth.2021.101291.\n\n\nVisalingam, M. 1981. “The Signed Chi-Score Measure for the\nClassification and Mapping of Plychotomous Data.” The\nCartographic Journal 18 (1): 32–43.\n\n\nWhite, T. 2017. “Symbolization and the Visual\nVariables.” In He Geographic\nInformation Science &\nTechnology Body of\nKnowledge, edited by John P. Wilson.\n\n\nWickham, H. 2010. “A Layered Grammar of Graphics.”\nJournal of Computational and Graphical Statistics 19 (1): 3–28.\n\n\n———. 2014. “Tidy Data.” Journal of\nStatistical Software 59 (10): 1–23.\n\n\nWickham, H., D. Cook, H. Hofmann, and A. Buja. 2010. “Graphical\nInference for Infovis.” IEEE Transactions on Visualization\nand Computer Graphics (Proc. InfoVis ’10) 16 (6): 973–79.\n\n\nWickham, H., and G. Grolemund. 2017. R for Data\nScience: Import, Tidy,\nTransform, Visualize, and Model\nData. Sebastopol, California: O’Reilly Media.\n\n\nWickham, H., D. Navarro, and T. Lin Pedersen. 2020. Ggplot2:\nElegant Graphics for Data Analysis. Springer.\n\n\nWilkinson, L. 1999. The Grammar of\nGraphics. New York: Springer.\n\n\nWood, J., J. Dykes, and A. Slingsby. 2010. “Visualisation of\nOrigins, Destinations and Flows\nwith OD Maps.” The Cartographic\nJournal 47 (2): 117–29.\n\n\nWood, J., A. Slingsby, and J. Dykes. 2011. “Visualizing the\nDynamics of London’s\nBicycle-Hire Scheme.”\nCartographica: The International Journal for Geographic Information\nand Geovisualization, no. 4: 239–51.\n\n\nYang, Y., R. Beecham, A. Heppenstall, A. Turner, and A. Comber. 2022.\n“Understanding the Impacts of Public Transit Disruptions on\nBikeshare Schemes and Cycling Behaviours Using Spatiotemporal and\nGraph-Based Analysis: A Case Study of Four\nLondon Tube Strikes.” Journal of\nTransport Geography 98: 103255. https://doi.org/https://doi.org/10.1016/j.jtrangeo.2021.103255."
  }
]