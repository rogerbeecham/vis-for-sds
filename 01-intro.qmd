# Introduction {#sec-introduction}

```{r}
#| echo: false
#| eval: true
#| message: false
#| warning: false
library(knitr)
library(kableExtra)
```

By the end of this chapter you should gain the following knowledge and practical skills.

::: {.callout-note icon="false"}
## Knowledge outcomes

-   [x] Appreciate the motivation for this book: why visualization, why R and why `ggplot2`.
-   [x] Recognise the characteristics of reproducible research and the role of *projects* and *computational notebooks* for curating modern data analysis products.
:::



::: {.callout-note icon="false"}
## Skills outcomes

-   [x] Open `R` using the RStudio Integrated Developer Environment (IDE).
-   [x] Install and enable `R` packages and query package documentation.
-   [x] Perform basic calculations via the R Console.
-   [x] Render Quarto files.
-   [x] Create `R` Projects.
-   [x] Read-in datasets from external resources as objects.
:::

## Introduction

This chapter introduces the *what*, *why* and *how* of the book. An argument is presented for the use of visual approaches in modern data analysis, especially social data science analysis, and the key technologies and analysis frameworks for the book are introduced: computational notebooks executed in Quarto. The technical component consolidates on any prior knowledge of R and Quarto as well as demonstrates how to organise data science analyses as RStudio Projects.

## Concepts

### Why vis4sds?

It is now taken-for-granted that new data, new technology and new ways of doing science have transformed how we approach the world's problems. Evidence for this can be seen in the response to the Covid-19 pandemic. Enter [Covid19 github](https://www.google.com/search?q=covid-19+github&rlz=1C5CHFA_enGB632GB632&oq=covid-19+github&aqs=chrome..69i57j69i60l3.5575j0j1&sourceid=chrome&ie=UTF-8) into a search and you'll be confronted with hundreds of repositories demonstrating how various data related to the pandemic can be collected, processed and analysed. *Data Science* (hereafter data science) is a catch-all term used to capture this shift.

The definition has been somewhat stretched over the years, but data science has its origins in the work of John Tukey's *The Future of Data Analysis* [-@tukey_future_1962]. Drawing on this, and a survey of more recent work, @donoho_50_2017 identifies six key facets that a data science discipline might encompass[^01-intro-1]:

<!-- [^01-intro-1]: For an excellent precis and interpretation of this for geographers and social scientists, see @arribas_geography_2018. -->

1.  data gathering, preparation and exploration;
2.  data representation and transformation;
3.  computing with data;
4.  data visualization and presentation;
5.  data modelling;
6.  and a more introspective "science about data science"

Each is covered to varying degrees within the book, although *data visualization and presentation* gets a special status. Rather than a single and self-contained facet of data science process -- something that happens after data gathering, preparation, exploration, but before modelling -- the book demonstrates how data visualization is intrinsic to, or at least should inform, every facet of data science work.

This special status is justified when considering the circumstances under which *Social Data Science* (hereafter social data science) projects operate. Often new datasets are repurposed for social sciences research for the first time; contain complex structure and relations that cannot be easily captured and modelled using conventional statistics; and, as a consequence, the types of questions asked and techniques deployed to answer them cannot be easily specified in advance. Through the examples in the book, we will demonstrate how visual approaches can capture complex, multivariate structure (chapters [-@sec-visual], [-@sec-explore], [-@sec-network]), provoke critical thinking around data transformation and modelling (chapters [-@sec-explore], [-@sec-network] and [-@sec-model]) and communicate observed patterns with integrity (chapters [-@sec-uncertainty] and [-@sec-storytelling]).


 <!-- Numeric summaries that simplify patterns are extremely useful and Statistics has at its disposal an array of tools for helping to guard against making false claims from datasets -- a theme that we will return to in chapters [-@sec-model] and [-@sec-uncertainty] when we think critically about the use of visual approaches for building and evaluating model outputs. There remain, though, certain classes of relation and context that cannot be easily captured through statistics alone.

[^01-intro-2]: Checkout @matejka_same_2017's Same Stats, Different Graphs paper for a fun take on this.

Geographic context, for example, is undoubtedly challenging to capture numerically. Many of the early examples of data visualization have been of spatial phenomena and generated by Geographers [see @friendly_brief_2007]. We can also make a special case for the use of visual approaches in Social Data Science (SDS) applications due to their exploratory nature. Often datasets are repurposed for social sciences research for the first time; contain complex structure and relations that cannot be easily captured by statistical summaries alone; and so the types of questions to be asked and the techniques deployed to answer them cannot be easily specified in advance. We will experience this through the book as we explore ([Chapter -@sec-exploratory] and [-@sec-networks]), model under uncertainty ([Chapter -@sec-models] and [-@sec-uncertainty]) and communicate ([Chapter -@sec-storytelling]) with various social science datasets.  -->


<!-- The most commonly-cited case for visualization is [Anscombe's quartet](https://en.wikipedia.org/wiki/Anscombe%27s_quartet). This consists of four datasets, each containing eleven observations and two variables for each observation. The data are synthetic, but let's say that they are the weight and height of independent samples taken from a population of postgraduate students studying Social Data Science. Presented with a new dataset it makes sense to compute some summaries, and doing so we observe that the data appear identical -- they contain the same means, variances and strong positive correlation coefficient (@fig-anscombe-data). This seems appropriate since the data are measuring individuals' weight and height. Although there may be some variation, we'd expect taller students to be heavier. Since the summary statistics are identical, we might reasonably expect that each dataset is a sample drawn from the same population of (Data Science) students. -->

<!-- In her book *Visualization Analysis and Design*, Tamara @munzner_visualization_2014 considers how humans and computers interface in the decision-making process. She makes the point that data visualization is ultimately about connecting people with data in order to make decisions -- or to install humans in a 'decision-making-loop'. There are very occasionally decision-making loops that are entirely computational and where an automated solution can be rolled out. Most situations, however, require some form of human intervention.

The most commonly-cited example demonstrating problems with relying solely on computation, and so for the use of visualization, is -->

<!-- ```{r}
#| eval: true
#| echo: false
#| include: false
#| label: fig-anscombe-data
#| fig-cap: "Data from Anscombe's quartet"
knitr::include_graphics("/figs/anscombe_data.png", error = FALSE)
```

 ![Data from Anscombe's quartet](figs/01/anscombe_data.svg){#fig-anscombe-data width=95% fig-align="center"}


Laying out the data in a meaningful way, horizontally according to *weight* ($x$) and vertically according to the *height* ($y$) to form a scatterplot, we quickly see that whilst these data contain the same high-level statistical properties they are very different. Only `dataset #1` now looks plausible if it were truly a sample of weights and heights drawn from a population of students.

```{r}
#| eval: true
#| echo: false
#| include: false
#| label: fig-anscombe-plot
#| fig-cap: "Plots of Anscombe's quartet"
knitr::include_graphics("/figs/01/anscombe_plot.svg", error = FALSE)
```

 ![Plots of Anscombe's quartet](figs/01/anscombe_plot.svg){#fig-anscombe-plot width=100% fig-align="center"}



Anscombe's is a deliberate example[^01-intro-2], but one can imagine many cases where important structure is missed or incorrect assumptions are made without data being subject to visual examination. The consequences are poorly specified models and potentially faulty claims. This is not to undermine the importance of numerical analysis. Numeric summaries that simplify patterns are extremely useful and Statistics has at its disposal an array of tools for helping to guard against making false claims from datasets -- a theme that we will return to in chapters [-@sec-model] and [-@sec-uncertainty] when we think critically about the use of visual approaches for building and evaluating model outputs. There remain, though, certain classes of relation and context that cannot be easily captured through statistics alone.

[^01-intro-2]: Checkout @matejka_same_2017's Same Stats, Different Graphs paper for a fun take on this.

Geographic context, for example, is undoubtedly challenging to capture numerically. Many of the early examples of data visualization have been of spatial phenomena and generated by Geographers [see @friendly_brief_2007]. We can also make a special case for the use of visual approaches in Social Data Science (SDS) applications due to their exploratory nature. Often datasets are repurposed for social sciences research for the first time; contain complex structure and relations that cannot be easily captured by statistical summaries alone; and so the types of questions to be asked and the techniques deployed to answer them cannot be easily specified in advance. We will experience this through the book as we explore ([Chapter -@sec-exploratory] and [-@sec-networks]), model under uncertainty ([Chapter -@sec-models] and [-@sec-uncertainty]) and communicate ([Chapter -@sec-storytelling]) with various social science datasets. -->

<!-- ::: {.callout-tip icon="false"}
## Task

Watch [Jo Wood's TEDx](https://www.youtube.com/embed/FaRBUnO5PZI) talk demonstrating how visual techniques can be used to analyse urban travel behaviours. In the video Jo argues that bikeshare schemes can help democratise cycling, but also for their potential contributions to research -- he briefly contrasts new, passively collected data sets with more traditional actively collected data for analysing how people move around cities. A compelling case is then made for the use of visualization to support analysis of these new forms of behavioural data.
::: -->

### What vis4sds?

The chapters of this book blend both theory and practical coding activity to cover the fundamentals of visual data analysis. As the chapters progress, data processing and analysis code is applied to datasets from the Political Science, Urban and Transport Planning and Health domains. So the examples in the book demonstrate how visual approaches can be used to generate and evaluate real findings and knowledge.

To do this, a reasonably broad set of data processing and analysis procedures is covered. As well as developing expertise around designing data-rich, visually compelling graphics, some tedious aspects of data processing and wrangling are required. Additionally, to learn how to make and communicate claims under uncertainty with data graphics, techniques for estimation and modelling from Statistics are needed. In short, @donoho_50_2017's six key facets of a data science discipline:

1.  data gathering, preparation, and exploration (Chapters [-@sec-data], [-@sec-visual], [-@sec-explore]);
2.  data representation and transformation (Chapters [-@sec-data], [-@sec-visual]);
3.  computing with data (Chapter [-@sec-data], All chapters);
4.  data visualization and presentation (All chapters);
5.  data modelling (Chapters [-@sec-explore], [-@sec-model], [-@sec-uncertainty]);
6.  and the "science about data science" (All chapters)

There is already an impressive set of open resources practically introducing how to do modern Data Science [@wickham_r_2017], Visualization [@healy_data_2018] and Geographic Analysis [@lovelace_geocomputation_2019]. What makes this book different is the emphasis on *doing* applied data science throughout -- we will be identifying and diagnosing problems when gathering data, discovering patterns (some may even be spurious) as we do exploratory analysis, and attempt to make claims under uncertainty as we generate models from observed patterns.

### How vis4sds?

#### R for modern data analysis

All data collection, analysis and reporting activity will be completed using the open source statistical programming environment [R](https://www.r-project.org/). There are many benefits that come from being fully open-source, with a critical mass of users. Firstly, there is an array of online fora, tutorials and code examples from which to learn. Second, with such a large community, there are numerous expert R users who themselves contribute by developing *packages* that extend its use.

Of particular importance is the [`tidyverse`](http://www.tidyverse.org). This is a set of packages for doing data science authored by the software development team at [Posit](https://posit.co/). `tidyverse` packages share a principled underlying philosophy, syntax and documentation. Contained within the `tidyverse` is its data visualization package, [`ggplot2`](https://ggplot2.tidyverse.org/). This package predates the `tidyverse` and is one of the most widely-used toolkits for generating data graphics. As with other visualization toolkits it is inspired by @wilkinson_grammar_1999's [The Grammar of Graphics](https://www.springer.com/gp/book/9780387245447), the `gg` in `ggplot2` stands for *Grammar of Graphics*. We will cover some of the design principles behind `ggplot2` (and `tidyverse`) in [Chapter -@sec-visual].

#### Quarto for reproducible research

<!-- > Reproducible research is the idea that data analyses, and more generally, scientific claims, are published with their data and software code so that others may verify the findings and build upon them.
>
> Roger Peng, Jeff Leek and Brian Caffo -->

In recent years there has been much introspection into how science works -- into how statistical claims are made from reasoning over evidence. This came on the back of, amongst other things, a high profile paper published in Science [@osc_estimating_2015], which found that of 100 contemporary peer-reviewed empirical papers in Psychology, the findings of only 39 could be replicated. The upshot is that researchers must now endeavour to make their work transparent, such that "*all* aspects of the answer generated by any given analysis \[can\] be tested" [@brunsdon_opening_2020].

A reproducible research project should be accompanied with *code* and *data* that:

-   allows tables and figures presented in research outputs to be regenerated
-   does what it claims (the code works)
-   can be justified and explained through proper documentation

If these goals are met, then it may be possible for others to use the code on new and different data to study whether the findings reported in one project might be consistent with another. Or alternatively to use the same data, but update the code to extend the original analysis. This model -- generate findings, explore replicability in new contexts and re-analysis -- is how knowledge development has always worked. However, to achieve this the data and procedures on which findings are generated must be made open and transparent.

In this setting proprietary data analysis software that support point-and-click interaction, used widely in the social sciences, are problematic. First, these software are often underpinned by code for implementing statistical procedures that is closed. It is not possible, and therefore less common, for the researcher to fully interrogate into the underlying processes that are being implemented and the results need to be taken more or less on faith. Second, replicating and updating anayses in light of new data is chellenging. It would be tedious to make notes describing all interactions performed when working with a dataset via a point-and-click- interface. As a declarative programming environment, it is very easy to provide such a provenance trail in R (with the `tidyverse`) since this necessarily exists in the analysis scripts. But also, the [Integrated Development Environments](https://en.wikipedia.org/wiki/Integrated_development_environment) (IDEs) through which R is accessed provide notebook environments that allow users to curate reproducible computational documents that blend *input code*, *explanatory prose* and *outputs*. Through the technical elements, we will prepare these sorts of notebooks using [Quarto](https://quarto.org/).

## Techniques

Readers of this book might already have some familiarity with R and the [RStudio](https://www.rstudio.com/) IDE. If not, then this section is designed to quickly acclimatise readers with R and RStudio and to briefly introduce Quarto, R scripts and RStudio Projects. The accompanying template file, [01-template.qmd]() can be downloaded from the book's companion website.

### R and RStudio

-   Install the latest version of [R](https://cloud.r-project.org/). Note that there are installations for [Windows](https://cloud.r-project.org/bin/windows/), [macOS](https://cloud.r-project.org/) and [Linux](https://cloud.r-project.org/). Run the installation from the file you downloaded (an `.exe` or `.pkg` extension).
-   Install the latest version of [RStudio Desktop](https://rstudio.com/products/rstudio/download/#download). Note again that there are separate installations depending on operating system -- for Windows an `.exe` extension, macOS a `.dmg` extension.
-   Once installed, open the RStudio IDE.
-   Open an R Script by clicking `File` \> `New File` \> `R Script` .

```{r}
#| echo: false
#| label: fig-rstudio
#| fig-cap: "The RStudio IDE"
knitr::include_graphics("figs/01/rstudio_annotate.png", error = FALSE)
```

<!-- ![The RStudio IDE](figs/01/rstudio_annotate.png){#fig-rstudio width=100% fig-align="center"} -->

You should see a set of windows roughly similar to those in @fig-rstudio. The top left pane is used either as a Code Editor (the tab named `Untitled1`) or Data Viewer. This is where you'll write, organise and comment R code for execution or inspect datasets as a spreadsheet representation. Below this in the bottom left pane is the R Console, in which you write and execute commands directly. To the top right is a pane with the tabs Environment and History. This displays all objects -- data and plot items, calculated functions -- stored in-memory during an R session. In the bottom right is a pane for navigating through project directories, displaying plots, details of installed and loaded packages and documentation on their functions.

### Compute in the console

You will write and execute almost all code from the code editor pane. To start though let's use `R` as a calculator by typing some commands into the Console. You'll create an object (`x`) and assign it a value using the assignment operator (`<-`), then perform some simple statistical calculations using functions that are held within the `base` package.

::: callout-note
## R package documentation

The `base` package exists as standard in `R`. Unlike other packages, it does not need to be installed and called explicitly. One means of checking the package to which a function you are using belongs is to call the help command (`?`) on that function: e.g. `?mean()`.
:::

Type the commands contained in the code block below into your R Console. Notice that since you are assigning values to each of these objects they are stored in memory and appear under the Global Environment pane.

```{r}
#| echo: true
#| eval: false
# Create variable and assign a value.
x <- 4
# Perform some calculations using R as a calculator.
x_2 <- x^2
# Perform some calculations using functions that form baseR.
x_root <- sqrt(x_2)
```

### Install some packages

There are two steps to getting packages down and available in your working environment:

1.  `install.packages("<package-name>")` downloads the named package from a repository.
2.  `library(<package-name>)` makes the package available in your current session.

Install `tidyverse`, the core collection of packages for doing Data Science in R, by running the code below:

```{r}
#| echo: true
#| eval: false
install.packages("tidyverse")
```

If you have little or no experience in R, it is easy to get confused about downloading and then using packages in a session. For example, let's say we want to make use of the simple features package ([`sf`](https://r-spatial.github.io/sf/index.html)), which we will draw on heavily for performing spatial operations.

```{r}
#| echo: true
#| eval: false
library(sf)
```

Unless you've previously installed `sf`, you'll probably get an error message that looks like this:

```{r}
#| echo: true
#| eval: false
> Error in library(sf): there is no package called ‘sf’
```

So let's install it.

```{r}
#| echo: true
#| eval: false
install.packages("sf")
```

And now it's installed, why not bring up some documentation on one of its functions ([`st_contains()`](https://r-spatial.github.io/sf/reference/geos_binary_pred.html)).

```{r}
#| echo: true
#| eval: false
?st_contains()
```

Since you've downloaded the package but not made it available to your session, you should get the message:

```{r}
#| echo: true
#| eval: false
> No documentation for ‘st_contains’ in specified packages and libraries
```

So let's try again, by first calling `library(sf)`.

```{r}
#| echo: true
#| eval: false
library(sf)
## Linking to GEOS 3.7.2, GDAL 2.4.1, PROJ 6.1.0
?st_contains()
```

Now let's install some of the remaining core packages on which this book depends. Run the block below, which passes a [vector](https://r4ds.had.co.nz/vectors.html) of package names to the `install.packages()` function.

```{r}
#| echo: true
#| eval: false
pkgs <- c(
  "devtools","here", "quarto","fst","tidyverse", "lubridate",
  "tidymodels", "gganimate", "ggforce", "distributional", "ggdist"
)
install.packages(pkgs)
```

::: callout-note
## R package visibility

If you wanted to make use of a package only very occasionally in a single session, you could access it without explicitly loading it via `library(<package-name>)`, using this syntax: `<package-name>::<function_name>`, e.g. `?sf::st_contains()`.
:::

### Experiment with Quarto

Quarto documents are suffixed with the extension `.qmd`. They are computational notebooks that blend code with textual explanation and images, and so are a mechanism for supporting literate programming [@knuth_literate_1984]. They resemble  [Markdown](https://en.wikipedia.org/wiki/Markdown), a lightweight language designed to minimise tedious markup tags (`<header></header>`) when preparing HTML documents. The idea is that you trade some flexibility in the formatting of your HTML for ease-of-writing. Working with Quarto documents feels very similar to Markdown. Sections are denoted hierarchically with hashes (`#`, `##`, `###`) and emphasis using `*` symbols (`*emphasis* **added**` reads *emphasis* **added** ).

Different from standard Markdown, Quarto documents can also contain code chunks to be run when the document is rendered; they are a mechanism for producing reproducible, dynamic and interactive notebooks. *Dynamic* and reproducible because the outputs may change when there are changes to the underlying data; *interactive* because they can execute not just R code blocks, but also [Jupyter Widgets](https://jupyter.org/widgets), [Shiny](https://shiny.rstudio.com/) and [Observable JS](https://observablehq.com/@observablehq/observables-not-javascript). Each chapter has an accompanying Quarto file. In later chapters you will use these to author computational notebooks that blend code, analysis prose and outputs.

Download the [01-template.qmd]() file for this chapter and open it in RStudio by clicking `File` \> `Open File ...` \> `<your-downloads>/01-template.qmd`. Note that there are two tabs that you can switch between when working with `.qmd` files. *Source* retains markdown syntax (e.g. `#|##|###` for headings); *Visual* renders these tags and allows you to, for example, perform formatting and build tables through point-and-click utilities.

A quick anatomy of `.qmd` files :

-   [YAML](https://en.wikipedia.org/wiki/YAML) - positioned at the head of the document and contains metadata determining amongst other things the author details and the output format when rendering.
-   TEXT - incorporated throughout to document and comment on your analysis.
-   CODE chunks - containing discrete blocks that are run when the .qmd file is rendered.

```{r}
#| echo: false
#| label: fig-quarto-annotate
#| fig-cap: "The anatomy of `.qmd` files"
knitr::include_graphics("figs/01/quarto.png", error = FALSE)
```

<!-- ![The anatomy of `.qmd` files](figs/01/quarto.png){#fig-quarto-annotate width=100% fig-align="center"} -->


The [YAML](https://en.wikipedia.org/wiki/YAML) section of an `.qmd` file controls how your file is rendered and consists of `key : value` pairs enclosed by `---`. Notice that you can change the output format to generate for example `.pdf`, `.docx` files for your reports.

    ---
    author: "Roger Beecham"
    title: "Chapter 01"
    format: html
    ---

Quarto files are rendered with the *Render* button, annotated in the Figure above. This starts [`pandoc`](https://pandoc.org/) and executes all the code chunks and outputs in the case above an `.html` file. The markdown file can then be converted to many different output formats via [pandoc](https://pandoc.org/).

-   Render the [01-template.qmd]() file for this chapter by clicking the *Render* button.

You will notice that code chunks in Quarto can be customised in different ways. This is achieved by populating fields immediately after the curly brackets used to declare the code chunk:

````
```{r}`r ''`
#| label: chunk-name
#| echo: true
#| eval: false

# Any R code below is not run (evaluated) but printed (echoed)
# in this position when the .qmd doc is rendered.
```
````

A quick overview of the parameters.

-   `label: <chunk-name>`  Chunks can be given distinct names. This is useful for navigating Quarto files. It also supports chaching -- chunks with distinct names are only run once, important if certain chunks take some time to execute.
-   `echo: <true|false>`  Determines whether the code is visible or hidden from the rendered file. If the output file is a data analysis report you may not wish to expose lengthy code chunks as these may disrupt the discursive text that appears outside of the code chunks.
-   `eval: <true|false>`  Determines whether the code is evaluated (executed). This is useful if you wish to present some code in your document for display purposes.
-   `cache: <true|false>`  Determines whether the results from the code chunk are cached.




### R Scripts

Whilst there are obvious benefits to working in `.qmd` documents when doing data analysis, there may be occasions where a script is preferable. R scripts are plain text files with the extension `.R`. They are typically used for writing discrete but substantial code blocks that are to be executed. For example, a set of [functions](https://r4ds.had.co.nz/functions.html) that relate to a particular use case might be organised into an R script, and those functions referred to in a data analysis from a `.qmd` in a similar way as one might import a package. Below is an example script file with helper functions to support flow visualizations in R. The script is saved with the file name `bezier_path.R`. If it were stored in a sensible location, like a project's `code` folder, it could be called from a `.qmd` file with `source(./code/bezier_path)`. R Scripts can be edited in the same way as Quarto files in RStudio, via the Code Editor pane.

```{r}
#| echo: true
#| eval: false
# bezier_path.R
#
# Author: Roger Beecham
##############################################################################

# This function takes cartesian coordinates defining origin and destination
# locations and returns a tibble representing a path for an asymmetric bezier
# curve, which curves towards the destination location.
#
# The tibble has three rows representing an origin, destination and control
# point for the bezier curve. The parameterisation follows that published in
# Wood et al. 2011. doi: 10.3138/carto.46.4.239.

# o_x, o_y : numeric coords of origin
# d_x, d_y : numeric coords of destination
# od_pair : text string identifying name of od-pair

get_trajectory <- function (o_x, o_y, d_x, d_y, od_pair, curve_extent=-90, curve_position=6)
{
    curve_angle = get_radians(-curve_extent)
    x = (o_x - d_x)/curve_position
    y = (o_y - d_y)/curve_position
    c_x = d_x + x * cos(curve_angle) - y * sin(curve_angle)
    c_y = d_y + y * cos(curve_angle) + x * sin(curve_angle)
    d <- tibble::tibble(x = c(o_x, c_x, d_x), y = c(o_y, c_y,
        d_y), od_pair = od_pair)
    return(d)
}

# Convert degrees to radians.
get_radians <- function(degrees) { (degrees * pi) / (180) }
```

R Scripts are more straightforward than Quarto files in that you don't have to worry about configuring code chunks. They are really useful for quickly developing bits of code. This can be achieved by highlighting over the code that you wish to execute and clicking the `Run` icon at the top of the Code Editor pane or by typing <kbd>ctrl</kbd> + <kbd>rtn</kbd> on Windows, <kbd>⌘</kbd> + <kbd>rtn</kbd> on macOS.

### Create an RStudio Project

Throughout the book we will use project-oriented workflows. This is where all files pertaining to a data analysis -- data, code and outputs -- are organised from a single root folder and where *file path discipline* is used such that all paths are relative to the project's root folder (see [Bryan & Hester 2020](https://rstats.wtf/project-oriented-workflow.html)). You can imagine this self-contained project set-up is necessary for achieving reproducibility of your research. It allows anyone to take a project and run it on their own machines with minimal adjustment.

When opening RStudio, the IDE automatically *points to* a working directory, likely the home folder for your local machine. RStudio will save any outputs to this folder and expect any data you use to be saved there. Clearly if you want to incorporate neat, self-contained project workflows then you will want a dedicated project folder rather than the default home folder for your machine. This can be achieved with the `setwd(<path-to-your-project>)` function. The problem with doing this is that you insert a path which cannot be understood outside of your local machine at the time it was created. This is a real pain. It makes simple things like moving projects around on your machine an arduous task and most importantly it hinders reproducibility if others are to reuse your work.

RStudio Projects resolve these problems. Whenever you load an RStudio Project, `R` starts up and the working directory is automatically set to the project's root folder. If you were to move the project elsewhere on your machine, or to another machine, a new root is automatically generated -- so RStudio projects ensure that relative paths work.

```{r}
#| echo: false
#| label: fig-studio-project
#| fig-cap: "Creating an RStudio Project"
knitr::include_graphics("figs/01/rstudio_project.png", error = FALSE)
```

<!-- ![Creating an RStudio Project](figs/01/rstudio_project.png){#fig-rstudio-project width=100% fig-align="center"} -->


Let's create a new Project for this book:

-   Select `File` \> `New Project` \> `New Directory`.
-   Browse to a sensible location and give the project a suitable name. Then click `Create Project`.

You will notice that the top of the Console window now indicates the root for this new project (`~projects/vis4sds`).

-   In the root of your project, create folders called `reports`, `code`, `data`, `figures`.
-   Save this session's [01-template.qmd]() file to the `reports` folder.

Your project's folder structure should now look like this:

``` text
vis4sds\
  vis4sds.Rproj
  code\
  data\
  figures\
  reports\
    01-template.qmd
```

## Conclusions

Visual data analysis approaches are necessary for exploring complex patterns in data and to make and communicate claims under uncertainty. This is especially true of social data science applications, where: datasets are repurposed for research often for the first time; contain complex structure and geo-spatial relations that cannot be easily captured by statistical summaries alone; and, consequently, where the types of questions that can be asked and the techniques deployed to answer them cannot be specified in advance. This is demonstrated in the book as we explore (Chapter [-@sec-explore] and [-@sec-network]), model under uncertainty (Chapter [-@sec-model]) and communicate (Chapter [-@sec-uncertainty] and [-@sec-storytelling]) with various social science datasets. All technical activity is completed in `R`, making use of tools and software libraries that form part of the `R` ecosystem: the `tidyverse` for doing modern data science and Quarto for authoring reproducible research projects.

<!-- We will work with both new, large-scale behavioural datasets, as well as more traditional, administrative datasets located within various social science domains. We will do so using the statistical programming environment `R`, making use of tools and software libraries that form part of the `R` ecosystem: the `tidyverse` for doing modern data science and Quarto for authoring reproducible research projects. -->
