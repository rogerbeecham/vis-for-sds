# Data Fundamentals {#sec-data}

```{r}
#| echo: false
#| eval: true
#| message: false
#| warning: false
library(knitr)
library(kableExtra)
library(tidyverse)
```

By the end of this chapter you should gain the following knowledge and practical skills.

::: {.callout-note icon=false}

## Knowledge outcomes

- [x] Learn the vocabulary and concepts used to describe data.
- [x] Appreciate the characteristics and importance of tidy data [@wickham_tidy_2014].
:::


::: {.callout-note icon=false}

## Skills outcomes

- [x] Load flat file datasets.
- [x] Calculate descriptive summaries over datasets.
- [x] Apply high-level functions in `dplyr` and `tidyr` for working with data.
- [x] Create statistical graphics that expose high-level structure in data for cleaning purposes.

:::


## Introduction

This chapter covers some of the basics of how to describe and organise data. Whilst this might sound prosaic, there are several reasons why being able to consistently describe a dataset is important.  First, it is the initial step in any analysis and helps delimit the analytical procedures that can be deployed. This is especially relevant to data science-type workflows, where it is common to apply the same analysis templates for working over many different datasets. Describing your dataset using a consistent vocabulary enables you to identify which analysis templates to reuse. Second relates to the point in [Chapter -@sec-introduction] that Social Data Science  (SDS) projects usually involve repurposing datasets for the first time. It is often not obvious whether the data contain sufficient detail and structure to characterise the behaviours being researched and the target populations they are assumed to represent. This leads to additional levels of uncertainty and places greater importance on the initial steps of data processing, description and exploration.

Through the chapter we will learn both language for describing and thinking about data, but also how to deploy some of the most important data processing and organisation techniques in R to wrangle real datasets. We will do so using data from New York's [Citibike](https://www.citibikenyc.com/) scheme, accessed through the `bikedata` package, an API to Citibike's [publicly available origin-destination trip data](https://www.citibikenyc.com/system-data).

::: {.callout-note}
## Data vocabulary
The idea of applying a consistent vocabulary to describing your data applies especially to working with modern visualization toolkits ([ggplot2](https://ggplot2.tidyverse.org/), [Tableau](https://www.tableau.com/en-gb), [vega-lite](https://vega.github.io/vega-lite/)), and will be covered in some detail in chapter 3 as we introduce Visualization Fundamentals and the [Grammar of Graphics](https://www.springer.com/gp/book/9780387245447) [@wilkinson_grammar_1999].
:::


## Concepts

### Data structure

Throughout this book we will work with [data frames](http://adv-r.had.co.nz/Data-structures.html#data-frames). These are spreadsheet-like representations where rows are *observations* and columns are *variables*. In a data frame, variables are [vectors](http://adv-r.had.co.nz/Data-structures.html#vectors) that must be of equal length. Where observations have missing values for certain variables -- that is, where they may violate this equal-length requirement -- the missing values must be substituted with something, usually with `NA` or similar. This constraint can cause difficulties. For example, when working with variables that contain many values of different length for an observation. In these cases we create a special class of column, a [`list-column`](https://jennybc.github.io/purrr-tutorial/ls13_list-columns.html). Organising data according to this simple structure -- rows as observations, columns as variables -- makes working with data somewhat straightforward. A dedicted set of tools, made available via the `tidyverse`, can be deployed for performing most data *tidy*ing operations [@wickham_tidy_2014].


### Types of variable

A familiar classification for describing variables is that developed by @stevens_on_1946 when cosidering the *level of measurement* of a variable. @stevens_on_1946 organises variables into two classes: variables that describe *categories* of things and variables that describe *measurements* of things. 

Categories include attributes like gender, titles, ranked orders (1st, 2nd, 3rd largest etc.). Measurements include quantities like distance, age, travel time. Categories can be further subdivided into those that are unordered (*nominal*) from those that are ordered (*ordinal*). Measurements can also be subdivided. *Interval* measurements are quantities where the computed difference between two values is meaningful. *Ratio* measurements have this property, but also have a meaningful `0`, where `0` means the absence of something, and the ratio of two values can be computed. The most common cited example of an interval measurement is temperature in &deg;C. Temperatures can be ordered and compared additively, but 0 in &deg;C does not mean the absence of temperature and 20&deg;C is not twice as hot as 10&deg;C.

Why is this useful? The measurement level of a variable determines the types of data analysis operations that can be performed and therefore allows us to efficiently make decisions when working with a dataset for the first time (@tbl-variable-types).

```{r}
#| label: tbl-variable-types
#| tbl-cap: "Breakdown of variable types"
#| echo: false
#| eval: true

variable_types <- tibble::tibble(
  Measurement = c("Nominal", "Ordinal", "Interval", "Ratio"),
  Example = c(
    "Political parties; street names",
    "Terrorism threat levels",
    "Temperatures; years",
    "Distances; prices "
    ),
  Operators = c(
    "&#61;  &#8800;",
    "&#61;  &#8800; <>",
    "&#61;  &#8800; <> +  -",
    "&#61;  &#8800; <> +  - &#124 &#215; &#247;"
    ),
  Midpoint = c(
    "mode",
    "median",
    "mean",
    "mean"
   ),
  Spread = c(
    "entropy",
    "percentile",
    "variance",
    "variance"
  )
)

kbl(variable_types, protect_latex = TRUE, escape=FALSE) |>
 pack_rows("Categories", 1, 2, bold=FALSE, label_row_css = "border-bottom: 0px solid;") |>
 pack_rows("Measures", 3, 4, bold=FALSE, label_row_css = "border-bottom: 0px solid;") |>
 column_spec(c(1,3), width = "8em",  extra_css = "font-family:  Monospace") |> kable_minimal()
```

### Types of observation

Observations either together form an entire *population* or a  subset, or *sample*, that we expect is representative of a *target population*. In Social Data Science applications we may often be working with datasets that are so-called population-level. The [Citibike dataset]((https://www.citibikenyc.com/system-data)) is a complete, population-level dataset in that every journey made through the  scheme is recorded. Whether or not this is truly a population-level dataset, however, depends on the analysis purpose. When analysing the bikeshare dataset are we interested only in describing use within the Citibike scheme? Or are we taking the patterns observed through our analysis to make claims and inferences about cycling more generally?

If the latter, then there are problems as the level of detail we have on our sample is pretty trivial compared to traditional, *actively-collected* datasets, where data collection activities are designed with a specified target population in mind. It may therefore be difficult to gauge how representative Citibike users and Citibike cycling is of New York's general cycling population. The flipside is that such *passively-collected* data do not suffer from the same problems such as non-response bias and social-desirability  bias as traditional, actively-collected datasets.

### Tidy data

We will be working with data frames organised such that columns always and only refer to variables and rows always and only refer to observations. This arrangement, called *tidy* [@wickham_tidy_2014],  has two key advantages. First, if data are arranged in a consistent way, then it is easier to apply and re-use tools for wrangling them due to data having the same underlying structure. Second, placing variables into columns, with each column containing a vector of values,  means that we can take advantage of R's vectorised functions for transforming data -- we will demonstrate this in the technical element of the chapter.

The three rules for tidy data:

1. Each variable forms a column.
2. Each observation forms a row.
3. Each type of observational unit forms a table.

#### Drug treatment dataset {-}

To elaborate further, we can use the example given in @wickham_tidy_2014, a drug treatment dataset in which two different treatments are administered to participants.


```{r}
#| label: tbl-drugs
#| tbl-cap: "Table 1 of Wickham 2014"
#| tbl-subcap:
#|   - "One untidy organisation"
#|   - "Alternative untidy organisation"
#|   - "Tidy organisation"
#| layout-ncol: 2
#| echo: false


drugs <- tibble::tibble(
  person = c("John Smith", "Jane Doe", "Mary Johnson"),
  treatment_a = c("--", "16", "3"),
  treatment_b = c("2", "11", "1")
    ) 
kbl(drugs) |> kable_minimal()

drugs2 <- tibble::tibble(
  treatment = c("treatment_a", "treatment_b"),
  "John Smith" = c("--", "2"),
  "Jane Doe" = c("16", "11"),
  "Mary Johnson" = c("3", "1")
    )
kbl(drugs2) |> kable_minimal()

drugs_tidy <- tibble::tibble(
  person = c("John Smith", "John Smith", "Jane Doe", "Jane Doe", "Mary Johnson", "Mary Johnson"),
  treatment = c("a", "b", "a", "b", "a", "b"),
  result = c("--", "2", "16", "11", "3", "1")
    )
kbl(drugs_tidy) |> kable_minimal()
```

Both untidy tables present the same information unambiguously -- @tbl-drugs (b) is simply @tbl-drugs (a) transposed. However, neither is *tidy* as the observations are spread across both the rows and columns. This means that we need to apply different procedures to extract, perform computations on and visually represent, these data.

In the *tidy* form, each *observation* is a test result returned for each combination of `person` and `treatment`.  To get to this formulation, it is necessary to identify the discrete variables:

1. `person`:  a categorical nominal variable which takes three values: John Smith, Jane Doe, Mary Johnson.
2. `treatment`: a categorical nominal variable which takes values: a and b.
3. `result`: a measurement ratio variable with six recorded values (including the missing value): --, 16, 3, 2, 11,



#### Gapminder population dataset {-}

In @wickham_r_2017, the benefits of tidy layouts are demonstrated using the [`gapminder`](https://www.gapminder.org/data/) dataset and with reference to key data processing functions in R.  To consolidate our understanding of *tidy* data let's quickly look at the `gapminder` data, as it is an example that we're probably more likely to encounter in social science research.


```{r, echo=FALSE}
#| label: tbl-gapminder
#| tbl-cap: "Excerpts of [`gapminder`](https://www.gapminder.org/data/) dataset."
#| tbl-subcap:
#|   - "Tidy organisation"
#|   - "Untidy organisation"
#|   - "Another untidy organisation"
#| layout-ncol: 2

gapminder_tidy <- tibble::tibble(
  country = c("Afghanistan", "Afghanistan", "Brazil", "Brazil", "China", "China"),
  year = c("1999", "2000", "1999", "2000", "1999", "2000"),
  cases = c("745", "2666", "37737", "80488", "212258", "213766"),
  pop = c("19987071", "20595360", "172006362", "174504898", "1272915272", "1280428583")
    )
kbl(gapminder_tidy) |> kable_minimal()

gapminder_untidy1 <- tibble::tibble(
  country = c("Afghanistan", "Afghanistan", "Afghanistan", "Afghanistan",  "Brazil", "Brazil", "..."),
  year = c("1999", "1999", "2000", "2000", "1999", "1999", "..."),
  type = c("cases", "pop", "cases", "pop", "cases", "pop", "..."),
  count = c("745", "19987071", "2666", "20595360", "37737", "174504898", "...")
)
kbl(gapminder_untidy1) |> kable_minimal()

gapminder_untidy2 <- tibble::tibble(
  country = c("Afghanistan", "Afghanistan", "Brazil", "Brazil", "China", "China"),
  year = c("1999", "2000", "1999", "2000", "1999", "2000"),
  f_cases = c("447", "1599", "16982", "39440", "104007", "104746"),
  m_cases = c("298", "1067", "20755", "41048", "108252", "109759"),
  f_pop = c("9993400", "10296280", "86001181", "87251329", "636451250", "640212600"),
  m_pop = c("9993671", "10299080", "86005181", "87253569", "636464022", "640215983")
    )
kbl(gapminder_untidy2) |> kable_minimal()
```
From the *tidy* version of the data, we can identify the *variables* and note that each *observation* is a recorded count of cases and population for a country in a year.

1. `country`: a categorical nominal variable.
2. `year`: a date (interval) variable.
3. `cases`: a ratio (count) variable.
4. `population`: a ratio (count) variable.


An alternative organisation is in @tbl-gapminder (b). This is *untidy* as the observations are spread across two rows, making operations that we might want to perform on the `cases` and `population` variables, for example computing exposure rates, somewhat tedious.

Imagine that the `gapminder` dataset reported values of `cases` separately by gender  -- @tbl-gapminder (c). A type of representation often seen in social science domains, probably as it is helpful for data entry, is where observations are spread across the columns. This too creates problems for performing aggregate functions, but also for specifying visualization designs in [`ggplot2`](https://ggplot2.tidyverse.org/).

## Techniques

The technical element to this chapter imports, describes, transforms and tidies data from New York's [Citibike](https://www.citibikenyc.com/) scheme.

* Download the [02-template.qmd]() file and save it to the `reports` folder of your `vis4sds` project that you created in chapter 1.
* Open your `vis4sds` project in RStudio and load the template file by clicking `File` > `Open File ...` > `reports/02-template.qmd`.


### Import

In the template file there is a discussion of how to setup your R session with key packages -- `tidyverse` , `fst`, `lubridate`, `sf` -- and also the [`bikedata`](https://github.com/ropensci/bikedata) package for accessing bikeshare data.

Available via the `bikedata` package are trip and occupancy data for a number of bikeshare schemes (as below). We will work with data from New York's [Citibike](https://www.citibikenyc.com/) scheme for June 2020. A list of all cities covered by the `bikedata` package is below:

```{r}
#| echo: true
#| eval: false
bike_cities()
##    city     city_name      bike_system
## 1    bo        Boston           Hubway
## 2    ch       Chicago            Divvy
## 3    dc Washington DC CapitalBikeShare
## 4    gu   Guadalajara           mibici
## 5    la   Los Angeles            Metro
## 6    lo        London        Santander
## 7    mo      Montreal             Bixi
## 8    mn   Minneapolis         NiceRide
## 9    ny      New York         Citibike
## 10   ph  Philadelphia           Indego
## 11   sf      Bay Area       FordGoBike
```

In the template there are code chunks demonstrating how to  download and process these data using [bikedata](https://github.com/ropensci/bikedata)'s API. This  takes some time to execute and we use the [`fst`](https://www.fstpackage.org/) package for serializing and reading in the these data.

```{r}
#| echo: true
#| eval: false

library(bikedata) # API to TfL's trip data.

# Create subdirectory in data folder for storing bike data.
if(!dir.exists(here("bikedata"))) dir.create(here("bikedata"))

# Download data for June 2020.
dl_bikedata (city = "citibike",  data_dir = here("bikedata"), dates=202006)

# Read in and store in SQLite3 database.
# Create sqlite db container.
bikedb <- file.path (tempdir (), "bikedb.sqlite") 
store_bikedat (data_dir = here("data", "bikedata"), bikedb = bikedb)
# Index dbase to speed up working.
index_bikedata_db(bikedb = bikedb)

# Set up connection to SQLite db.
con <- DBI::dbConnect(RSQLite::SQLite(), bikedb)
# Check tables that form the dbase.
DBI::dbListTables(con)
# Collect trips and stations tables for writing out to fst.
trips <- tbl(con, "trips") |>  collect()
stations <- tbl(con, "stations") |>  collect()

# Write trips out to .fst.
fst::write_fst(trips, here("data", ""bikedata", "ny_trips.fst"))
# Write stations out to .csv.
write_csv(stations, here("data", "bikedata", "ny_stations.csv"))

# Clean workspace
bike_rm_db(bikedb)
rm(db, stations, trips, bikedb)
```
::: {.callout-note}
`fst` implements in the background various operations such as multi-threading to reduce load on disk space. This makes it possible to work with large datasets in-memory in R rather than connecting to a database and serving up summaries/subsets to be loaded into R. We will be working with just 2 million records, but with `fst` it is possible to work in-memory with much larger datasets.
:::

Some of the above may be familiar to you. The key arguments to look at are `read_csv()` and `read_fst()`, into which we pass the path to the file. In this case we create a `tmpfile()` within the R session. We then write these data out and save locally to the project's `data` folder. This is useful as we only want to download the data once. In the `write_*<>` functions we reference this location using the [`here`](https://here.r-lib.org/) package's `here()` function, useful for reliably creating paths relative to your project's root. 

To read in these data in future R sessions, we again use `read_csv()` and `read_fst()` and point to the new file locations. Notice that we use *assignment* here (`<-`) so that these data are loaded as objects and appear in the Environment pane of your RStudio window.

```{r}
#| echo: true
#| eval: false
# Read in these local copies of the trips and stations data.
ny_trips <- read_fst(here("data", "ny_trips.fst"))
ny_stations <- read_csv(here("data", "ny_stations.csv"))
```


`ny_stations` and `ny_trips` are data frames, spreadsheet type representations containing observations in rows and variables in columns. Inspecting the layout of the stations data with `View(ny_stations)` you will notice that the top line is the header and contains column (variable) names.

```{r, echo=FALSE}
#| include: false
#| label: fig-view-annotate
#| fig-cap: "`ny_trips` and `ny_stations` as they appear when calling `View()`."

include_graphics("/figs/view.png", error = FALSE)
```

![`ny_trips` and `ny_stations` as they appear when calling `View()`](figs/01/view.png){#fig-view-annotate width=100% fig-align="center"}

### Describe

There are several functions for generating a quick overview of a data frame's contents. `glimpse<dataset-name>` provides a summary of the data frame dimensions -- we have c. 1.9 million trip observations in `ny_trips` and 11 variables^[Note that the version of the trips data downloaded from my external repo contains a sample of just 500k records -- this is not ideal, but was due to data storage limits on my external repo.]. The function also prints out the object type for each of these variables, with the variables either of type `int`, `chr` or dbl` in this case.

```{r}
#| echo: true
#| eval: false
glimpse(ny_trips)
## Rows: 1,882,273
## Columns: 11
## $ id               <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21…
## $ city             <chr> "ny", "ny", "ny", "ny", "ny", "ny", "ny", "ny", "ny", "ny", "ny", "ny", "…
## $ trip_duration    <dbl> 1062, 3810, 1017, 226, 1437, 355, 99, 1810, 87, 2714, 2096, 1611, 529, 69…
## $ start_time       <chr> "2020-06-01 00:00:03", "2020-06-01 00:00:03", "2020-06-01 00:00:09", "202…
## $ stop_time        <chr> "2020-06-01 00:17:46", "2020-06-01 01:03:33", "2020-06-01 00:17:06", "202…
## $ start_station_id <chr> "ny3419", "ny366", "ny389", "ny3255", "ny367", "ny248", "ny3232", "ny3263…
## $ end_station_id   <chr> "ny3419", "ny336", "ny3562", "ny505", "ny497", "ny247", "ny390", "ny496",…
## $ bike_id          <chr> "39852", "37558", "37512", "39674", "21093", "39594", "43315", "16571", "…
## $ user_type        <chr> "Customer", "Subscriber", "Customer", "Customer", "Customer", "Subscriber…
## $ birth_year       <chr> "1997", "1969", "1988", "1969", "1997", "1990", "1938", "1995", "1971", "…
## $ gender           <dbl> 2, 0, 2, 0, 2, 1, 2, 2, 2, 1, 1, 0, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…
```

```{r}
#| echo: true
#| eval: false
glimpse(ny_stations)
## Rows: 1,010
## Columns: 6
## $ id        <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 2…
## $ city      <chr> "ny", "ny", "ny", "ny", "ny", "ny", "ny", "ny", "ny", "ny", "ny", "ny", "ny", "n…
## $ stn_id    <chr> "ny116", "ny119", "ny120", "ny127", "ny128", "ny143", "ny144", "ny146", "ny150",…
## $ name      <chr> "W 17 St & 8 Ave", "Park Ave & St Edwards St", "Lexington Ave & Classon Ave", "B…
## $ longitude <chr> "-74.00149746", "-73.97803415", "-73.95928168", "-74.00674436", "-74.00297088", …
## $ latitude  <chr> "40.74177603", "40.69608941", "40.68676793", "40.73172428", "40.72710258", "40.6…
```

```{r, echo=FALSE}
#| label: tbl-data-types
#| tbl-cap: "A breakdown of data types in R."
#| echo: false

data_types <- tibble::tibble(
  Type = c("`lgl`", "`int`", "`dbl`", "`chr`", "`dttm`", "`fctr`" ),
  Description = c(
    "Logical -- vectors that can contain only `TRUE` or `FALSE` values",
    "Integers -- whole numbers",
    "Double --  real numbers with decimals",
    "Character -- text strings",
    "Date-times -- a date + a time",
    "Factors -- represent categorical variables of fixed and potentially orderable values"
  )
)
```

The object type of a variable relates to its *measurement level*, though it is common to override the notional assignment of measurement levels depending on ise case. For example, we may which to convert the `start_time` and `stop_time` variables to a date-time format so that various time-related functions can be used. For efficient storage, we may wish to convert the *station identifier* variables as `int` types by removing the redundant "ny" text which prefaces `end_station_id`, `end_station_id`, `stn_id`. The geographic coordinates are currently stored as type `chr`. These could be regarded as quantitative variables, floating points with decimals. So converting to type `dbl` or as a [`POINT`](https://r-spatial.github.io/sf/articles/sf1.html#simple-feature-geometry-types) geometry type may be sensible.

In the [02-template.qmd]() file there are code chunks for doing these conversions. There are some slightly more involved data transform procedures in this code. Don't fixate too much on these, but the upshot can be seen when running `glimpse()` on the converted data frames:


```{r}
#| echo: true
#| eval: false
glimpse(ny_trips)
## Rows: 1,882,273
## Columns: 10
## $ id               <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 2…
## $ trip_duration    <dbl> 1062, 3810, 1017, 226, 1437, 355, 99, 1810, 87, 2714, 2096, 1611, 529, 695, 206,…
## $ start_time       <dttm> 2020-06-01 00:00:03, 2020-06-01 00:00:03, 2020-06-01 00:00:09, 2020-06-01 00:00…
## $ stop_time        <dttm> 2020-06-01 00:00:03, 2020-06-01 00:00:03, 2020-06-01 00:00:09, 2020-06-01 00:00…
## $ start_station_id <int> 3419, 366, 389, 3255, 367, 248, 3232, 3263, 390, 319, 237, 3630, 3610, 3708, 465…
## $ end_station_id   <int> 3419, 336, 3562, 505, 497, 247, 390, 496, 3232, 455, 3263, 3630, 3523, 3740, 379…
## $ bike_id          <int> 39852, 37558, 37512, 39674, 21093, 39594, 43315, 16571, 28205, 41760, 30745, 380…
## $ user_type        <chr> "Customer", "Subscriber", "Customer", "Customer", "Customer", "Subscriber", "Sub…
## $ birth_year       <int> 1997, 1969, 1988, 1969, 1997, 1990, 1938, 1995, 1971, 1989, 1990, 1969, 1984, 19…
## $ gender           <chr> "female", "unknown", "female", "unknown", "female", "male", "female", "female", …
```

```{r}
#| echo: true
#| eval: false
glimpse(ny_stations)
## Rows: 1,010
## Columns: 5
## $ id        <dbl> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, …
## $ stn_id    <int> 116, 119, 120, 127, 128, 143, 144, 146, 150, 151, 157, 161, 164, 167, 168, 173, 174, 19…
## $ name      <chr> "W 17 St & 8 Ave", "Park Ave & St Edwards St", "Lexington Ave & Classon Ave", "Barrow S…
## $ longitude <dbl> -74.00150, -73.97803, -73.95928, -74.00674, -74.00297, -73.99338, -73.98069, -74.00911,…
## $ latitude  <dbl> 40.74178, 40.69609, 40.68677, 40.73172, 40.72710, 40.69240, 40.69840, 40.71625, 40.7208…
```

### Transform

#### Transform with `dplyr` {-}

```{r}
#| label: tbl-dplyr-verbs
#| tbl-cap: "dplyr funcitions (verbs) for manipulating data frames."
#| echo: false

dplyr_verbs <- tibble::tibble(
  `function()` = c("filter()", "arrange()", "select()", "rename()", "mutate()", "group_by()", "summarise()" , "..and more"),
  Description = c(
    "Picks rows (observations) if their values match a specified criteria",
    "Reorders rows (observations) based on their values",
    "Picks a subset of columns (variables) by name (or name characteristics)",
    "Changes the name of columns in the data frame",
    "Adds new columns (or variables)",
    "Chunks the dataset into groups for grouped operations",
    "Calculate single-row (non-grouped) or multiple-row (if grouped) summary values",
    ""
  )
)

kbl(dplyr_verbs) |> column_spec(1, extra_css = "font-family:  Monospace") |> kable_minimal()
```

[`dplyr`](https://dplyr.tidyverse.org/) is a most important package. It provides a *grammar of data manipulation*, with access to functions that can be variously combined to support most data processing and transformation activity. Once you become familiar with `dplyr` functions (or *verbs*) you will find yourself generating analysis templates to re-use whenever you work on a new dataset.

All `dplyr` functions work in the same way:

1. Start with a data frame.
2. Pass some arguments to the function which control what you do to the data frame.
3. Return the updated data frame.

So every `dplyr` function expects a data frame and will always return a data frame.

####  Use pipes `|>` with `dplyr` {-}

`dplyr` is most effective when its functions are chained together -- you will see this shortly as we explore the New York bikeshare data. This chaining of functions can be achieved using the *pipe* operator (`|>`).  Pipes are used for passing information in a program. They take the output of a set of code (a `dplyr` specification) and make it the input of the next set (another `dplyr` specification).

Pipes can be easily applied to `dplyr` functions, and the functions of all packages that form the `tidyverse`. We mentioned in Chapter [-@sec-introduction] that `ggplot2` provides a framework for specifying a *layered grammar of graphics* (more on this in Chapter [-@sec-visual]). Together with the pipe operator (`|>`), `dplyr` supports a *layered grammar of data manipulation*.

#### `count()` rows {-}

This might sound a little abstract so let's use and combine some `dplyr` functions to generate statistical summaries on the New York bikeshare data.

First we'll count the number of trips made in Jun 2020 by gender. `dplyr` has a convenience  function for counting, so we could run the code below, also in the [02-template.qmd]() for this chapter. The code block is commented to convey what each line achieves.

```{r}
#| echo: true
#| eval: false
 # Take the ny_trips data frame.
ny_trips |> 
  # Run the count function over the data frame 
  # and set the sort parameter to TRUE.
  count(user_type, sort=TRUE) 
##   user_type       n
## 1 subscriber 1326513
## 2   customer  592681
```
There are a few things happening in the `count()` function. It takes the `gender` variable from `ny_trips`, organises or *groups* the rows in the data frame according to its values (`female` | `male` | `unknown`), counts the rows and then orders the *summarised* output descending on the counts.

#### `summarise()` over rows {-}

```{r, echo=FALSE}
#| label: tbl-aggregate-functions
#| tbl-cap: "A breakdown of aggregate functions commonly used with `summarise()`."
#| echo: false

aggregate_functions <- tibble::tibble(
  Function = c("n()", "n_distinct(var)", "sum(var)", "max(var)|min(var)", "mean(var)|median(var)| ...", "..."),
  Description = c(
    "Counts the number of observations",
    "Counts the number of unique observations",
    "Sums the values of observations",
    "Finds the min|max values of observations ",
    "Calculates central tendency of observations ",
    "Many more"
  )
)
kbl(aggregate_functions) |> column_spec(1, extra_css = "font-family:  Monospace") |> kable_minimal()
```

Often you will want to do more than simply counting and you may also want to be more explicit in the way the data frame is *grouped* for computation. We'll demonstrate this here with a more involved analysis of the usage data and with some key aggregate functions (@tbl-aggregate-functions).


A common workflow is to combine `group_by()` and `summarise()`, and in this case `arrange()` to replicate the `count()` example.

```{r}
#| echo: true
#| eval: false
# Take the ny_trips data frame.
ny_trips |> 
  # Group by user_type.
  group_by(user_type) |> 
    # Count the number of observations per group.
    summarise(count=n()) |>
    # Arrange the grouped and summarised (collapsed) rows 
    # according to count. 
    arrange(desc(count)) 
## # A tibble: 2 × 2
##   user_type    count
##   <chr>        <int>
## 1 subscriber 1326513
## 2 customer    592681
```

In `ny_trips` there is a variable measuring trip duration in seconds (`trip_duration`) and distinguishing  casual users from those formally registered to use the scheme (`user_type` - `customer` vs. `subscriber`). It may be instructive to calculate some summary statistics to see how trip duration varies between these groups.

The code below uses `group_by()`, `summarise()` and `arrange()` in exactly the same way, but with the addition of other aggregate functions profiles the `trip_duration` variable according to central tendency (mean and standard deviation) and by `user_type`.

```{r}
#| echo: true
#| eval: false
# Take the ny_trips data frame.
ny_trips |> 
  mutate(trip_duration=trip_duration/60) |>
  # Group by user type.
  group_by(user_type) |> 
  # Summarise over the grouped rows, 
  # generate a new variable for each type of summary.
  summarise( 
    count=n(),
    avg_duration=mean(trip_duration),
    median_duration=median(trip_duration),
    sd_duration=sd(trip_duration),
    min_duration=min(trip_duration),
    max_duration=max(trip_duration)
    ) |>
  # Arrange on the count variable.
  arrange(desc(count)) 

## # A tibble: 2 × 7
##   user_type    count avg_duration median_duration sd_duration min_duration max_duration
##   <chr>        <int>        <dbl>           <dbl>       <dbl>        <dbl>        <dbl>
## 1 subscriber 1326513         20.1            14.4        110.         1.02       33090.
## 2 customer    592681         44.1            23.3        393.         1.02       46982.
```


As each line is commented you hopefully get a sense of what is happening in the code above. You will notice that `dplyr` functions read like *verbs*, and this is a very deliberate design decision. With the code laid out as above -- each `dplyr` verb occupying a single line, separated by a pipe (`|>`) -- you can generally understand the code with a cursory glance. There are obvious benefits to this. Once you are familiar with `dplyr` it becomes very easy to read, write and share code.


::: {.callout-note}
Remembering that *pipes* (`|>`) take the output of a set of code and make it the input of the next set, you will notice separate lines for each call to the *pipe* operator. This is good practice for supporting readability of your code.
:::


#### Manipulate dates with [`lubridate`](https://lubridate.tidyverse.org/) {-}

Let's continue this investigation of trips by user-type by profiling how usage varies over time. To do this we will need to work with `date-time` variables. The [`lubridate`](https://lubridate.tidyverse.org/) package provides various convenience functions for this.

In the code block below we extract the *day of week* and *hour of day* from the `start_time` variable using `lubridate`'s [day accessor](https://lubridate.tidyverse.org/reference/day.html) functions. Documentation on these can be accessed in the usual way (`?<function-name>`), but reading down the code it should be clear to you how this works. Next we count the number of trips made by hour of day, day of week and user-type. The summarised data frame will be re-used several times in our analysis, so we store it as an object with a suitable name (`ny_temporal`) using the assignment operator.

```{r}
#| echo: true
#| eval: false
# Create a hod dow summary by gender and assign it the name "ny_temporal".
# Take the ny_trips data frame.
ny_temporal <- ny_trips |>  
  mutate(
    # Create a new column identify dow.
    day=wday(start_time, label=TRUE), 
    # Create a new column identify hod.
    hour=hour(start_time)) |> 
  # Group by day, hour, user_type.
  group_by(user_type, day, hour) |> 
  # Count the grouped rows.
  summarise(count=n()) |> 
  ungroup()
```
::: {.callout-note}
Whether or not to store derived data tables, like the newly assigned `ny_temporal`, in a session is not an easy decision. You want to try to avoid cluttering your Environment pane with many data objects. Often when generating charts it is necessary to create these sorts of derived tables as input data (to `ggplot2`), which risks an unhelpfully large number of such tables. A general rule: if the derived table is to be used more than 3 times in a data analysis or is computationally intensive, assign it (`<-`) to an object.
:::

In [@fig-plot-temporal] below the derived data are plotted. The template contains `ggplot2` code for creating the graphic. Don't obsess too much on it -- this is covered extensively in the following chapter. The plot demonstrates a familiar weekday-weekend pattern of usage. Trip frequencies peak in the morning and evening rush hours during weekdays and mid/late-morning and afternoon during weekends. This is consistent with typical travel behaviour. Notice though that the weekday afternoon peak is much larger than the morning peak. There are several speculative explanations for this.  Note that there are also obvious  differences in the type of trips made by  `subscribers` versus `customers` -- the temporal signature for `subscribers` appears to match more closely what one would expect of commuting behaviour.
```{r}
#| include: false
#| eval: false
#| echo: false
#| label: fig-plot-temporal
#| fig-cap: "Line charts generated with `ggplot2`. Plot data computed using `dplyr` and `lubridate`."
include_graphics("/figs/hod_dow.png", error = FALSE)
```

![Line charts generated with `ggplot2`. Plot data computed using `dplyr` and `lubridate`](figs/02/hod_dow.svg){#fig-plot-temporal width=100% fig-align="center"}


::: {.callout-note}
 Our analysis is based on data from June 2020, a time when New York residents were emerging from lockdown. It would be instructive to compare with data from a non-Covid year. The fact that bikeshare is collected continuously makes this sort of *behavioural change* analysis possible. Check out [Jo Wood's work](https://github.com/jwoLondon/mobv) analysing Covid-related change in movement behaviours across a range of cities.
:::



#### Relate tables with `join()` {-}

Trip distance is not recorded directly in the `ny_trips` table, but may be important for profiling usage behaviour. Calculating trip distance is eminently achievable as the `ny_trips` table contains the origin and destination station of every trip and the `ny_stations` table contains coordinates corresponding to those stations. To relate the two tables, we need to specify a *join* between them.

A sensible approach is to:

1. Select all uniquely cycled trip pairs (origin-destination pairs) that appear in the `ny_trips` table.
2. Bring in the corresponding coordinate pairs representing the origin and destination stations by joining on the `ny_stations` table.
3. Calculate the distance between the coordinate pairs representing the origin and destination.

The code below is one way of achieving this.

```{r}
#| echo: true
#| eval: false
# Take the ny_trips data frame.
od_pairs <- ny_trips |> 
  # Select trip origin and destination (OD) station columns 
  # and extract unique OD pairs.
  select(start_station_id, end_station_id) |>  unique() |> 
  # Select lat, lon columns from ny_stations and join on origin column.
  left_join(
    ny_stations |> select(stn_id, longitude, latitude), 
    by=c("start_station_id"="stn_id")
    ) |> 
  # Rename new lat, lon columns -- associate with origin station.
  rename(o_lon=longitude, o_lat=latitude) |>  
  # Select lat, lon columns from ny_stations and join on destination column.
  left_join(
    ny_stations |> select(stn_id, longitude, latitude), 
    by=c("end_station_id"="stn_id")
    ) |> 
  # Rename new lat, lon columns and associate with destination station.
  rename(d_lon=longitude, d_lat=latitude) |>  
  # Compute distance calculation one row-at-a-time.
  rowwise() |> 
  # Calculate distance and express in kms.
  mutate(dist=geosphere::distHaversine(c(o_lat, o_lon), c(d_lat, d_lon))/1000) |> 
  ungroup()
```

The code block above introduces some new functions: `select()` to pick or drop variables,  `rename()` to rename variables and a convenience function for calculating straight line distance from polar coordinates (`distHaversine()`). The key function to emphasise is the `left_join()`. If you've worked with relational databases, `dplyr`'s join functions will be familiar to you. In a `left_join`, all the values from the main table are retained, the one on the left -- `ny_trips`, and variables from the table on the right (`ny_stations`) -- are added. We specify explicitly the variable on which the tables should be joined with the `by=` parameter, `station_id` in this case. If there is a `station_id` in `ny_trips` that doesn't exist in `ny_stations` then `NA` is returned.

Other *join* functions provided by `dplyr` are in the table below. Rather than discussing each, I recommend consulting [Chapter 13](https://r4ds.had.co.nz/relational-data.html) of @wickham_r_2017.

```{r}
#| label: tbl-table-joins
#| tbl-cap: "A breakdown of `dplyr` join functions."
#| echo: false

join_functions <- tibble::tibble(
  Function   = c("left_join()", "right_join()", "full_join()", "`semi_join()`", "inner_join()", "anti_join"),
  Description = c(
    "all rows from x",
    "all rows from y",
    "all rows from both x and y",
    "all rows from x where there are matching values in y, keeping just columns from x",
    " all rows from x where there are matching values in y, return all combination of multiple matches in the case of multiple matches",
    "return all rows from x where there are not matching values in y, never duplicate rows of x"
  )
)
kbl(join_functions, col.names = NULL) |> column_spec(1, width = "10em") |> add_header_above(c("*_join(x, y) ..."=1, " ")) |>
column_spec(1, extra_css = "font-family:  Monospace") |>
row_spec(0, extra_css = "font-family:  Monospace") |> kable_minimal()

```


```{r}
#| include: false
#| label: fig-plot-dist
#| fig-cap: "Histograms generated with `ggplot2`. Plot data computed using `dplyr` and `lubridate`"
#| echo: false
include_graphics("/figs/dist.png", error = FALSE)
```

![Histograms generated with `ggplot2`. Plot data computed using `dplyr` and `lubridate`](figs/02/dist.svg){#fig-plot-dist width=100% fig-align="center"}



From the newly created distance variable, we can calculate the average (mean) trip distance for the 1.9 milliom trips -- 1.6km. This might seem very short, but remember that these are straight-line distances between pairs of docking stations. Ideally we would calculate network distances derived from cycle trajectories. A separate reason, discovered when generating a histogram on the `dist` variable, is that there are a large number of trips (c.124,000) that start and end at the same docking station. Initially these might seem to be unsuccessful hires -- people failing to undock a bike for example. We could investigate this further by paying attention to the docking stations at which same origin-destination trips occur, as in the code block below.

```{r}
#| echo: true
#| eval: false
ny_trips |> 
  filter(start_station_id==end_station_id) |> 
  group_by(start_station_id) |>  summarise(count=n()) |> 
  left_join(
    ny_stations |>   select(stn_id, name), 
    by=c("start_station_id"="stn_id")
    ) |> 
  arrange(desc(count))

## # A tibble: 958 x 3
##    start_station_id count name
##    <chr>            <int> <chr>
##  1 ny3423            2017 West Drive & Prospect Park West
##  2 ny3881            1263 12 Ave & W 125 St
##  3 ny514             1024 12 Ave & W 40 St
##  4 ny3349             978 Grand Army Plaza & Plaza St West
##  5 ny3992             964 W 169 St & Fort Washington Ave
##  6 ny3374             860 Central Park North & Adam Clayton Powell Blvd
##  7 ny3782             837 Brooklyn Bridge Park - Pier 2
##  8 ny3599             829 Franklin Ave & Empire Blvd
##  9 ny3521             793 Lenox Ave & W 111 St
## 10 ny2006             782 Central Park S & 6 Ave
## # … with 948 more rows
````

All of the top 10 docking stations are either in parks, near parks or located along river promenades. This coupled with the fact that these trips occur in much greater relative number for casual than regular users (`Customer` vs `Subscriber`) is  further evidence that these are valid trips.

#### Write functions of your own

Through most of the course we will be making use of functions written by others -- mainly those developed for packages that form the `tidyverse` and therefore that follow a consistent syntax. However, there may be times where you need to abstract over some of your code to make functions of your own. [Chapter 19](https://r4ds.had.co.nz/functions.html) of @wickham_r_2017 presents some helpful guidelines around the circumstances under a the data scientist might write functions. Most often this is when you find yourself copy and pasting the same chunks of code with minimal adaptation.

Functions have three key characteristics:

1. They are (usually) *named*  -- the name should be expressive and communicate what the function does (we talk about `dplyr` *verbs*).
2.  They have brackets `<function()>` usually containing *arguments* -- inputs which determine what the function does and returns.
3.  Immediately followed by `<function()>` are `{}` used to contain the *body* -- in this is code that performs a distinct task, described by the function's *name*.

Effective functions are *short*, perform *single* discrete operations and are *intuitive*.

You will recall that in the `ny_trips` table there is a variable called `birth_year`. From this we can derive cyclists' approximate age. Below I have written a function `get_age()` for doing this. The function expects two *arguments*: `yob` -- a year of birth as type `chr`; `yref` -- a reference year. In the *body*, `lubridate`'s `as.period` function is used to calculate the time in years that elapsed, the value that the function *returns*. Once defined, and loaded into the session by being executed, it can be used (as below).

```{r}
#| echo: true
#| eval: false

# Depends on lubridate
library(lubridate)
# Function for calculating time elapsed between two dates in years (age).
get_age <- function(yob, yref) {
    period <- as.period(interval(yob, yref),unit = "year")
    return(period$year)
}

ny_trips <- ny_trips |> 
  # Calculate age from birth_date.
  mutate(
    age=get_age(
      as.POSIXct(birth_year, format="%Y"), 
      as.POSIXct("2020", format="%Y")
    ) 
  )
````

We can use the two new derived variables -- distance travelled and age --  in our analysis. In @fig-plot-speeds, we explore how *approximate* travel speeds vary by age, trip distance and customer type. The code used to generate the summary data and plot is in the template file. Again the average speed calculation should be treated cautiously as it is based on straight line distances and it is likely that this will very much vary depending on whether the trip is made for 'utilitarian' or 'leisure' purposes. I have tried to do the latter by selecting trips that occur only on weekdays; those made by `Subscriber` cyclists might be most heavily associated with commuter trips. Additionally, due to the heavy subsetting, data become a little volatile for certain age groups and so the age variable is aggregated into 5-year bands. 

There are some interesting and expected patterns. `Subscribers` make faster trips than do  `Customers`, although this gap narrows as trip distance increases. Trips with a straight-line distance of 4.5km are non-trivial and so may be classed as 'utilitarian' even for non-regular `Customers`.  There is a very slight effect of decreasing trip speed by age cycled for the longer trips. The volatility in the older age groups for trips >4.5km suggests we probably need more data and a more involved analysis to establish this. For example, it may be that the comparatively rare occurrence of trips in the 65-70 age group is made by only a small subset of cyclists. A larger dataset would result in a regression to the mean effect and negate any noise caused by outlier individuals. 

```{r}
#| label: fig-plot-speeds
#| fig-cap: "Line charts generated with `ggplot2`. Plot data computed using `dplyr` and `lubridate`."
#| echo: false
#| include: false

include_graphics("/figs/speeds.png", error = FALSE)
```
![
#| fig-cap: "Line charts generated with `ggplot2`. Plot data computed using `dplyr` and `lubridate`.](figs/02/speeds.svg){#fig-plot-speeds width=100% fig-align="center"}


### Tidy

The `ny_trips` and `ny_stations` data already comply with the rules for *tidy* data [@wickham_tidy_2014]. Each row in `ny_trips` is a distinct trip and each row in `ny_stations` a distinct station. However, it is common to encounter datasets that need to be reshaped. There are two key functions to learn here: [`pivot_longer()`](https://tidyr.tidyverse.org/reference/pivot_longer.html) and [`pivot_wider()`](https://tidyr.tidyverse.org/reference/pivot_wider.html). `pivot_longer()` is used to tidy data in which observations are spread across columns, as in @tbl-gapminder (b) (the `gapminder` dataset). `pivot_wider()` is used to tidy data in which variables are spread across rows, as in @tbl-gapminder (c). You will find yourself using these functions not only for fixing messy data, but for flexibly reshaping data for use in `ggplot2` specifications (more on this in Chapter [-@sec-visual] and [-@sec-exploratory]) or joining tables.

A quick breakdown of `pivot_longer`:

```{r}
#| echo: true
#| eval: false
pivot_longer(
  data,
  # Columns to pivot longer (across rows).
  cols, 
  # Name of the column to create from values held in spread *column names*.
  names_to="name",
   # Name of column to create form values stored in spread *cells*. 
  values_to="name"
  )
```

A quick breakdown of `pivot_wider`:

```{r}
#| echo: true
#| eval: false
pivot_wider(
  data,
  # Column in the long format which contains what will be column names 
  # in the wide format.
  names_from, 
  # Column in the long format which contains what will be values
  # in the new wide format.
  values_from 
  )
```

In the *task* you will be tidying some messy derived tables based on the bikeshare data using both of these functions, but we can demonstrate their purpose in *tidying* the messy `gapminder` data in @tbl-gapminder. Remember that these data were messy as the observations by gender were spread across the columns:

```{r}
#| echo: true
#| eval: false
untidy_wide
##   country     year  f_cases m_cases f_population m_population
##   <chr>       <chr> <chr>   <chr>   <chr>        <chr>
## 1 Afghanistan 1999  447     298     9993400      9993671
## 2 Afghanistan 2000  1599    1067    10296280     10299080
## 3 Brazil      1999  16982   20755   86001181     86005181
## 4 Brazil      2000  39440   41048   87251329     87253569
## 5 China       1999  104007  108252  636451250    636464022
## 6 China       2000  104746  109759  640212600    640215983
```

First we need to gather the problematic columns with `pivot_longer()`.

```{r}
#| echo: true
#| eval: false
untidy_wide |> 
  pivot_longer(
    cols=c(f_cases: m_population), 
    names_to=c("gender_count_type"), 
    values_to=c("counts")
  )

##   country     year  gender_count_type       counts
##   <chr>       <chr> <chr>                   <chr>
##  1 Afghanistan 1999  f_cases                447
##  2 Afghanistan 1999  m_cases                298
##  3 Afghanistan 1999  f_population           9993400
##  4 Afghanistan 1999  m_population           9993671
##  5 Afghanistan 2000  f_cases                1599
##  6 Afghanistan 2000  m_cases                1067
##  7 Afghanistan 2000  f_population           10296280
##  8 Afghanistan 2000  m_population           10299080
##  9 Brazil      1999  f_cases                16982
## 10 Brazil      1999  m_cases                20755
## # … with 14 more rows
```
So this has usefully collapsed the dataset by gender, we now have a problem similar to that in @tbl-gapminder (b) where observations are spread across the rows -- in this instance `cases` and `population` are better treated as separate variables. This can be fixed by `separating` the `gender_count_type` variables and then spreading the values of the new `count_type` (`cases`, `population`) across the columns. Hopefully you can see how this gets us to the *tidy* `gapminder` data structure in @tbl-gapminder (a).

```{r}
#| echo: true
#| eval: false
untidy_wide |> 
  pivot_longer(
    cols=c(f_cases: m_population), 
    names_to=c("gender_count_type"), 
    values_to=c("counts")
  ) |> 
  separate(
    col=gender_count_type, 
    into=c("gender", "count_type"), 
    sep="_"
  )

##    country     year  gender count_type counts
##    <chr>       <chr> <chr>  <chr>      <chr>
##  1 Afghanistan 1999  f      cases      447
##  2 Afghanistan 1999  m      cases      298
##  3 Afghanistan 1999  f      population 9993400
##  4 Afghanistan 1999  m      population 9993671
##  5 Afghanistan 2000  f      cases      1599
##  6 Afghanistan 2000  m      cases      1067
##  7 Afghanistan 2000  f      population 10296280
##  8 Afghanistan 2000  m      population 10299080
##  9 Brazil      1999  f      cases      16982
## 10 Brazil      1999  m      cases      20755
## # … with 14 more rows

untidy_wide |> 
  pivot_longer(
    cols=c(f_cases: m_population), 
    names_to=c("gender_count_type"), 
    values_to=c("counts")
  ) |> 
  separate(
    col=gender_count_type, 
    into=c("gender", "count_type"), 
    sep="_"
  ) |> 
  pivot_wider(names_from=count_type, values_from=counts)

##    country     year  gender cases  population
##    <chr>       <chr> <chr>  <chr>  <chr>
##  1 Afghanistan 1999  f      447    9993400
##  2 Afghanistan 1999  m      298    9993671
##  3 Afghanistan 2000  f      1599   10296280
##  4 Afghanistan 2000  m      1067   10299080
##  5 Brazil      1999  f      16982  86001181
##  6 Brazil      1999  m      20755  86005181
##  7 Brazil      2000  f      39440  87251329
##  8 Brazil      2000  m      41048  87253569
##  9 China       1999  f      104007 636451250
## 10 China       1999  m      108252 636464022
## 11 China       2000  f      104746 640212600
## 12 China       2000  m      109759 640215983
```

## Conclusions

Developing the vocabulary and technical skills to systematically describe and organise data is crucial to modern data analysis. This chapter has covered the fundamentals here: that data consist of *observations* and *variables* of different types [@stevens_on_1946] and that in order to work effectively with datasets, these data must be organised according to the rules of *tidy*  data [@wickham_tidy_2014]. Most of the content was dedicated to the techniques that enable these concepts to be operationalised. We covered how to download, transform and reshape a reasonably large set of data from New York's [Citibike]() scheme.  In doing so, we generated insights that might inform further data collection and analysis activity. In the next chapter we will apply and extend this conceptual and technical knowledge as we introduce the fundamentals of visual data analysis and `ggplot2`'s grammar of graphics.
